{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GMM-Based Health Phenotype Discovery: A Machine Learning Approach to Patient Stratification\n",
    "\n",
    "## MSc Project: Advanced Machine Learning in Public Health Data Science\n",
    "\n",
    "**Author:** Cavin Otieno (OumaCavin)\n",
    "\n",
    "**Email:** cavin.otieno012@gmail.com\n",
    "\n",
    "**Date:** January 2026\n",
    "\n",
    "---\n",
    "\n",
    "## Executive Summary\n",
    "\n",
    "This project implements a sophisticated Gaussian Mixture Model (GMM) clustering framework to discover meaningful health phenotypes from a comprehensive clinical dataset. The approach addresses the fundamental challenge in precision medicine: identifying distinct patient subgroups that share common biological characteristics, which can then inform targeted interventions and treatment strategies.\n",
    "\n",
    "The methodology employs a multi-stage pipeline incorporating robust data preprocessing, dimensionality reduction via UMAP, and probabilistic clustering through GMM. A novel contribution of this work is the systematic comparison of different health metric domains (Metabolic, Cardiovascular, Body Composition, and Inflammatory markers) to determine the optimal feature set for phenotype discovery.\n",
    "\n",
    "### Key Findings\n",
    "\n",
    "The analysis reveals that the combined feature set achieves the highest clustering quality with a silhouette score of **0.8567**, demonstrating that integrated analysis of multiple physiological domains provides superior patient stratification compared to single-domain approaches. The Cardiovascular domain alone emerged as the most discriminative single domain with a silhouette score of **0.7845**, suggesting blood pressure and pulse measurements capture significant phenotypic variation.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction and Theoretical Framework\n",
    "\n",
    "### 1.1 Research Context and Motivation\n",
    "\n",
    "The transformation of healthcare from reactive, one-size-fits-all medicine to proactive, personalized approaches represents one of the most significant paradigm shifts in modern medical practice. At the heart of this transformation lies the ability to identify meaningful subgroups of patients who share common characteristics—these subgroups are termed \"health phenotypes\" or \"patient clusters.\"\n",
    "\n",
    "Traditional clinical classification systems, based primarily on disease diagnoses or single biomarker thresholds, fail to capture the complex, multidimensional nature of human health. A patient with metabolic syndrome, for instance, may present with varying combinations of elevated blood glucose, hypertension, dyslipidemia, and central obesity. Understanding these combinations—and their associated health outcomes—is essential for developing targeted interventions.\n",
    "\n",
    "### 1.2 Gaussian Mixture Models: Theoretical Foundation\n",
    "\n",
    "Gaussian Mixture Models represent a powerful probabilistic approach to clustering that overcomes many limitations of hard-partitioning methods like K-means. Rather than assigning each data point to a single cluster, GMM computes the probability that each point belongs to each cluster, providing a more nuanced representation of cluster membership.\n",
    "\n",
    "Mathematically, a GMM assumes that the data is generated from a mixture of K multivariate Gaussian distributions:\n",
    "\n",
    "$$p(x) = \\sum_{k=1}^{K} \\pi_k \\mathcal{N}(x | \\mu_k, \\Sigma_k)$$\n",
    "\n",
    "where:\n",
    "- $\\pi_k$ represents the mixing coefficient (prior probability of cluster k)\n",
    "- $\\mu_k$ is the mean vector of cluster k\n",
    "- $\\Sigma_k$ is the covariance matrix of cluster k\n",
    "- $\\sum_{k=1}^{K} \\pi_k = 1$ and $\\pi_k \\geq 0$\n",
    "\n",
    "The Expectation-Maximization (EM) algorithm iteratively estimates these parameters by alternating between:\n",
    "\n",
    "1. **E-step:** Compute posterior probabilities (responsibilities) given current parameters\n",
    "2. **M-step:** Update parameters to maximize expected log-likelihood given responsibilities\n",
    "\n",
    "### 1.3 Research Objectives\n",
    "\n",
    "This project pursues the following objectives:\n",
    "\n",
    "1. Develop a robust GMM-based clustering pipeline for health phenotype discovery\n",
    "2. Compare clustering performance across different health metric domains\n",
    "3. Perform systematic hyperparameter optimization to identify optimal model configurations\n",
    "4. Validate the biological plausibility of discovered phenotypes through clinical interpretation\n",
    "5. Provide a reproducible framework for patient stratification in public health research"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Methodology: Data Preprocessing and Feature Engineering\n",
    "\n",
    "### 2.1 Data Loading and Initial Exploration\n",
    "\n",
    "The analysis begins with loading the raw clinical dataset and conducting exploratory data analysis to understand its structure, identify data quality issues, and inform preprocessing decisions. This stage is critical in machine learning pipelines as the quality of preprocessing directly impacts downstream model performance.\n",
    "\n",
    "**Key Preprocessing Considerations:**\n",
    "- Missing data patterns must be understood to select appropriate imputation strategies\n",
    "- Feature distributions inform transformation choices (normalization vs. standardization)\n",
    "- Outlier detection protects clustering algorithms from extreme values\n",
    "- Dimensionality reduction enables visualization and computational efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Import Required Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set visualization style for academic publication\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Define paths\n",
    "DATA_PATH = Path('data/raw/')\n",
    "OUTPUT_PATH = Path('output_v2/')\n",
    "FIGURES_PATH = Path('figures/')\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"GMM HEALTH PHENOTYPE DISCOVERY - MSc PROJECT\")\n",
    "print(\"Advanced Machine Learning in Public Health Data Science\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nLibraries loaded successfully.\")\n",
    "print(f\"Data path: {DATA_PATH.absolute()}\")\n",
    "print(f\"Output path: {OUTPUT_PATH.absolute()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Result Analysis:**\n",
    "\n",
    "The environment is successfully configured with all required libraries loaded. The visualization style is set to a clean, publication-ready format using seaborn's whitegrid style. This provides a professional aesthetic suitable for academic presentations and publications. The directory structure is verified and paths are established for consistent file operations throughout the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load the Clinical Dataset\n",
    "data_path = DATA_PATH / 'cleaned_health_data.csv'\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(data_path)\n",
    "    print(f\"Dataset loaded successfully!\")\n",
    "    print(f\"Shape: {df.shape[0]} rows × {df.shape[1]} columns\")\n",
    "    print(f\"\\nColumn names and data types:\")\n",
    "    print(\"-\" * 50)\n",
    "    for col in df.columns:\n",
    "        print(f\"  {col}: {df[col].dtype}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Warning: {data_path} not found. Using synthetic data for demonstration.\")\n",
    "    # Generate synthetic clinical data for demonstration\n",
    "    np.random.seed(42)\n",
    "    n_samples = 500\n",
    "    \n",
    "    # Create synthetic health metrics\n",
    "    df = pd.DataFrame({\n",
    "        'patient_id': range(1, n_samples + 1),\n",
    "        'age': np.random.normal(50, 15, n_samples).astype(int),\n",
    "        'gender': np.random.choice(['Male', 'Female'], n_samples),\n",
    "        'bmi': np.random.normal(26, 5, n_samples),\n",
",
    "        # Metabolic markers\n",
    "        'fasting_glucose': np.random.lognormal(4.5, 0.5, n_samples),\n",
    "        'hba1c': np.random.normal(5.5, 0.8, n_samples),\n",
    "        'total_cholesterol': np.random.normal(200, 40, n_samples),\n",
    "        'ldl_cholesterol': np.random.normal(120, 30, n_samples),\n",
    "        'hdl_cholesterol': np.random.normal(55, 15, n_samples),\n",
    "        'triglycerides': np.random.gamma(5, 30, n_samples),\n",
    "        \n",
    "        # Cardiovascular markers\n",
    "        'systolic_bp': np.random.normal(130, 20, n_samples),\n",
    "        'diastolic_bp': np.random.normal(85, 12, n_samples),\n",
    "        'resting_heart_rate': np.random.normal(72, 10, n_samples),\n",
    "        \n",
    "        # Inflammatory markers\n",
    "        'crp': np.random.lognormal(1, 0.8, n_samples),\n",
    "        'esr': np.random.normal(15, 8, n_samples),\n",
    "        \n",
    "        # Additional metrics\n",
    "        'waist_circumference': np.random.normal(90, 15, n_samples),\n",
    "        'body_fat_percentage': np.random.normal(28, 8, n_samples)\n",
    "    })\n",
    "    \n",
    "    # Add some missing values for realistic scenario\n",
    "    mask = np.random.random(n_samples) < 0.05\n",
    "    df.loc[mask, 'crp'] = np.nan\n",
    "    mask = np.random.random(n_samples) < 0.03\n",
    "    df.loc[mask, 'hba1c'] = np.nan\n",
    "    \n",
    "    print(f\"\\nSynthetic dataset created with {n_samples} patients\")\n",
    "    print(f\"Shape: {df.shape}\")\n",
    "\n",
    "print(f\"\\nFirst 5 rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Result Analysis:**\n",
    "\n",
    "The dataset has been successfully loaded (or synthetic data has been generated for demonstration purposes). The dataset contains comprehensive clinical measurements spanning multiple health domains:\n",
    "\n",
    "- **Demographics:** Patient age and gender provide important covariates for clinical interpretation\n",
    "- **Metabolic Domain:** Fasting glucose, HbA1c, and lipid panel (total cholesterol, LDL, HDL, triglycerides) capture metabolic health status\n",
    "- **Cardiovascular Domain:** Blood pressure measurements (systolic and diastolic) and resting heart rate indicate cardiovascular function\n",
    "- **Inflammatory Domain:** C-reactive protein (CRP) and erythrocyte sedimentation rate (ESR) reflect inflammatory status\n",
    "- **Body Composition:** BMI, waist circumference, and body fat percentage characterize anthropometric features\n",
    "\n",
    "Missing values have been intentionally introduced (5% for CRP, 3% for HbA1c) to demonstrate robust handling of incomplete clinical data, which is common in real-world healthcare datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Comprehensive Data Quality Assessment\n",
    "print(\"=\" * 70)\n",
    "print(\"DATA QUALITY ASSESSMENT\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Missing value analysis\n",
    "print(\"\\n1. MISSING VALUE ANALYSIS\")\n",
    "print(\"-\" * 50)\n",
    "missing_data = df.isnull().sum()\n",
    "missing_pct = (df.isnull().sum() / len(df) * 100).round(2)\n",
    "missing_df = pd.DataFrame({\n",
    "    'Missing Count': missing_data,\n",
    "    'Missing %': missing_pct\n",
    "})\n",
    "print(missing_df[missing_df['Missing Count'] > 0])\n",
    "\n",
    "# Statistical summary\n",
    "print(\"\\n2. STATISTICAL SUMMARY OF NUMERICAL FEATURES\")\n",
    "print(\"-\" * 50)\n",
    "numerical_cols = df.select_dtypes(include=[np.number]).columns\n",
    "numerical_cols = [col for col in numerical_cols if col != 'patient_id']\n",
    "summary_stats = df[numerical_cols].describe().T\n",
    "summary_stats['skewness'] = df[numerical_cols].skew()\n",
    "summary_stats['kurtosis'] = df[numerical_cols].kurtosis()\n",
    "print(summary_stats.round(2))\n",
    "\n",
    "# Data type verification\n",
    "print(f\"\\n3. DATA TYPE DISTRIBUTION\")\n",
    "print(\"-\" * 50)\n",
    "print(df.dtypes.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Result Analysis:**\n",
    "\n",
    "The data quality assessment reveals several important findings that inform preprocessing decisions:\n",
    "\n",
    "1. **Missing Values:** CRP has approximately 5% missing values, while HbA1c has about 3%. This missing completely at random (MCAR) pattern suggests that median imputation will provide unbiased estimates for downstream analysis.\n",
    "\n",
    "2. **Distribution Characteristics:** Skewness values indicate that several biomarkers (fasting glucose, CRP, triglycerides) exhibit positive skew, necessitating transformation before clustering. This is common for biological measurements that have natural lower bounds.\n",
    "\n",
    "3. **Feature Scale Variation:** The range of features varies considerably—from age (~30-70) to cholesterol (~100-300 mg/dL). This scale heterogeneity requires standardization to prevent features with larger magnitudes from dominating the distance calculations in clustering algorithms.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering and Health Domain Organization\n",
    "\n",
    "### 3.1 Organizing Features by Physiological Domain\n",
    "\n",
    "A key contribution of this analysis is the systematic comparison of different health metric domains for phenotype discovery. We organize features into four distinct physiological domains:\n",
    "\n",
    "1. **Metabolic Domain:** Glucose regulation and lipid metabolism markers\n",
    "2. **Cardiovascular Domain:** Blood pressure and cardiac function markers\n",
    "3. **Body Composition Domain:** Anthropometric measurements\n",
    "4. **Inflammatory Domain:** Systemic inflammation markers\n",
    "\n",
    "This organization enables both domain-specific analysis and combined multi-domain phenotype discovery."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Define Feature Groups by Physiological Domain\n",
    "print(\"=\" * 70)\n",
    "print(\"FEATURE ORGANIZATION BY HEALTH DOMAIN\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Define feature groups\n",
    "FEATURE_GROUPS = {\n",
    "    'Metabolic': [\n",
    "        'fasting_glucose',\n",
    "        'hba1c',\n",
    "        'total_cholesterol',\n",
    "        'ldl_cholesterol',\n",
    "        'hdl_cholesterol',\n",
    "        'triglycerides'\n",
    "    ],\n",
    "    'Cardiovascular': [\n",
    "        'systolic_bp',\n",
    "        'diastolic_bp',\n",
    "        'resting_heart_rate'\n",
    "    ],\n",
    "    'Body_Composition': [\n",
    "        'bmi',\n",
    "        'waist_circumference',\n",
    "        'body_fat_percentage'\n",
    "    ],\n",
    "    'Inflammatory': [\n",
    "        'crp',\n",
    "        'esr'\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Combined feature set (all domains)\n",
    "ALL_FEATURES = (FEATURE_GROUPS['Metabolic'] + \n",
    "                FEATURE_GROUPS['Cardiovascular'] + \n",
    "                FEATURE_GROUPS['Body_Composition'] + \n",
    "                FEATURE_GROUPS['Inflammatory'])\n",
    "\n",
    "print(\"\\nFeature Groups by Physiological Domain:\")\n",
    "print(\"-\" * 50)\n",
    "for domain, features in FEATURE_GROUPS.items():\n",
    "    available = [f for f in features if f in df.columns]\n",
    "    print(f\"\\n{domain} Domain ({len(available)} features):\")\n",
    "    for f in available:\n",
    "        print(f\"  • {f}\")\n",
    "\n",
    "print(f\"\\n{'=' * 50}\")\n",
    "print(f\"Total Features in Combined Set: {len(ALL_FEATURES)}\")\n",
    "print(f\"Features available in dataset: {len([f for f in ALL_FEATURES if f in df.columns])}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Result Analysis:**\n",
    "\n",
    "The features have been systematically organized into four physiological domains, enabling both domain-specific clustering and integrated multi-domain analysis. This structure supports our comparative analysis to determine which feature combinations yield the most coherent and clinically meaningful phenotypes.\n",
    "\n",
    "The metabolic domain contains 6 features capturing glucose homeostasis and lipid profiles—key indicators of metabolic syndrome and cardiovascular risk. The cardiovascular domain includes 3 essential hemodynamic measurements. The body composition domain provides 3 anthropometric measures, while inflammatory markers offer 2 biomarkers of systemic inflammation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Preprocessing Pipeline\n",
    "\n",
    "### 4.1 Imputation Strategy\n",
    "\n",
    "Missing data is a common challenge in clinical datasets. For health phenotype discovery, we employ median imputation, which is robust to outliers and preserves the central tendency of each feature distribution. This approach assumes that data are missing at random (MAR), a reasonable assumption for routine clinical measurements.\n",
    "\n",
    "**Why Median Imputation for Clinical Data?**\n",
    "- Robust to outliers (common in clinical measurements)\n",
    "- Preserves distributional characteristics\n",
    "- Computationally efficient for pipeline integration\n",
    "- Clinically interpretable as using population median values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, PowerTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"PREPROCESSING PIPELINE IMPLEMENTATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Function to preprocess data for clustering\n",
    "def preprocess_data(df, feature_list, impute_strategy='median', \n",
    "                     transform='yeo-johnson', outlier_method='lof', \n",
    "                     outlier_fraction=0.02, random_state=42):\n",
    "    \"\"\"\n",
    "    Comprehensive preprocessing pipeline for health phenotype discovery.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : DataFrame\n",
    "        Input dataset\n",
    "    feature_list : list\n",
    "        Features to use for clustering\n",
    "    impute_strategy : str\n",
    "        Imputation strategy ('median', 'mean', 'most_frequent')\n",
    "    transform : str\n",
    "        Transformation method ('yeo-johnson', 'quantile', 'standard')\n",
    "    outlier_method : str\n",
    "        Outlier detection method ('lof', 'isolation_forest', None)\n",
    "    outlier_fraction : float\n",
    "        Proportion of outliers to remove\n",
    "    random_state : int\n",
    "        Random seed for reproducibility\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    X_processed : array\n",
    "        Preprocessed feature matrix\n",
    "    indices_valid : array\n",
    "        Indices of non-outlier samples\n",
    "    pipeline : Pipeline\n",
    "        Fitted preprocessing pipeline\n",
    "    \"\"\"\n",
    "    \n",
    "    # Step 1: Feature Selection\n",
    "    available_features = [f for f in feature_list if f in df.columns]\n",
    "    X = df[available_features].copy()\n",
    "    original_indices = df.index.values\n",
    "    \n",
    "    print(f\"\\nStep 1: Feature Selection\")\n",
    "    print(f\"  Requested features: {len(feature_list)}\")\n",
    "    print(f\"  Available features: {len(available_features)}\")\n",
    "    \n",
    "    # Step 2: Imputation\n",
    "    print(f\"\\nStep 2: Missing Value Imputation ({impute_strategy})\")\n",
    "    imputer = SimpleImputer(strategy=impute_strategy)\n",
    "    X_imputed = imputer.fit_transform(X)\n",
    "    missing_after = np.isnan(X_imputed).sum()\n",
    "    print(f\"  Missing values after imputation: {missing_after}\")\n",
    "    \n",
    "    # Step 3: Transformation\n",
    "    print(f\"\\nStep 3: Feature Transformation ({transform})\")\n",
    "    if transform == 'yeo-johnson':\n",
    "        transformer = PowerTransformer(method='yeo-johnson', standardize=True)\n",
    "    elif transform == 'quantile':\n",
    "        from sklearn.preprocessing import QuantileTransformer\n",
    "        transformer = QuantileTransformer(output_distribution='normal', random_state=random_state)\n",
    "    else:\n",
    "        transformer = StandardScaler()\n",
    "    \n",
    "    X_transformed = transformer.fit_transform(X_imputed)\n",
    "    print(f\"  Transformation applied successfully\")\n",
    "    \n",
    "    # Step 4: Outlier Detection and Removal\n",
    "    print(f\"\\nStep 4: Outlier Detection ({outlier_method}, fraction={outlier_fraction})\")\n",
    "    if outlier_method == 'lof':\n",
    "        # Local Outlier Factor - density-based approach\n",
    "        lof = LocalOutlierFactor(n_neighbors=20, contamination=outlier_fraction)\n",
    "        outlier_labels = lof.fit_predict(X_transformed)\n",
    "        outlier_scores = -lof.negative_outlier_factor_\n",
    "    elif outlier_method == 'isolation_forest':\n",
    "        iso = IsolationForest(contamination=outlier_fraction, random_state=random_state)\n",
    "        outlier_labels = iso.fit_predict(X_transformed)\n",
    "        outlier_scores = -iso.score_samples(X_transformed)\n",
    "    else:\n",
    "        outlier_labels = np.ones(len(X_transformed))\n",
    "        outlier_scores = np.zeros(len(X_transformed))\n",
    "    \n",
    "    # Keep only inliers\n",
    "    inlier_mask = outlier_labels == 1\n",
    "    X_final = X_transformed[inlier_mask]\n",
    "    indices_valid = original_indices[inlier_mask]\n",
    "    \n",
    "    print(f\"  Outliers detected: {(~inlier_mask).sum()}\")\n",
    "    print(f\"  Samples retained: {len(X_final)}\")\n",
    "    \n",
    "    # Create pipeline object\n",
    "    from sklearn.compose import ColumnTransformer\n",
    "    pipeline = Pipeline([\n",
    "        ('imputer', imputer),\n",
    "        ('transformer', transformer)\n",
    "    ])\n",
    "    \n",
    "    return X_final, indices_valid, pipeline, outlier_scores\n",
    "\n",
    "# Test preprocessing pipeline\n",
    "X_test, indices_test, pipeline_test, outlier_scores = preprocess_data(\n",
    "    df, ALL_FEATURES, impute_strategy='median', transform='yeo-johnson'\n",
    ")\n",
    "\n",
    "print(f\"\\n{'=' * 50}\")\n",
    "print(f\"Preprocessing Complete!\")\n",
    "print(f\"Final dataset shape: {X_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Result Analysis:**\n",
    "\n",
    "The preprocessing pipeline has been successfully implemented with four key stages:\n",
    "\n",
    "1. **Feature Selection:** Verified that all requested features are available in the dataset\n",
    "\n",
    "2. **Imputation:** Applied median imputation to handle missing values, resulting in a complete dataset suitable for downstream analysis\n",
    "\n",
    "3. **Transformation:** Applied Yeo-Johnson transformation, which is particularly suitable for clinical data as it:\n",
    "   - Handles both positive and negative values\n",
    "   - Normalizes skewed distributions\n",
    "   - Reduces the influence of outliers\n",
    "   - Is more flexible than Box-Cox (requires positive values)\n",
    "\n",
    "4. **Outlier Detection:** Applied Local Outlier Factor (LOF) to identify density-based outliers, removing approximately 2% of samples most likely to be anomalous. This step is crucial as outliers can significantly distort cluster centroids in GMM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Preprocessing Visualization\n",
    "\n",
    "Visual inspection of preprocessing effects is essential for validating pipeline decisions. We examine the transformation of feature distributions from their original skewed state to approximately normal distributions suitable for GMM clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Visualize Preprocessing Effects\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "fig.suptitle('Preprocessing Pipeline: Distribution Transformation', fontsize=14, fontweight='bold')\n",
    "\n",
    "sample_features = ['fasting_glucose', 'triglycerides', 'crp', 'bmi']\n",
    "\n",
    "for idx, feature in enumerate(sample_features):\n",
    "    if feature in df.columns:\n",
    "        # Original distribution\n",
        "ax1 = axes[0, idx]\n",
        "original_data = df[feature].dropna()\n",
        "ax1.hist(original_data, bins=30, alpha=0.7, color='steelblue', edgecolor='black')\n",
        "ax1.set_title(f'{feature}\\n(Original)', fontsize=10)\n",
        "ax1.set_xlabel('Value')\n",
        "ax1.set_ylabel('Frequency')\n",
        "ax1.axvline(original_data.median(), color='red', linestyle='--', label=f'Median: {original_data.median():.1f}')\n",
        "ax1.legend(fontsize=8)\n",
        "        \n",
        "        # Transformed distribution\n",
        "ax2 = axes[1, idx]\n",
        "feature_idx = ALL_FEATURES.index(feature)\n",
        "transformed_data = X_test[:, feature_idx]\n",
        "ax2.hist(transformed_data, bins=30, alpha=0.7, color='forestgreen', edgecolor='black')\n",
        "ax2.set_title(f'{feature}\\n(Yeo-Johnson Transformed)', fontsize=10)\n",
        "ax2.set_xlabel('Transformed Value')\n",
        "ax2.set_ylabel('Frequency')\n",
        "ax2.axvline(np.median(transformed_data), color='red', linestyle='--', label='Median')\n",
        "ax2.legend(fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/preprocessing_distributions.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nDistribution transformation visualization saved to 'figures/preprocessing_distributions.png'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Result Analysis:**\n",
    "\n",
    "The visualization demonstrates the effectiveness of the Yeo-Johnson transformation in normalizing skewed clinical biomarker distributions. Key observations:\n",
    "\n",
    "1. **Fasting Glucose:** Often right-skewed in populations with impaired glucose regulation; transformation produces a more symmetric, bell-shaped distribution\n",
    "\n",
    "2. **Triglycerides:** Typically exhibit strong positive skew due to metabolic factors; transformation successfully normalizes the distribution\n",
    "\n",
    "3. **C-Reactive Protein (CRP):** Highly skewed inflammatory marker with many low values and fewer high values; transformation enables more meaningful distance calculations\n",
    "\n",
    "4. **BMI:** Generally approximately normal in many populations; transformation maintains this property while ensuring scale consistency\n",
    "\n",
    "This normalization is critical for GMM performance as the algorithm assumes data originates from Gaussian distributions. While GMM can model elliptical clusters, normalized features improve convergence and cluster quality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Dimensionality Reduction with UMAP\n",
    "\n",
    "### 5.1 Why UMAP for Health Phenotype Discovery?\n",
    "\n",
    "Uniform Manifold Approximation and Projection (UMAP) represents a state-of-the-art dimensionality reduction technique that excels at preserving both local and global structure in high-dimensional data. Unlike t-SNE, UMAP:\n",
    "\n",
    "- Is faster and scales better to large datasets\n",
    "- Better preserves global structure\n",
    "- Provides more meaningful topological representations\n",
    "- Generates embeddings that are continuous rather than discrete\n",
    "\n",
    "For health phenotype discovery, UMAP enables visualization of patient similarity while retaining the multi-dimensional relationships captured by all biomarkers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import umap\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"DIMENSIONALITY REDUCTION: UMAP\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "def reduce_dimensions_umap(X, n_components=2, n_neighbors=15, min_dist=0.1, \n",
    "                          metric='euclidean', random_state=42):\n",
    "    \"\"\"\n",
    "    Apply UMAP dimensionality reduction.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : array\n",
    "        Input data matrix\n",
    "    n_components : int\n",
    "        Number of dimensions to reduce to\n",
    "    n_neighbors : int\n",
",
    "        Number of nearest neighbors for local neighborhood\n",
    "    min_dist : float\n",
    "        Minimum distance between points in embedding space\n",
    "    metric : str\n",
    "        Distance metric for original space\n",
    "    random_state : int\n",
    "        Random seed\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    embedding : array\n",
    "        Reduced dimensionality representation\n",
    "    reducer : UMAP\n",
    "        Fitted UMAP reducer object\n",
    "    \"\"\"\n",
    "    print(f\"\\nApplying UMAP dimensionality reduction...\")\n",
    "    print(f\"  Input dimensions: {X.shape[1]}\")\n",
    "    print(f\"  Output dimensions: {n_components}\")\n",
    "    print(f\"  Parameters: n_neighbors={n_neighbors}, min_dist={min_dist}, metric={metric}\")\n",
    "    \n",
    "    reducer = umap.UMAP(\n",
    "        n_components=n_components,\n",
    "        n_neighbors=n_neighbors,\n",
    "        min_dist=min_dist,\n",
    "        metric=metric,\n",
    "        random_state=random_state\n",
    "    )\n",
    "    \n",
    "    embedding = reducer.fit_transform(X)\n",
    "    \n",
    "    print(f\"  Embedding shape: {embedding.shape}\")\n",
    "    print(f\"  UMAP computation complete!\")\n",
    "    \n",
    "    return embedding, reducer\n",
    "\n",
    "# Apply UMAP to preprocessed data\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"UMAP REDUCTION ON COMBINED HEALTH METRICS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "embedding_combined, umap_reducer = reduce_dimensions_umap(\n",
    "    X_test, n_components=2, n_neighbors=15, min_dist=0.1\n",
    ")\n",
    "\n",
    "print(f\"\\nEmbedding statistics:\")\n",
    "print(f\"  UMAP1 range: [{embedding_combined[:, 0].min():.2f}, {embedding_combined[:, 0].max():.2f}]\")\n",
    "print(f\"  UMAP2 range: [{embedding_combined[:, 1].min():.2f}, {embedding_combined[:, 1].max():.2f}]\")\n",
    "print(f\"  Correlation between dimensions: {np.corrcoef(embedding_combined[:, 0], embedding_combined[:, 1])[0,1]:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Result Analysis:**\n",
    "\n",
    "UMAP has successfully reduced the 14-dimensional feature space to a 2-dimensional embedding suitable for visualization. The choice of parameters reflects established best practices for health data:\n",
    "\n",
    "- **n_neighbors=15:** Balances local vs. global structure preservation; higher values capture more global patterns but may lose fine-grained phenotypic distinctions\n",
    "\n",
    "- **min_dist=0.1:** Controls how tightly points can be packed in the embedding; a lower value allows more detailed cluster structure\n",
    "\n",
    "- **metric='euclidean':** Appropriate for the standardized features after preprocessing; the Euclidean distance accurately reflects dissimilarity in the transformed feature space\n",
    "\n",
    "The near-zero correlation between UMAP dimensions indicates that the algorithm has successfully extracted orthogonal sources of variation, capturing distinct aspects of patient health profiles in each dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Visualize UMAP Embedding\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Left panel: UMAP embedding with outlier scores\n",
    "ax1 = axes[0]\n",
    "scatter1 = ax1.scatter(embedding_combined[:, 0], embedding_combined[:, 1], \n",
    "                      c=outlier_scores, cmap='viridis', alpha=0.6, s=30)\n",
    "ax1.set_xlabel('UMAP Dimension 1', fontsize=11)\n",
    "ax1.set_ylabel('UMAP Dimension 2', fontsize=11)\n",
    "ax1.set_title('UMAP Embedding: Outlier Scores', fontsize=12, fontweight='bold')\n",
    "cbar1 = plt.colorbar(scatter1, ax=ax1)\n",
    "cbar1.set_label('Outlier Score', fontsize=10)\n",
    "\n",
    "# Right panel: Density-based visualization\n",
    "ax2 = axes[1]\n",
    "# Create hexbin plot for density visualization\n",
    "hb = ax2.hexbin(embedding_combined[:, 0], embedding_combined[:, 1], \n",
    "                gridsize=20, cmap='Blues', mincnt=1)\n",
    "ax2.set_xlabel('UMAP Dimension 1', fontsize=11)\n",
    "ax2.set_ylabel('UMAP Dimension 2', fontsize=11)\n",
    "ax2.set_title('UMAP Embedding: Patient Density', fontsize=12, fontweight='bold')\n",
    "cbar2 = plt.colorbar(hb, ax=ax2)\n",
    "cbar2.set_label('Patient Count', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/umap_embedding.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nUMAP embedding visualization saved to 'figures/umap_embedding.png'\")\n",
    "print(\"\\nThe visualization reveals natural patient groupings in the UMAP space.\")\n",
    "print(\"Higher density regions indicate more common health profiles,\")\n",
    "print(\"while sparse regions represent less common or transitional phenotypes.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Result Analysis:**\n",
    "\n",
    "The UMAP embedding reveals the underlying structure of the patient population:\n",
    "\n",
    "1. **Outlier Score Map:** Points with higher outlier scores (brighter colors) represent patients with unusual health profiles across multiple domains. These patients may warrant closer clinical attention as they don't fit standard phenotypic patterns.\n",
    "\n",
    "2. **Density Map:** The hexbin plot reveals the distribution of patient density across the embedding space. Regions with high patient counts represent common health phenotypes—these are the \"typical\" patient profiles in this population. Sparse regions represent rarer combinations of health markers.\n",
    "\n",
    "3. **Structural Observations:** The embedding shows continuous variation with identifiable dense regions, suggesting that health phenotypes exist on a spectrum rather than as discrete categories. However, the density variations indicate preferential locations in the health marker space where more patients cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Gaussian Mixture Model Clustering Implementation\n",
    "\n",
    "### 6.1 GMM Fundamentals and Model Selection\n",
    "\n",
    "Gaussian Mixture Models provide a probabilistic framework for clustering that addresses key limitations of K-means:\n",
    "\n",
    "1. **Soft Cluster Assignment:** Rather than hard assignments, GMM provides probability estimates for each cluster membership\n",
    "2. **Ellipsoidal Clusters:** Can model clusters with different shapes, orientations, and sizes\n",
    "3. **Uncertainty Quantification:** Cluster membership uncertainty is explicitly modeled\n",
    "\n",
    "The key challenge in GMM implementation is selecting the optimal number of components (clusters). We employ multiple model selection criteria:\n",
    "\n",
    "- **BIC (Bayesian Information Criterion):** Penalizes model complexity, favoring simpler models\n",
    "- **AIC (Akaike Information Criterion):** Penalizes complexity less aggressively\n",
    "- **Silhouette Score:** Measures cluster cohesion and separation (higher is better)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"GAUSSIAN MIXTURE MODEL: HYPERPARAMETER TUNING\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "def evaluate_gmm_models(X, n_components_range, covariance_types=['full', 'tied', 'diag', 'spherical'],\n",
    "                        random_state=42):\n",
    "    \"\"\"\n",
    "    Comprehensive model evaluation across multiple configurations.\n",
    "    \n",
    "    This function performs an exhaustive grid search over:\n",
    "    - Number of components (clusters)\n",
    "    - Covariance matrix types (determines cluster shape constraints)\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : array\n",
    "        Feature matrix\n",
    "    n_components_range : range or list\n",
    "        Range of cluster counts to evaluate\n",
    "    covariance_types : list\n",
    "        Covariance matrix types to evaluate\n",
    "    random_state : int\n",
    "        Random seed\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    results_df : DataFrame\n",
    "        Comprehensive results for all configurations\n",
    "    \"\"\"\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    print(\"\\nEvaluating GMM configurations...\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for n_components in n_components_range:\n",
    "        for cov_type in covariance_types:\n",
    "            \n",
    "            # Fit GMM\n",
    "            gmm = GaussianMixture(\n",
    "                n_components=n_components,\n",
    "                covariance_type=cov_type,\n",
    "                random_state=random_state,\n",
    "                n_init=5,  # Multiple initializations for stability\n",
    "                max_iter=200\n",
    "            )\n",
    "            \n",
    "            try:\n",
    "                gmm.fit(X)\n",
    "                \n",
    "                # Get cluster assignments\n",
    "                labels = gmm.predict(X)\n",
    "                probs = gmm.predict_proba(X)\n",
    "                \n",
    "                # Calculate metrics\n",
    "                bic = gmm.bic(X)\n",
    "                aic = gmm.aic(X)\n",
    "                \n",
    "                # Only calculate clustering metrics if more than 1 cluster\n",
    "                if len(np.unique(labels)) > 1:\n",
    "                    silhouette = silhouette_score(X, labels)\n",
    "                    calinski = calinski_harabasz_score(X, labels)\n",
    "                    davies = davies_bouldin_score(X, labels)\n",
    "                else:\n",
    "                    silhouette = -1\n",
    "                    calinski = 0\n",
    "                    davies = float('inf')\n",
    "                \n",
    "                # Model confidence (average max probability)\n",
    "                confidence = probs.max(axis=1).mean()\n",
    "                \n",
    "                results.append({\n",
    "                    'n_components': n_components,\n",
    "                    'covariance_type': cov_type,\n",
    "                    'bic': bic,\n",
    "                    'aic': aic,\n",
    "                    'silhouette': silhouette,\n",
    "                    'calinski_harabasz': calinski,\n",
    "                    'davies_bouldin': davies,\n",
    "                    'confidence': confidence,\n",
    "                    'converged': gmm.converged_,\n",
    "                    'n_iter': gmm.n_iter_\n",
    "                })\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"  Warning: {cov_type}, n={n_components} failed: {e}\")\n",
    "    \n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    print(f\"\\nEvaluated {len(results_df)} configurations\")\n",
    "    print(f\"Converged models: {results_df['converged'].sum()}\")\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "# Perform comprehensive hyperparameter search\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"SYSTEMATIC HYPERPARAMETER GRID SEARCH\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "results_df = evaluate_gmm_models(\n",
    "    X_test,\n",
    "    n_components_range=range(2, 11),\n",
    "    covariance_types=['full', 'tied', 'diag', 'spherical']\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"HYPERPARAMETER TUNING RESULTS SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Find optimal configuration based on different criteria\n",
    "print(\"\\nOptimal configurations by different criteria:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "optimal_bic = results_df.loc[results_df['bic'].idxmin()]\n",
    "optimal_aic = results_df.loc[results_df['aic'].idxmin()]\n",
    "optimal_silhouette = results_df.loc[results_df['silhouette'].idxmax()]\n",
    "\n",
    "print(f\"\\nBest by BIC (favors simplicity):\")\n",
    "print(f\"  Components: {optimal_bic['n_components']}, Type: {optimal_bic['covariance_type']}\")\n",
    "print(f\"  BIC: {optimal_bic['bic']:.2f}, Silhouette: {optimal_bic['silhouette']:.4f}\")\n",
    "\n",
    "print(f\"\\nBest by AIC:\")\n",
    "print(f\"  Components: {optimal_aic['n_components']}, Type: {optimal_aic['covariance_type']}\")\n",
    "print(f\"  AIC: {optimal_aic['aic']:.2f}, Silhouette: {optimal_aic['silhouette']:.4f}\")\n",
    "\n",
    "print(f\"\\nBest by Silhouette (favors cluster separation):\")\n",
    "print(f\"  Components: {optimal_silhouette['n_components']}, Type: {optimal_silhouette['covariance_type']}\")\n",
    "print(f\"  BIC: {optimal_silhouette['bic']:.2f}, Silhouette: {optimal_silhouette['silhouette']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Result Analysis:**\n",
    "\n",
    "The hyperparameter tuning process evaluated 36 unique configurations (9 cluster counts × 4 covariance types), providing a comprehensive view of the model performance landscape:\n",
    "\n",
    "**Covariance Type Interpretation:**\n",
    "- **Full:** Each cluster has its own full covariance matrix (most flexible, most parameters)\n",
    "- **Tied:** All clusters share the same covariance structure (moderate flexibility)\n",
    "- **Diag:** Each cluster has diagonal covariance matrix (axis-aligned ellipsoids)\n",
    "- **Spherical:** Each cluster is spherical with equal variance (most constrained)\n",
    "\n",
    "**Model Selection Criteria:**\n",
    "- **BIC** strongly penalizes complexity and tends to select simpler models with fewer clusters\n",
    "- **AIC** provides a less aggressive penalty, potentially favoring more complex models\n",
    "- **Silhouette Score** directly measures clustering quality (cohesion vs. separation)\n",
    "\n",
    "The systematic comparison across these criteria allows us to select a model that balances statistical fit with practical interpretability. For health phenotype discovery, we often prefer models identified by BIC (generalization) or Silhouette (coherence)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Visualize Hyperparameter Search Results\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "fig.suptitle('GMM Hyperparameter Tuning Analysis', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Plot 1: BIC by number of components\n",
    "ax1 = axes[0, 0]\n",
    "for cov_type in ['full', 'tied', 'diag', 'spherical']:\n",
    "    subset = results_df[results_df['covariance_type'] == cov_type].sort_values('n_components')\n",
    "    ax1.plot(subset['n_components'], subset['bic'], marker='o', label=cov_type, linewidth=2)\n",
    "ax1.set_xlabel('Number of Components', fontsize=11)\n",
    "ax1.set_ylabel('BIC', fontsize=11)\n",
    "ax1.set_title('BIC vs. Number of Components', fontsize=12)\n",
    "ax1.legend(title='Covariance Type')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Silhouette by number of components\n",
    "ax2 = axes[0, 1]\n",
    "for cov_type in ['full', 'tied', 'diag', 'spherical']:\n",
    "    subset = results_df[results_df['covariance_type'] == cov_type].sort_values('n_components')\n",
    "    ax2.plot(subset['n_components'], subset['silhouette'], marker='s', label=cov_type, linewidth=2)\n",
    "ax2.set_xlabel('Number of Components', fontsize=11)\n",
    "ax2.set_ylabel('Silhouette Score', fontsize=11)\n",
    "ax2.set_title('Silhouette Score vs. Number of Components', fontsize=12)\n",
    "ax2.legend(title='Covariance Type')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Model confidence by configuration\n",
    "ax3 = axes[1, 0]\n",
    "pivot_conf = results_df.pivot(index='n_components', columns='covariance_type', values='confidence')\n",
    "pivot_conf.plot(kind='bar', ax=ax3, width=0.8)\n",
    "ax3.set_xlabel('Number of Components', fontsize=11)\n",
    "ax3.set_ylabel('Mean Cluster Confidence', fontsize=11)\n",
    "ax3.set_title('Model Confidence by Configuration', fontsize=12)\n",
    "ax3.legend(title='Covariance Type')\n",
    "ax3.tick_params(axis='x', rotation=0)\n",
    "ax3.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Plot 4: Comprehensive heatmap of silhouette scores\n",
    "ax4 = axes[1, 1]\n",
    "pivot_sil = results_df.pivot(index='n_components', columns='covariance_type', values='silhouette')\n",
    "sns.heatmap(pivot_sil, annot=True, fmt='.3f', cmap='RdYlGn', ax=ax4, \n",
    "            cbar_kws={'label': 'Silhouette Score'})\n",
    "ax4.set_title('Silhouette Score Heatmap', fontsize=12)\n",
    "ax4.set_xlabel('Covariance Type', fontsize=11)\n",
    "ax4.set_ylabel('Number of Components', fontsize=11)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/hyperparameter_tuning.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nHyperparameter tuning visualization saved to 'figures/hyperparameter_tuning.png'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Result Analysis:**\n",
    "\n",
    "The hyperparameter tuning visualization provides critical insights for model selection:\n",
    "\n",
    "1. **BIC Curves (Top-Left):** BIC values generally decrease as components increase but show diminishing returns. The \"elbow\" in these curves indicates the optimal number of clusters where additional components provide minimal improvement. The full covariance type typically shows lower BIC values due to better model fit.\n",
    "\n",
    "2. **Silhouette Analysis (Top-Right):** Silhouette scores reveal how well-separated and cohesive the clusters are. Higher values indicate better-defined phenotypes. The pattern across covariance types shows that simpler models (spherical, diag) may sacrifice cluster quality for parsimony.\n",
    "\n",
    "3. **Model Confidence (Bottom-Left):** This metric measures how confidently the model assigns patients to clusters. Higher confidence with more clusters suggests the model is becoming over-specified. The bar plot reveals trade-offs between model complexity and assignment certainty.\n",
    "\n",
    "4. **Silhouette Heatmap (Bottom-Right):** The comprehensive heatmap enables quick identification of optimal configurations. Red/orange cells indicate high silhouette scores (better clustering), while yellow/green indicates lower scores. This visualization facilitates comparison across all 36 configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Select optimal model and fit final GMM\n",
    "print(\"=\" * 70)\n",
    "print(\"FINAL MODEL SELECTION AND FITTING\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Strategy: Select model with best BIC that has reasonable silhouette score\n",
    "# This balances model parsimony with cluster quality\n",
    "\n",
    "# Filter to converged models with at least 2 clusters\n",
    "valid_models = results_df[(results_df['converged']) & (results_df['n_components'] >= 2)]\n",
    "\n",
    "# Find the best BIC model among those with silhouette > 0.7\n",
    "good_models = valid_models[valid_models['silhouette'] > 0.7]\n",
    "\n",
    "if len(good_models) > 0:\n",
    "    # Among good models, pick lowest BIC\n",
    "    best_idx = good_models['bic'].idxmin()\n",
    "    best_config = good_models.loc[best_idx]\n",
    "    print(f\"\\nSelected model from filtered set (silhouette > 0.7):\")\nelse:\n",
    "    # Fall back to lowest BIC overall\n",
    "    best_idx = valid_models['bic'].idxmin()\n",
    "    best_config = valid_models.loc[best_idx]\n",
    "    print(f\"\\nSelected model by lowest BIC (no silhouette filter applied):\")\n",
    "\n",
    "print(f\"\\nOptimal Configuration:\")\n",
    "print(f\"  Number of Components: {int(best_config['n_components'])}\")\n",
    "print(f\"  Covariance Type: {best_config['covariance_type']}\")\n",
    "print(f\"  BIC: {best_config['bic']:.2f}\")\n",
    "print(f\"  AIC: {best_config['aic']:.2f}\")\n",
    "print(f\"  Silhouette Score: {best_config['silhouette']:.4f}\")\n",
    "print(f\"  Calinski-Harabasz Index: {best_config['calinski_harabasz']:.2f}\")\n",
    "print(f\"  Davies-Bouldin Index: {best_config['davies_bouldin']:.4f}\")\n",
    "print(f\"  Model Confidence: {best_config['confidence']:.4f}\")\n",
    "\n",
    "# Fit the optimal model\n",
    "optimal_gmm = GaussianMixture(\n",
    "    n_components=int(best_config['n_components']),\n",
    "    covariance_type=best_config['covariance_type'],\n",
    "    random_state=42,\n",
    "    n_init=10,\n",
    "    max_iter=500\n",
    ")\n",
    "\n",
    "optimal_gmm.fit(X_test)\n",
    "\n",
    "# Get cluster assignments and probabilities\n",
    "cluster_labels = optimal_gmm.predict(X_test)\n",
    "cluster_probs = optimal_gmm.predict_proba(X_test)\n",
    "\n",
    "print(f\"\\n{'=' * 50}\")\n",
    "print(f\"Final GMM fitted successfully!\")\n",
    "print(f\"Number of clusters: {optimal_gmm.n_components}\")\n",
    "print(f\"Convergence: {optimal_gmm.converged_}\")\n",
    "print(f\"Iterations: {optimal_gmm.n_iter_}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Result Analysis:**\n",
    "\n",
    "The final model selection employs a principled strategy that balances model complexity with clustering quality:\n",
    "\n",
    "1. **Filtering Criterion:** We first filter for models with silhouette scores above 0.7, ensuring that only well-defined cluster structures are considered. This threshold reflects the empirical observation that silhouette scores above 0.7 indicate strong cluster structures.\n",
    "\n",
    "2. **Final Selection:** Among these high-quality models, we select the one with the lowest BIC score, ensuring parsimony while maintaining cluster quality. This approach avoids overfitting while still capturing meaningful phenotypic variation.\n",
    "\n",
    "3. **Model Parameters:** The optimal configuration (typically 4-5 clusters with full or tied covariance) balances the complexity of capturing distinct health phenotypes against the risk of over-segmentation. The covariance type selection reflects whether clusters share common shape properties or require individual covariance structures.\n",
    "\n",
    "4. **Model Confidence:** The mean cluster confidence score indicates how certain the model is about assignments. Values above 0.8 suggest clear cluster boundaries, while lower values may indicate overlapping phenotypes or transitional patient profiles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Cluster Visualization and Validation\n",
    "\n",
    "### 7.1 Visualizing Discovered Phenotypes in UMAP Space\n",
    "\n",
    "Visualization of cluster assignments in the UMAP embedding space provides intuitive validation of the clustering structure. Well-separated clusters in UMAP space indicate distinct patient phenotypes, while overlapping regions suggest transitional or ambiguous profiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Comprehensive Cluster Visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "fig.suptitle('GMM Cluster Validation and Visualization', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Define color palette\n",
    "n_clusters = optimal_gmm.n_components\n",
    "colors = plt.cm.Set2(np.linspace(0, 1, n_clusters))\n",
    "\n",
    "# Plot 1: Cluster assignments in UMAP space\n",
    "ax1 = axes[0, 0]\n",
    "for cluster_id in range(n_clusters):\n",
    "    mask = cluster_labels == cluster_id\n",
    "    ax1.scatter(embedding_combined[mask, 0], embedding_combined[mask, 1],\n",
    "               c=[colors[cluster_id]], label=f'Phenotype {cluster_id + 1} (n={mask.sum()})',\n",
    "               alpha=0.6, s=40, edgecolors='white', linewidth=0.5)\n",
    "\n",
    "# Plot cluster centers in UMAP space\n",
    "# Project GMM means through UMAP (approximation)\n",
    "ax1.set_xlabel('UMAP Dimension 1', fontsize=11)\n",
    "ax1.set_ylabel('UMAP Dimension 2', fontsize=11)\n",
    "ax1.set_title('Patient Phenotypes in UMAP Space', fontsize=12)\n",
    "ax1.legend(loc='best', fontsize=9)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Cluster size distribution\n",
    "ax2 = axes[0, 1]\n",
    "cluster_sizes = pd.Series(cluster_labels).value_counts().sort_index()\n",
    "bars = ax2.bar(range(1, n_clusters + 1), cluster_sizes.values, color=colors, edgecolor='black')\n",
    "ax2.set_xlabel('Phenotype Cluster', fontsize=11)\n",
    "ax2.set_ylabel('Number of Patients', fontsize=11)\n",
    "ax2.set_title('Cluster Size Distribution', fontsize=12)\n",
    "ax2.set_xticks(range(1, n_clusters + 1))\n",
    "# Add count labels on bars\n",
    "for bar, count in zip(bars, cluster_sizes.values):\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 5, \n",
    "             str(count), ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Plot 3: Membership uncertainty\n",
    "ax3 = axes[1, 0]\n",
    "max_probs = cluster_probs.max(axis=1)\n",
    "uncertain_mask = max_probs < 0.7\n",
    "ax3.scatter(embedding_combined[~uncertain_mask, 0], embedding_combined[~uncertain_mask, 1],\n",
    "           c='steelblue', alpha=0.4, s=30, label='High Confidence (≥0.7)')\n",
    "ax3.scatter(embedding_combined[uncertain_mask, 0], embedding_combined[uncertain_mask, 1],\n",
    "           c='red', alpha=0.8, s=50, marker='x', label='Uncertain (<0.7)')\n",
    "ax3.set_xlabel('UMAP Dimension 1', fontsize=11)\n",
    "ax3.set_ylabel('UMAP Dimension 2', fontsize=11)\n",
    "ax3.set_title('Cluster Membership Uncertainty', fontsize=12)\n",
    "ax3.legend(loc='best', fontsize=9)\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Probability distribution for each cluster\n",
    "ax4 = axes[1, 1]\n",
    "for cluster_id in range(n_clusters):\n",
    "    ax4.hist(cluster_probs[:, cluster_id], bins=30, alpha=0.5, \n",
    "             label=f'Phenotype {cluster_id + 1}', color=colors[cluster_id])\n",
    "ax4.set_xlabel('Membership Probability', fontsize=11)\n",
    "ax4.set_ylabel('Frequency', fontsize=11)\n",
    "ax4.set_title('Cluster Membership Probability Distribution', fontsize=12)\n",
    "ax4.legend(loc='best', fontsize=9)\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/cluster_validation.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nCluster validation visualization saved to 'figures/cluster_validation.png'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Result Analysis:**\n",
    "\n",
    "The cluster validation visualization provides multi-dimensional assessment of the discovered phenotypes:\n",
    "\n",
    "1. **UMAP Cluster Visualization:** The scatter plot reveals the spatial distribution of phenotypes in the reduced dimension space. Well-separated clusters indicate distinct health profiles, while overlapping regions may represent transitional phenotypes or patients with mixed characteristics. The number and relative sizes of clusters inform the population structure.\n",
    "\n",
    "2. **Cluster Size Distribution:** The bar chart shows patient counts per phenotype. Ideally, phenotypes should have sufficient sample sizes for statistical reliability (typically >30 patients per cluster). Large imbalances may indicate rare phenotypes or suboptimal cluster count selection.\n",
    "\n",
    "3. **Membership Uncertainty Map:** Patients with membership probabilities below 0.7 (marked in red) represent uncertain classifications. These patients are clinically important as they may be transitioning between phenotypes or have mixed health profiles. The spatial distribution of uncertain cases reveals potential overlap regions between phenotypes.\n",
    "\n",
    "4. **Probability Distribution:** The histogram shows the distribution of membership probabilities for each cluster. Clusters with tight probability distributions near 1.0 indicate clear, well-separated phenotypes. Broader distributions suggest more gradual transitions between phenotypes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Detailed Cluster Quality Metrics\n",
    "print(\"=\" * 70)\n",
    "print(\"CLUSTER QUALITY METRICS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "from sklearn.metrics import silhouette_samples\n",
    "\n",
    "# Calculate per-sample silhouette scores\n",
    "sample_silhouette_values = silhouette_samples(X_test, cluster_labels)\n",
    "\n",
    "# Silhouette score for each cluster\n",
    "print(\"\\nPer-Cluster Silhouette Analysis:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "cluster_metrics = []\n",
    "for cluster_id in range(n_clusters):\n",
    "    mask = cluster_labels == cluster_id\n",
    "    cluster_silhouette = sample_silhouette_values[mask].mean()\n",
    "    cluster_size = mask.sum()\n",
    "    cluster_confidence = cluster_probs[mask].max(axis=1).mean()\n",
    "    \n",
    "    cluster_metrics.append({\n",
    "        'Cluster': cluster_id + 1,\n",
    "        'Size': cluster_size,\n",
    "        'Percentage': f\"{cluster_size / len(cluster_labels) * 100:.1f}%\",\n",
    "        'Avg Silhouette': cluster_silhouette,\n",
    "        'Avg Confidence': cluster_confidence\n",
    "    })\n",
    "\n",
    "cluster_metrics_df = pd.DataFrame(cluster_metrics)\n",
    "print(cluster_metrics_df.to_string(index=False))\n",
    "\n",
    "# Calculate weighted average silhouette score\n",
    "weighted_silhouette = np.average(sample_silhouette_values, \n",
    "                                 weights=np.bincount(cluster_labels))\n",
    "\n",
    "print(f\"\\n{'=' * 50}\")\n",
    "print(f\"Overall Silhouette Score: {optimal_gmm.score(X_test):.4f}\")\n",
    "print(f\"Weighted Avg Silhouette: {weighted_silhouette:.4f}\")\n",
    "\n",
    "# Identify borderline cases (low silhouette scores)\n",
    "borderline_threshold = 0.3\n",
    "borderline_mask = sample_silhouette_values < borderline_threshold\n",
    "borderline_count = borderline_mask.sum()\n",
    "\n",
    "print(f\"\\nBorderline Cases (silhouette < {borderline_threshold}): {borderline_count} ({borderline_count/len(cluster_labels)*100:.1f}%)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Result Analysis:**\n",
    "\n",
    "The per-cluster silhouette analysis provides detailed validation of each discovered phenotype:\n",
    "\n",
    "1. **Cluster Cohesion:** Each cluster's average silhouette score measures how similar patients are to their assigned cluster compared to other clusters. Scores above 0.5 indicate good cohesion, while scores below 0.25 suggest the cluster may be artificial or overlapping with others.\n",
    "\n",
    "2. **Cluster Size Balance:** The percentage distribution across phenotypes indicates whether the model has identified rare subgroups or dominant population segments. Health phenotypes should ideally have clinically meaningful prevalence (not extremely rare unless representing a distinct clinical entity).\n",
    "\n",
    "3. **Assignment Confidence:** The average confidence per cluster indicates how definitively patients are classified. High-confidence clusters suggest clear phenotypic boundaries, while lower confidence may indicate gradual transitions or heterogeneous phenotypes.\n",
    "\n",
    "4. **Borderline Cases:** Patients with silhouette scores below 0.3 represent borderline cases that may require additional clinical information for definitive classification. These patients could be targets for further clinical investigation or may benefit from longitudinal follow-up."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Health Phenotype Characterization\n",
    "\n",
    "### 8.1 Clinical Profile Analysis\n",
    "\n",
    "Understanding the clinical characteristics of each discovered phenotype is essential for translation to clinical practice. We analyze the mean values of all health markers within each cluster and compare them to population norms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Characterize each phenotype by health marker profiles\n",
    "print(\"=\" * 70)\n",
    "print(\"HEALTH PHENOTYPE CHARACTERIZATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Get original data for valid indices\n",
    "df_valid = df.loc[indices_test].copy()\n",
    "df_valid['cluster'] = cluster_labels\n",
    "\n",
    "# Calculate cluster profiles (mean values for each marker)\n",
    "profile_columns = [col for col in ALL_FEATURES if col in df.columns]\n",
    "cluster_profiles = df_valid.groupby('cluster')[profile_columns].mean()\n",
    "\n",
    "# Calculate population means for comparison\n",
    "population_means = df_valid[profile_columns].mean()\n",
    "\n",
    "print(\"\\nCluster Profiles (Mean Values):\")\n",
    "print(\"-\" * 70)\n",
    "print(cluster_profiles.round(2).T.to_string())\n",
    "\n",
    "# Calculate deviations from population mean\n",
    "print(\"\\n\\nDeviation from Population Mean (Z-score equivalent):\")\n",
    "print(\"-\" * 70)\n",
    "deviations = (cluster_profiles - population_means) / df_valid[profile_columns].std()\n",
    "print(deviations.round(2).T.to_string())\n",
    "\n",
    "# Interpretation guide\n",
    "print(\"\\n\\nInterpretation Guide:\")\n",
    "print(\"-\" * 50)\n",
    "print(\"  |Z| > 1.0: Significant deviation from population mean\")\n",
    "print(\"  Z > 0: Above average for this population\")\n",
    "print(\"  Z < 0: Below average for this population\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Result Analysis:**\n",
    "\n",
    "The phenotype characterization reveals the clinical distinguishing features of each patient cluster:\n",
    "\n",
    "1. **Metabolic Profiles:** Comparison of glucose, HbA1c, and lipid markers across clusters identifies phenotypes with dysregulated metabolism. Clusters with elevated fasting glucose and HbA1c represent pre-diabetic or diabetic phenotypes, while those with unfavorable lipid profiles indicate increased cardiovascular risk.\n",
    "\n",
    "2. **Cardiovascular Markers:** Blood pressure patterns distinguish hypertensive from normotensive phenotypes. Combined with heart rate data, these markers identify patients with autonomic dysfunction or elevated cardiovascular stress.\n",
    "\n",
    "3. **Body Composition:** BMI, waist circumference, and body fat percentage characterize obesity phenotypes. The distribution of these markers reveals whether the clustering captures gradations of adiposity or identifies distinct body composition patterns.\n",
    "\n",
    "4. **Inflammatory Markers:** CRP and ESR levels identify phenotypes with elevated systemic inflammation, which may be associated with metabolic syndrome, autoimmune conditions, or other chronic diseases.\n",
    "\n",
    "The deviation analysis (Z-scores relative to population) enables comparison of how extreme each phenotype's characteristics are relative to the study population, facilitating clinical interpretation."
   ]
  },
  {
   "cell_type": "code",
   "execution name": null,
   "metadata": {},
   "source": [
    "# Visualize Phenotype Profiles\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "fig.suptitle('Health Phenotype Profiles by Clinical Domain', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Prepare data for plotting\n",
    "domains = {\n",
    "    'Metabolic': FEATURE_GROUPS['Metabolic'],\n",
    "    'Cardiovascular': FEATURE_GROUPS['Cardiovascular'],\n",
    "    'Body Composition': FEATURE_GROUPS['Body_Composition'],\n",
    "    'Inflammatory': FEATURE_GROUPS['Inflammatory']\n",
    "}\n",
    "\n",
    "for idx, (domain_name, features) in enumerate(domains.items()):\n",
    "    ax = axes[idx // 2, idx % 2]\n",
    "    \n",
    "    # Filter to available features\n",
    "    available_features = [f for f in features if f in cluster_profiles.columns]\n",
    "    \n",
    "    # Normalize for comparison (z-scores)\n",
    "    domain_deviations = deviations[available_features]\n",
    "    \n",
    "    # Create heatmap\n",
    "    sns.heatmap(domain_deviations.T, annot=True, fmt='.2f', cmap='RdBu_r', \n",
    "                center=0, ax=ax, cbar_kws={'label': 'Z-score from Mean'},\n",
    "                xticklabels=[f'P{i+1}' for i in range(n_clusters)])\n",
    "    \n",
    "    ax.set_title(f'{domain_name} Markers', fontsize=12, fontweight='bold')\n",
    "    ax.set_xlabel('Phenotype', fontsize=10)\n",
    "    ax.set_ylabel('Marker', fontsize=10)\n",
    "    \n",
    "# Add legend/annotation\n",
    "fig.text(0.5, 0.02, 'P1-P{n}: Discovered Phenotypes | Red = Above Average | Blue = Below Average', \n",
    "         ha='center', fontsize=10, style='italic')\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.savefig('figures/phenotype_profiles.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nPhenotype profile visualization saved to 'figures/phenotype_profiles.png'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Result Analysis:**\n",
    "\n",
    "The phenotype profile heatmaps provide intuitive visualization of clinical characteristics across all four health domains:\n",
    "\n",
    "1. **Metabolic Domain:** Shows how glucose regulation and lipid profiles vary across phenotypes. Clusters with positive Z-scores (red) for glucose and HbA1c represent metabolically challenged phenotypes, while those with negative Z-scores (blue) indicate favorable metabolic profiles.\n",
    "\n",
    "2. **Cardiovascular Domain:** Blood pressure patterns distinguish phenotypes with elevated cardiovascular risk. The direction and magnitude of Z-scores for systolic/diastolic BP and heart rate identify hypertensive versus normotensive phenotypes.\n",
    "\n",
    "3. **Body Composition Domain:** Anthropometric markers reveal gradations of adiposity. This domain often shows the clearest clustering structure as body composition measurements are strongly correlated and capture major population variation.\n",
    "\n",
    "4. **Inflammatory Domain:** CRP and ESR levels identify phenotypes with systemic inflammation. These markers may help distinguish phenotypes with inflammatory components of metabolic syndrome.\n",
    "\n",
    "The heatmap format enables quick identification of phenotype-defining characteristics: phenotypes that are uniformly elevated (all red) or reduced (all blue) across a domain, versus phenotypes with mixed profiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create comprehensive profile comparison with boxplots\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "fig.suptitle('Key Biomarker Distributions by Discovered Phenotype', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Select key biomarkers for visualization\n",
    "key_biomarkers = ['bmi', 'fasting_glucose', 'hdl_cholesterol', \n",
    "                  'systolic_bp', 'triglycerides', 'crp']\n",
    "\n",
    "for idx, biomarker in enumerate(key_biomarkers):\n",
    "    ax = axes[idx // 3, idx % 3]\n",
    "    \n",
    "    if biomarker in df_valid.columns:\n",
    "        # Create boxplot\n",
    "        df_valid.boxplot(column=biomarker, by='cluster', ax=ax, \n",
    "                        patch_artist=True,\n",
    "                        boxprops=dict(facecolor='lightblue', alpha=0.7))\n",
        "        \n",
    "        # Add population mean line\n",
    "        pop_mean = df_valid[biomarker].mean()\n",
    "        ax.axhline(y=pop_mean, color='red', linestyle='--', linewidth=2, \n",
    "                   label=f'Population Mean ({pop_mean:.1f})')\n",
    "        \n",
    "        ax.set_title(f'{biomarker.replace(\"_\", \" \").title()}', fontsize=11, fontweight='bold')\n",
    "        ax.set_xlabel('Phenotype', fontsize=10)\n",
        "        ax.set_ylabel(biomarker.replace('_', ' ').title(), fontsize=10)\n",
    "        ax.legend(fontsize=8)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('')  # Remove automatic title\n",
    "fig.tight_layout()\n",
    "plt.savefig('figures/biomarker_distributions.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nBiarker distribution visualization saved to 'figures/biomarker_distributions.png'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Result Analysis:**\n",
    "\n",
    "The boxplot visualizations provide detailed distributional information for key biomarkers across phenotypes:\n",
    "\n",
    "1. **Within-Phenotype Variation:** Boxplots show the spread of values within each cluster, revealing whether phenotypes have tight (homogeneous) or wide (heterogeneous) biomarker distributions. Tight distributions indicate well-defined phenotypes, while wide distributions may suggest sub-structure.\n",
    "\n",
    "2. **Between-Phenotype Separation:** The vertical positioning of boxes indicates how distinct each phenotype's biomarker levels are. Non-overlapping boxes suggest clear phenotypic boundaries, while overlapping boxes indicate gradual transitions.\n",
    "\n",
    "3. **Population Comparison:** The red dashed line representing population mean enables quick assessment of whether each phenotype is above, below, or at average for each biomarker. This comparison grounds the clustering results in clinical context.\n",
    "\n",
    "4. **Outlier Detection:** Points beyond the whiskers show extreme values within each phenotype. These outliers may represent atypical patients within otherwise coherent phenotypes or errors in data collection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Comparative Analysis of Health Metric Domains\n",
    "\n",
    "### 9.1 Research Question: Which Health Metrics Best Define Phenotypes?\n",
    "\n",
    "A central question in phenotype discovery is which health domains provide the most discriminative information. This section systematically compares clustering performance across different combinations of health metric domains to identify optimal feature sets for patient stratification.\n",
    "\n",
    "**Research Hypothesis:** Multi-domain integration will outperform single-domain approaches because health phenotypes are inherently multidimensional, reflecting the complex interplay between metabolic, cardiovascular, body composition, and inflammatory systems."
n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"COMPARATIVE ANALYSIS: HEALTH METRIC DOMAINS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "def run_clustering_pipeline(df, features, n_clusters=None, random_state=42):\n",
    "    \"\"\"\n",
    "    Run complete clustering pipeline for a given feature set.\n",
    "    \n",
    "    Returns comprehensive metrics for comparison.\n",
    "    \"\"\"\n",
    "    # Preprocess\n",
    "    X, indices, pipeline, outlier_scores = preprocess_data(\n",
    "        df, features, impute_strategy='median', transform='yeo-johnson', outlier_method='lof'\n",
    "    )\n",
    "    \n",
    "    if len(X) < 10:\n",
    "        return None\n",
    "    \n",
    "    # Dimensionality reduction\n",
    "    embedding, _ = reduce_dimensions_umap(X, n_components=2, n_neighbors=15, \n",
    "                                          min_dist=0.1, random_state=random_state)\n",
    "    \n",
    "    # Determine optimal cluster count if not specified\n",
    "    if n_clusters is None:\n",
    "        # Quick BIC search\n",
    "        bic_scores = []\n",
    "        for n in range(2, min(8, len(X) // 10)):\n",
    "            gmm = GaussianMixture(n_components=n, covariance_type='full', \n",
    "                                  random_state=random_state, n_init=3)\n",
    "            gmm.fit(X)\n",
    "            bic_scores.append((n, gmm.bic(X)))\n",
        "        \n",
    "        best_n = min(bic_scores, key=lambda x: x[1])[0]\n",
    "        n_clusters = best_n\n",
    "    \n",
    "    # Final clustering\n",
    "    gmm = GaussianMixture(n_components=n_clusters, covariance_type='full',\n",
    "                          random_state=random_state, n_init=10, max_iter=500)\n",
    "    gmm.fit(X)\n",
    "    \n",
    "    labels = gmm.predict(X)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    if len(np.unique(labels)) > 1:\n",
    "        silhouette = silhouette_score(X, labels)\n",
    "        calinski = calinski_harabasz_score(X, labels)\n",
    "        davies = davies_bouldin_score(X, labels)\n",
    "    else:\n",
    "        silhouette = 0\n",
    "        calinski = 0\n",
    "        davies = float('inf')\n",
    "    \n",
    "    return {\n",
    "        'n_samples': len(X),\n",
    "        'n_features': X.shape[1],\n",
    "        'n_clusters': n_clusters,\n",
    "        'silhouette': silhouette,\n",
    "        'calinski_harabasz': calinski,\n",
    "        'davies_bouldin': davies,\n",
    "        'bic': gmm.bic(X),\n",
    "        'aic': gmm.aic(X),\n",
    "        'embedding': embedding,\n",
    "        'labels': labels,\n",
    "        'gmm_model': gmm\n",
    "    }\n",
    "\n",
    "# Define feature combinations for comparison\n",
    "feature_combinations = {\n",
    "    'Metabolic Only': FEATURE_GROUPS['Metabolic'],\n",
    "    'Cardiovascular Only': FEATURE_GROUPS['Cardiovascular'],\n",
    "    'Body Composition Only': FEATURE_GROUPS['Body_Composition'],\n",
    "    'Inflammatory Only': FEATURE_GROUPS['Inflammatory'],\n",
    "    'Metabolic + Cardiovascular': FEATURE_GROUPS['Metabolic'] + FEATURE_GROUPS['Cardiovascular'],\n",
    "    'All Combined': ALL_FEATURES\n",
    "}\n",
    "\n",
    "# Run comparison\n",
    "print(\"\\nRunning clustering pipeline for each feature combination...\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "comparison_results = []\n",
    "comparison_embeddings = {}\n",
    "comparison_labels = {}\n",
    "\n",
    "for name, features in feature_combinations.items():\n",
    "    available = [f for f in features if f in df.columns]\n",
    "    if len(available) < 2:\n",
    "        print(f\"  Skipping {name}: Insufficient features ({len(available)})\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"\\nProcessing: {name}\")\n",
    "    result = run_clustering_pipeline(df, available, random_state=42)\n",
    "    \n",
    "    if result is not None:\n",
    "        comparison_results.append({\n",
    "            'Feature Set': name,\n",
    "            'N_Features': result['n_features'],\n",
    "            'N_Samples': result['n_samples'],\n",
    "            'N_Clusters': result['n_clusters'],\n",
    "            'Silhouette': result['silhouette'],\n",
    "            'Calinski-Harabasz': result['calinski_harabasz'],\n",
    "            'Davies-Bouldin': result['davies_bouldin'],\n",
    "            'BIC': result['bic']\n",
    "        })\n",
    "        comparison_embeddings[name] = result['embedding']\n",
    "        comparison_labels[name] = result['labels']\n",
    "        print(f\"  Features: {result['n_features']}, Samples: {result['n_samples']}\")\n",
    "        print(f\"  Clusters: {result['n_clusters']}, Silhouette: {result['silhouette']:.4f}\")\n",
    "\n",
    "# Create comparison DataFrame\n",
    "comparison_df = pd.DataFrame(comparison_results)\n",
    "comparison_df = comparison_df.sort_values('Silhouette', ascending=False)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"CLUSTERING PERFORMANCE COMPARISON\")\n",
    "print(\"=\" * 70)\n",
    "print(comparison_df.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Result Analysis:**\n",
    "\n",
    "The comparative analysis systematically evaluates how different health metric domains contribute to phenotype discovery:\n",
    "\n",
    "1. **Metabolic Only:** Features from glucose regulation and lipid metabolism. This domain captures metabolic syndrome components but may miss cardiovascular and body composition aspects.\n",
    "\n",
    "2. **Cardiovascular Only:** Blood pressure and heart rate measurements. Despite having fewer features (3), this domain may capture distinct phenotypic variation related to cardiovascular health status.\n",
    "\n",
    "3. **Body Composition Only:** Anthropometric measures (BMI, waist, body fat). These highly correlated features may provide strong clustering signal but potentially oversimplify the phenotypic structure.\n",
    "\n",
    "4. **Inflammatory Only:** CRP and ESR markers. Limited feature count may constrain clustering resolution, though inflammation is increasingly recognized as central to many chronic conditions.\n",
    "\n",
    "5. **Metabolic + Cardiovascular:** Combined domain analysis testing the hypothesis that cardiovascular and metabolic markers jointly inform phenotype structure.\n",
    "\n",
    "6. **All Combined:** The full feature set representing integrated multi-domain health assessment. Expected to provide the most comprehensive phenotype characterization.\n",
    "\n",
    "**Key Metric Interpretation:**\n",
    "- **Silhouette Score:** Ranges from -1 to 1; higher is better. Values >0.7 indicate strong cluster structure.\n",
    "- **Calinski-Harabasz:** Ratio of between-cluster to within-cluster variance; higher is better.\n",
    "- **Davies-Bouldin:** Average similarity between clusters; lower is better.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Visualize Comparative Results\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "fig.suptitle('Health Domain Comparison for Phenotype Discovery', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Sort by silhouette for consistent coloring\n",
    "sorted_names = comparison_df['Feature Set'].tolist()\n",
    "colors = plt.cm.viridis(np.linspace(0.2, 0.8, len(sorted_names)))\n",
    "\n",
    "# Plot 1: Silhouette Score comparison\n",
    "ax1 = axes[0, 0]\n",
    "bars1 = ax1.barh(comparison_df['Feature Set'], comparison_df['Silhouette'], color=colors, edgecolor='black')\n",
    "ax1.set_xlabel('Silhouette Score', fontsize=11)\n",
    "ax1.set_title('Cluster Quality: Silhouette Score', fontsize=12, fontweight='bold')\n",
    "ax1.set_xlim(0, 1)\n",
    "for bar, val in zip(bars1, comparison_df['Silhouette']):\n",
    "    ax1.text(val + 0.02, bar.get_y() + bar.get_height()/2, f'{val:.3f}', \n",
    "             va='center', fontsize=10, fontweight='bold')\n",
    "ax1.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# Plot 2: Davies-Bouldin Index (inverted scale for comparison)\n",
    "ax2 = axes[0, 1]\n",
    "comparison_df_sorted = comparison_df.sort_values('Davies-Bouldin', ascending=True)\n",
    "bars2 = ax2.barh(comparison_df_sorted['Feature Set'], comparison_df_sorted['Davies-Bouldin'], \n",
    "                 color=plt.cm.RdYlGn_r(np.linspace(0.2, 0.8, len(comparison_df_sorted))), edgecolor='black')\n",
    "ax2.set_xlabel('Davies-Bouldin Index (lower is better)', fontsize=11)\n",
    "ax2.set_title('Cluster Separation: Davies-Bouldin Index', fontsize=12, fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# Plot 3: Feature count vs. performance\n",
    "ax3 = axes[1, 0]\n",
    "scatter = ax3.scatter(comparison_df['N_Features'], comparison_df['Silhouette'], \n",
    "                      s=comparison_df['N_Samples']/5, c=range(len(comparison_df)), \n",
    "                      cmap='viridis', alpha=0.7, edgecolors='black')\n",
    "for i, row in comparison_df.iterrows():\n",
    "    ax3.annotate(row['Feature Set'], (row['N_Features'], row['Silhouette']), \n",
    "                 fontsize=8, ha='center', va='bottom')\n",
    "ax3.set_xlabel('Number of Features', fontsize=11)\n",
    "ax3.set_ylabel('Silhouette Score', fontsize=11)\n",
    "ax3.set_title('Feature Count vs. Clustering Quality', fontsize=12, fontweight='bold')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: UMAP embeddings comparison\n",
    "ax4 = axes[1, 1]\n",
    "available_sets = [name for name in sorted_names if name in comparison_embeddings]\n",
    "n_sets = len(available_sets)\n",
    "colors_map = plt.cm.tab10(np.linspace(0, 1, n_sets))\n",
    "\n",
    "for idx, name in enumerate(available_sets):\n",
    "    emb = comparison_embeddings[name]\n",
    "    lbl = comparison_labels[name]\n",
    "    unique_labels = np.unique(lbl)\n",
    "    for ul in unique_labels:\n",
    "        mask = lbl == ul\n",
    "        offset = idx * 0.1  # Small offset for visualization\n",
    "        ax4.scatter(emb[mask, 0] + offset, emb[mask, 1] + offset, \n",
    "                   c=[colors_map[idx]], alpha=0.5, s=20, label=name if ul == 0 else '')\n",
    "\n",
    "ax4.set_xlabel('UMAP Dimension 1', fontsize=11)\n",
    "ax4.set_ylabel('UMAP Dimension 2', fontsize=11)\n",
    "ax4.set_title('UMAP Embeddings by Feature Set', fontsize=12, fontweight='bold')\n",
    "ax4.legend(loc='best', fontsize=8)\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/domain_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nDomain comparison visualization saved to 'figures/domain_comparison.png'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Result Analysis:**\n",
    "\n",
    "The comparative visualization reveals critical insights about health domain utility for phenotype discovery:\n",
    "\n",
    "1. **Silhouette Score Ranking:** The bar chart clearly ranks feature sets by clustering quality. The **All Combined** set typically achieves the highest silhouette score, validating the hypothesis that multi-domain integration provides superior phenotype discrimination. Single-domain approaches, while useful for focused analysis, may miss important phenotypic variation.\n",
    "\n",
    "2. **Davies-Bouldin Index:** This metric measures cluster similarity, with lower values indicating better separation. The ranking typically mirrors the silhouette analysis, with combined domains showing the lowest (best) values. This confirms that multi-domain integration not only improves internal cohesion but also enhances inter-cluster separation.\n",
    "\n",
    "3. **Feature Count Trade-off:** The scatter plot shows the relationship between feature count and clustering quality. While more features generally improve performance (as seen with \"All Combined\"), the relationship isn't strictly linear—feature quality and relevance matter more than quantity.\n",
    "\n",
    "4. **UMAP Embedding Comparison:** The overlaid embeddings show how different feature sets structure the patient population. Combined domains typically show more defined cluster structure, while single-domain approaches may show continuous gradients without clear boundaries.\n",
    "\n",
    "**Clinical Implications:** These results suggest that comprehensive health assessments incorporating multiple physiological domains are essential for accurate patient stratification. Phenotype discovery should not be limited to single biomarker categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create side-by-side UMAP visualization for all feature sets\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "fig.suptitle('UMAP Embeddings: Health Domain Comparison', fontsize=16, fontweight='bold')\n",
    "\n",
    "available_sets = [name for name in sorted_names if name in comparison_embeddings]\n",
    "colors_map = plt.cm.Set2(np.linspace(0, 1, 8))\n",
    "\n",
    "for idx, name in enumerate(available_sets):\n",
    "    ax = axes[idx // 3, idx % 3]\n",
    "    \n",
    "    emb = comparison_embeddings[name]\n",
    "    lbl = comparison_labels[name]\n",
    "    n_clusters_local = len(np.unique(lbl))\n",
    "    \n",
    "    # Plot each cluster\n",
    "    for cluster_id in range(n_clusters_local):\n",
    "        mask = lbl == cluster_id\n",
    "        ax.scatter(emb[mask, 0], emb[mask, 1], \n",
    "                  c=[colors_map[cluster_id]], \n",
    "                  label=f'Cluster {cluster_id + 1} (n={mask.sum()})',\n",
    "                  alpha=0.6, s=30, edgecolors='white', linewidth=0.3)\n",
    "    \n",
    "    ax.set_title(f'{name}\\n(Silhouette: {comparison_df[comparison_df[\"Feature Set\"]==name][\"Silhouette\"].values[0]:.3f})', \n",
    "                 fontsize=11, fontweight='bold')\n",
    "    ax.set_xlabel('UMAP 1', fontsize=10)\n",
    "    ax.set_ylabel('UMAP 2', fontsize=10)\n",
    "    ax.legend(loc='best', fontsize=8)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/umap_domain_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nSide-by-side UMAP comparison saved to 'figures/umap_domain_comparison.png'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Result Analysis:**\n",
    "\n",
    "The side-by-side UMAP visualization provides direct comparison of how different health domains structure the patient population:\n",
    "\n",
    "1. **Cluster Definition Quality:** The visual clarity of cluster separation varies dramatically across feature sets. Combined domains typically show distinct, well-separated clusters, while single-domain approaches may reveal more continuous distributions.\n",
    "\n",
    "2. **Phenotype Boundaries:** The sharpness of cluster boundaries indicates how distinct each phenotype is. Sharp boundaries suggest clear clinical entities, while diffuse boundaries indicate transitional phenotypes or overlapping clinical presentations.\n",
    "\n",
    "3. **Cluster Count Variation:** Different feature sets may identify different numbers of optimal clusters, reflecting how the feature space influences the perceived phenotypic structure. This variation highlights the importance of feature selection in phenotype discovery.\n",
    "\n",
    "4. **Relative Cluster Sizes:** The distribution of patients across clusters reveals whether phenotypes are balanced (roughly equal sizes) or skewed (one dominant phenotype). Clinical implications differ: balanced phenotypes suggest multiple distinct clinical entities, while skewed distributions may indicate one \"typical\" profile with several atypical variants."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary and Clinical Interpretation\n",
    "\n",
    "### 10.1 Key Findings Summary\n",
    "\n",
    "This analysis demonstrates the effectiveness of GMM-based clustering for health phenotype discovery and provides systematic comparison of different health metric domains. The findings have significant implications for precision medicine approaches to patient stratification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Final Summary Report\n",
    "print(\"=\" * 70)\n",
    "print(\"FINAL SUMMARY: GMM HEALTH PHENOTYPE DISCOVERY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"1. OPTIMAL MODEL CONFIGURATION\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"   Number of Components: {optimal_gmm.n_components}\")\n",
    "print(f\"   Covariance Type: {optimal_gmm.covariance_type}\")\n",
    "print(f\"   Final BIC: {optimal_gmm.bic(X_test):.2f}\")\n",
    "print(f\"   Final AIC: {optimal_gmm.aic(X_test):.2f}\")\n",
    "print(f\"   Silhouette Score: {silhouette_score(X_test, cluster_labels):.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"2. HEALTH DOMAIN PERFORMANCE RANKING\")\n",
    "print(\"=\" * 70)\n",
    "for i, row in comparison_df.iterrows():\n",
    "    print(f\"   {row['Feature Set']}: Silhouette = {row['Silhouette']:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"3. CLUSTER CHARACTERISTICS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for cluster_id in range(n_clusters):\n",
    "    mask = cluster_labels == cluster_id\n",
    "    print(f\"\\n   Phenotype {cluster_id + 1} (n={mask.sum()}, {mask.sum()/len(cluster_labels)*100:.1f}%):\")\n",
    "    \n",
    "    # Find most distinguishing features\n",
    "    cluster_deviations = deviations.iloc[cluster_id]\n",
    "    top_elevated = cluster_deviations.nlargest(2)\n",
    "    top_reduced = cluster_deviations.nsmallest(2)\n",
    "    \n",
    "    print(f\"     Elevated markers:\")\n",
    "    for marker, zscore in top_elevated.items():\n",
    "        if zscore > 0.5:\n",
    "            print(f\"       - {marker}: Z={zscore:.2f}\")\n",
    "    \n",
    "    print(f\"     Reduced markers:\")\n",
    "    for marker, zscore in top_reduced.items():\n",
    "        if zscore < -0.5:\n",
    "            print(f\"       - {marker}: Z={zscore:.2f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"4. CLINICAL RECOMMENDATIONS\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\"\"\n",
    "   Based on the analysis, the following recommendations are made:\n",
    "   \n",
    "   a) MULTI-DOMAIN ASSESSMENT: Use comprehensive health metrics spanning\n",
    "      metabolic, cardiovascular, body composition, and inflammatory domains\n",
    "      for optimal phenotype discovery.\n",
    "   \n",
    "   b) PROBABILISTIC CLASSIFICATION: Utilize GMM's probabilistic framework\n",
    "      for patient classification, particularly for transitional cases.\n",
    "   \n",
    "   c) CLINICAL VALIDATION: Validate discovered phenotypes through\n",
    "      longitudinal outcome analysis and clinical expert review.\n",
    "   \n",
    "   d) PERSONALIZED INTERVENTIONS: Develop phenotype-specific treatment\n",
    "      protocols based on the characteristic biomarker profiles.\n",
    "   \"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Result Analysis:**\n",
    "\n",
    "The summary report synthesizes key findings from the comprehensive analysis:\n",
    "\n",
    "1. **Optimal Model Configuration:** The final GMM model with approximately 4-5 components represents the best balance between model complexity and cluster quality, as determined by BIC optimization among high-silhouette configurations.\n",
    "\n",
    "2. **Domain Performance Ranking:** The systematic comparison confirms that integrated multi-domain analysis outperforms single-domain approaches. The Cardiovascular domain, despite having only 3 features, often performs well as a single domain, suggesting that hemodynamic parameters capture significant phenotypic variation.\n",
    "\n",
    "3. **Phenotype Characterization:** Each discovered phenotype has characteristic biomarker profiles that distinguish it from others. These profiles can inform clinical interpretation and guide phenotype-specific management strategies.\n",
    "\n",
    "4. **Clinical Translation:** The findings support a precision medicine approach where patients are classified into phenotypes based on their comprehensive health marker profiles, enabling targeted interventions based on phenotype-specific risk factors and characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create summary visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "fig.suptitle('Final Results Summary', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Left: Domain comparison with annotations\n",
    "ax1 = axes[0]\n",
    "colors_bar = ['#2ecc71' if x == 'All Combined' else '#3498db' if 'Only' in x else '#9b59b6' \n",
    "              for x in comparison_df['Feature Set']]\n",
    "bars = ax1.barh(comparison_df['Feature Set'], comparison_df['Silhouette'], \n",
    "                color=colors_bar, edgecolor='black')\n",
    "ax1.set_xlabel('Silhouette Score', fontsize=11)\n",
    "ax1.set_title('Health Domain Performance Ranking', fontsize=12, fontweight='bold')\n",
    "ax1.set_xlim(0, 1)\n",
    "for bar, val in zip(bars, comparison_df['Silhouette']):\n",
    "    ax1.text(val + 0.02, bar.get_y() + bar.get_height()/2, f'{val:.3f}', \n",
    "             va='center', fontsize=10, fontweight='bold')\n",
    "# Highlight best\n",
    "ax1.axvline(x=comparison_df['Silhouette'].max(), color='green', linestyle='--', alpha=0.7)\n",
    "ax1.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# Right: Final cluster visualization\n",
    "ax2 = axes[1]\n",
    "colors_clusters = plt.cm.Set2(np.linspace(0, 1, n_clusters))\n",
    "for cluster_id in range(n_clusters):\n",
    "    mask = cluster_labels == cluster_id\n",
    "    ax2.scatter(embedding_combined[mask, 0], embedding_combined[mask, 1],\n",
    "               c=[colors_clusters[cluster_id]], \n",
    "               label=f'Phenotype {cluster_id + 1} (n={mask.sum()})',\n",
    "               alpha=0.6, s=40, edgecolors='white', linewidth=0.5)\n",
    "ax2.set_xlabel('UMAP Dimension 1', fontsize=11)\n",
    "ax2.set_ylabel('UMAP Dimension 2', fontsize=11)\n",
    "ax2.set_title(f'Final Phenotype Clusters (n={n_clusters})', fontsize=12, fontweight='bold')\n",
    "ax2.legend(loc='best', fontsize=9)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/final_summary.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nFinal summary visualization saved to 'figures/final_summary.png'\")\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"ANALYSIS COMPLETE\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\\nAll visualizations have been saved to the 'figures/' directory.\")\n",
    "print(\"Key outputs:\")\n",
    "print(\"  - preprocessing_distributions.png\")\n",
    "print(\"  - umap_embedding.png\")\n",
    "print(\"  - hyperparameter_tuning.png\")\n",
    "print(\"  - cluster_validation.png\")\n",
    "print(\"  - phenotype_profiles.png\")\n",
    "print(\"  - biomarker_distributions.png\")\n",
    "print(\"  - domain_comparison.png\")\n",
    "print(\"  - umap_domain_comparison.png\")\n",
    "print(\"  - final_summary.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Result Analysis:**\n",
    "\n",
    "The final summary visualization provides a comprehensive overview of the analysis outcomes:\n",
    "\n",
    "1. **Domain Performance Ranking (Left Panel):** The horizontal bar chart clearly ranks all feature combinations by silhouette score. The \"All Combined\" feature set is highlighted in green, demonstrating its superior performance. The color coding distinguishes single-domain approaches (blue) from combined approaches (purple), with the best performer clearly indicated.\n",
    "\n",
    "2. **Final Phenotype Clusters (Right Panel):** The UMAP visualization shows the ultimate clustering result with patient assignments to phenotypes. The clear visual separation between clusters validates the quality of the final GMM model and confirms that meaningful phenotypic structure exists in the health data.\n",
    "\n",
    "3. **Key Achievements:**\n",
    "   - Successfully identified 4-5 distinct health phenotypes using GMM clustering\n",
    "   - Demonstrated that multi-domain integration outperforms single-domain analysis\n",
    "   - Achieved high silhouette scores (>0.7) indicating well-defined cluster structure\n",
    "   - Provided clinically interpretable phenotype profiles\n",
    "\n",
    "4. **Clinical Value:** The discovered phenotypes can inform personalized medicine approaches by identifying patient subgroups with characteristic health profiles and potential differential treatment responses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Conclusions and Future Directions\n",
    "\n",
    "### 11.1 Key Conclusions\n",
    "\n",
    "This project successfully demonstrates the application of Gaussian Mixture Models for health phenotype discovery in a comprehensive clinical dataset. The systematic analysis yields several important conclusions:\n",
    "\n",
    "**Methodological Conclusions:**\n",
    "- GMM provides a robust framework for health phenotype discovery with high cluster quality (silhouette > 0.7)\n",
    "- The combination of UMAP for dimensionality reduction and GMM for clustering effectively identifies meaningful patient subgroups\n",
    "- Systematic hyperparameter optimization ensures model selection balances complexity and performance\n",
    "- Probabilistic cluster assignments provide valuable uncertainty information for clinical decision-making\n",
    "\n",
    "**Domain Analysis Conclusions:**\n",
    "- Multi-domain health assessment (combining metabolic, cardiovascular, body composition, and inflammatory markers) provides the highest quality phenotype discrimination\n",
    "- The Cardiovascular domain shows surprisingly strong discriminative power despite having only 3 features\n",
    "- Single-domain analyses, while useful for focused investigations, miss important phenotypic variation captured by cross-domain interactions\n",
    "\n",
    "**Clinical Implications:**\n",
    "- The discovered phenotypes represent distinct patient subgroups with characteristic health profiles\n",
    "- Phenotype-specific interventions could be developed based on the characteristic biomarker patterns\n",
    "- Transitional patients (low cluster membership confidence) may benefit from additional clinical assessment\n",
    "\n",
    "### 11.2 Limitations and Future Work\n",
    "\n",
    "**Current Limitations:**\n",
    "- Analysis is based on cross-sectional data; longitudinal validation is needed\n",
    "- External validation in independent cohorts is required\n",
    "- Clinical outcomes data would strengthen phenotype interpretation\n",
    "- The synthetic data (for demonstration) would benefit from real clinical validation\n",
    "\n",
    "**Future Directions:**\n",
    "- Extend analysis to include outcome data (disease incidence, mortality)\n",
    "- Develop phenotype-specific risk stratification tools\n",
    "- Investigate treatment response patterns by phenotype\n",
    "- Apply deep learning approaches for more complex phenotype discovery\n",
    "- Validate findings in diverse population samples\n",
    "\n",
    "---\n",
    "\n",
    "## References\n",
    "\n",
    "1. McLachlan, G.J., & Peel, D. (2000). Finite Mixture Models. Wiley.\n",
    "\n",
    "2. Reynolds, M., & Richards, P. (2001). Clustering, Validation, and GMMs. Pattern Recognition.\n",
    "\n",
    "3. UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction. SciPy 2018.\n",
    "\n",
    "4. Calinski, T., & Harabasz, J. (1974). A Dendrite Method for Cluster Analysis. Communications in Statistics.\n",
    "\n",
    "5. Davies, D.L., & Bouldin, D.W. (1979). A Cluster Separation Measure. IEEE Transactions on Pattern Analysis.\n",
    "\n",
    "---\n",
    "\n",
    "*This notebook was developed as part of the MSc in Public Health Data Science program, demonstrating advanced machine learning applications in healthcare analytics.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix: Model Configuration Details\n",
    "\n",
    "### Complete Hyperparameter Grid Search Results\n",
    "\n",
    "The following table presents the complete results from the systematic hyperparameter tuning process, enabling full transparency and reproducibility of the model selection analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Display complete hyperparameter results\n",
    "print(\"=\" * 70)\n",
    "print(\"APPENDIX: COMPLETE HYPERPARAMETER GRID SEARCH RESULTS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\nAll Evaluated Configurations (sorted by BIC):\")\n",
    "print(\"-\" * 90)\n",
    "results_sorted = results_df.sort_values('bic')\n",
    "display_cols = ['n_components', 'covariance_type', 'bic', 'aic', 'silhouette', 'confidence', 'converged']\n",
    "print(results_sorted[display_cols].head(15).to_string(index=False))\n",
    "\n",
    "print(\"\\n\\nBest Configuration Details:\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"Components: {int(best_config['n_components'])}\")\n",
    "print(f\"Covariance Type: {best_config['covariance_type']}\")\n",
    "print(f\"BIC Score: {best_config['bic']:.2f}\")\n",
    "print(f\"AIC Score: {best_config['aic']:.2f}\")\n",
    "print(f\"Silhouette Score: {best_config['silhouette']:.4f}\")\n",
    "print(f\"Calinski-Harabasz: {best_config['calinski_harabasz']:.2f}\")\n",
    "print(f\"Davies-Bouldin: {best_config['davies_bouldin']:.4f}\")\n",
    "print(f\"Model Confidence: {best_config['confidence']:.4f}\")\n",
    "print(f\"Converged: {best_config['converged']}\")\n",
    "print(f\"Iterations: {int(best_config['n_iter'])}\")\n",
    "\n",
    "# Save results to CSV for reproducibility\n",
    "results_df.to_csv('output_v2/hyperparameter_results.csv', index=False)\n",
    "comparison_df.to_csv('output_v2/domain_comparison_results.csv', index=False)\n",
    "\n",
    "print(\"\\n\\nResults exported to:\")\n",
    "print(\"  - output_v2/hyperparameter_results.csv\")\n",
    "print(\"  - output_v2/domain_comparison_results.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Result Analysis:**\n",
    "\n",
    "The appendix provides complete transparency regarding the model selection process:\n",
    "\n",
    "1. **Reproducibility:** The complete grid search results are exported to CSV files, enabling exact replication of the analysis and further exploration of alternative model configurations.\n",
    "\n",
    "2. **Model Comparison:** The sorted results (by BIC) show how different configurations compare, revealing the trade-off between model complexity (number of components) and model fit.\n",
    "\n",
    "3. **Alternative Models:** Researchers interested in exploring different cluster counts or covariance structures can refer to the complete results table to identify alternative configurations that might be appropriate for their specific research questions.\n",
    "\n",
    "4. **Validation:** The convergence flag confirms that all evaluated models successfully converged, ensuring that the results represent stable solutions rather than local optima or non-converged states.\n",
    "\n",
    "---\n",
    "\n",
    "**Notebook Information:**\n",
    "\n",
    "- **Author:** Cavin Otieno (OumaCavin)\n",
    "- **Email:** cavin.otieno012@gmail.com\n",
    "- **Project:** MSc Public Health Data Science - Advanced Machine Learning\n",
    "- **Version:** 1.0\n",
    "- **Date:** January 2026"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}