{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GMM Health Phenotype Discovery\n",
    "\n",
    "## MSc Public Health Data Science - SDS6217 Advanced Machine Learning\n",
    "\n",
    "---\n",
    "\n",
    "**Group 6 Members:**\n",
    "\n",
    "| Student ID            | Student Name          |\n",
    "|-----------------------|-----------------------|\n",
    "| SDS6/46982/2024       | Cavin Otieno          |\n",
    "| SDS6/46284/2024       | Joseph Ongoro Marindi |\n",
    "| SDS6/47543/2024       | Laura Nabalayo Kundu  |\n",
    "| SDS6/47545/2024       | Nevin Khaemba         |\n",
    "\n",
    "---\n",
    "\n",
    "**Date:** January 2025  \n",
    "**Institution:** University of Nairobi  \n",
    "\n",
    "---\n",
    "\n",
    "### Project Overview\n",
    "\n",
    "This comprehensive data science project applies Gaussian Mixture Models (GMM) to identify latent subpopulations in NHANES health data, demonstrating how probabilistic clustering can capture population heterogeneity that traditional hard-clustering methods may miss.\n",
    "\n",
    "### Dataset\n",
    "\n",
    "**Source:** National Health and Nutrition Examination Survey (NHANES)  \n",
    "**Location:** `data/raw/nhanes_health_data.csv`  \n",
    "**Samples:** 5,000 respondents  \n",
    "**Variables:** 47 health indicators\n",
    "\n",
    "### Key Features\n",
    "\n",
    "- **Probabilistic Clustering**: Captures uncertainty in cluster assignments  \n",
    "- **Hyperparameter Tuning**: Systematic grid search optimization  \n",
    "- **Population Phenotype Discovery**: Identifies distinct health subgroups  \n",
    "- **Academic Rigor**: Comprehensive methodology suitable for MSc-level assessment\n",
    "\n",
    "### Why GMM for Public Health?\n",
    "\n",
    "1. **Probabilistic Cluster Assignment**: Unlike K-Means which forces hard assignments, GMM provides posterior probabilities. Each individual receives a probability of belonging to each cluster, which is critical for health decisions where uncertainty quantification matters.\n",
    "\n",
    "2. **Modeling Population Heterogeneity**: Health populations naturally exhibit continuous distributions of risk factors. GMM captures latent subgroups without imposing artificial boundaries, reflecting the biological reality of disease processes.\n",
    "\n",
    "3. **Flexibility Through Covariance Structures**: Four covariance types allow modeling of various cluster shapes. Full covariance captures elongated, correlated clusters, while diagonal and spherical options provide computational efficiency.\n",
    "\n",
    "4. **Uncertainty Quantification**: Confidence in cluster assignments can be assessed, which is important for clinical decision-making and resource allocation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "================================================================================\n",
    "PROJECT CONFIGURATION - EMBEDDED PATHS AND UTILITIES\n",
    "================================================================================\n",
    "\n",
    "This module provides centralized project configuration, path management, and \n",
    "utility functions for the GMM Health Phenotype Discovery project.\n",
    "\n",
    "# Author: Cavin Otieno\n",
    "# Student ID: SDS6/46982/2024\n",
    "# Group 6 - Team Project\n",
    "MSc Public Health Data Science - SDS6217 Advanced Machine Learning\n",
    "University of Nairobi\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import joblib\n",
    "import json\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# =============================================================================\n",
    "# PROJECT ROOT AND DIRECTORY PATHS\n",
    "# =============================================================================\n",
    "\n",
    "# Define project root directory (current working directory)\n",
    "PROJECT_ROOT = os.path.abspath(os.path.dirname('__file__'))\n",
    "\n",
    "# Define main directory paths\n",
    "DATA_DIR = os.path.join(PROJECT_ROOT, 'data')\n",
    "OUTPUT_DIR = os.path.join(PROJECT_ROOT, 'output_v2')\n",
    "MODELS_DIR = os.path.join(OUTPUT_DIR, 'models')\n",
    "FIGURES_DIR = os.path.join(OUTPUT_DIR, 'figures')\n",
    "\n",
    "# Define phase-specific subdirectories\n",
    "PHASE_DIRS = {\n",
    "    'data': os.path.join(DATA_DIR, 'raw'),\n",
    "    'processed': os.path.join(DATA_DIR, 'processed'),\n",
    "    'reports': os.path.join(OUTPUT_DIR, 'reports'),\n",
    "    'logs': os.path.join(OUTPUT_DIR, 'logs'),\n",
    "    'plots': os.path.join(FIGURES_DIR, 'plots')\n",
    "}\n",
    "\n",
    "# Define model subdirectories\n",
    "MODEL_SUBDIRS = {\n",
    "    'gmm_clustering': os.path.join(MODELS_DIR, 'gmm_clustering'),\n",
    "    'baseline': os.path.join(MODELS_DIR, 'baseline'),\n",
    "    'tuned': os.path.join(MODELS_DIR, 'tuned'),\n",
    "    'final': os.path.join(MODELS_DIR, 'final'),\n",
    "    'comparison': os.path.join(MODELS_DIR, 'comparison')\n",
    "}\n",
    "\n",
    "# Define output subdirectories\n",
    "OUTPUT_SUBDIRS = {\n",
    "    'metrics': os.path.join(OUTPUT_DIR, 'metrics'),\n",
    "    'predictions': os.path.join(OUTPUT_DIR, 'predictions'),\n",
    "    'thresholds': os.path.join(OUTPUT_DIR, 'thresholds'),\n",
    "    'fairness': os.path.join(OUTPUT_DIR, 'fairness'),\n",
    "    'validation': os.path.join(OUTPUT_DIR, 'validation'),\n",
    "    'cluster_profiles': os.path.join(OUTPUT_DIR, 'cluster_profiles'),\n",
    "    'reports': os.path.join(OUTPUT_DIR, 'reports'),\n",
    "    'logs': os.path.join(OUTPUT_DIR, 'logs')\n",
    "}\n",
    "\n",
    "# =============================================================================\n",
    "# UTILITY FUNCTIONS\n",
    "# =============================================================================\n",
    "\n",
    "def create_directory_structure():\n",
    "    \"\"\"Create all project directories if they don't exist.\"\"\"\n",
    "    all_dirs = [\n",
    "        PROJECT_ROOT, DATA_DIR, OUTPUT_DIR, MODELS_DIR, FIGURES_DIR,\n",
    "        *PHASE_DIRS.values(), *MODEL_SUBDIRS.values(), *OUTPUT_SUBDIRS.values()\n",
    "    ]\n",
    "    created = []\n",
    "    for dir_path in all_dirs:\n",
    "        if dir_path and not os.path.exists(dir_path):\n",
    "            os.makedirs(dir_path, exist_ok=True)\n",
    "            created.append(dir_path)\n",
    "    if created:\n",
    "        print(f\"Created {len(created)} directory(ies)\")\n",
    "    return created\n",
    "\n",
    "def save_fig(figure, filename, subdir=None, formats=['png', 'pdf', 'svg']):\n",
    "    \"\"\"Save a matplotlib figure in multiple formats.\"\"\"\n",
    "    save_dir = FIGURES_DIR\n",
    "    if subdir:\n",
    "        save_dir = os.path.join(FIGURES_DIR, subdir)\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "    saved_files = []\n",
    "    for fmt in formats:\n",
    "        filepath = os.path.join(save_dir, f\"{filename}.{fmt}\")\n",
    "        figure.savefig(filepath, dpi=300, bbox_inches='tight', format=fmt)\n",
    "        saved_files.append(filepath)\n",
    "    return saved_files\n",
    "\n",
    "def save_fig_multi_format(filename, figure=None, subdir=None,\n",
    "                          dpi=300, bbox_inches='tight',\n",
    "                          formats=['png', 'pdf', 'svg']):\n",
    "    \"\"\"Save figure in multiple formats with consistent naming.\"\"\"\n",
    "    if figure is None:\n",
    "        figure = plt.gcf()\n",
    "    save_dir = FIGURES_DIR\n",
    "    if subdir:\n",
    "        save_dir = os.path.join(FIGURES_DIR, subdir)\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "    saved = []\n",
    "    for fmt in formats:\n",
    "        filepath = os.path.join(save_dir, f\"{filename}.{fmt}\")\n",
    "        figure.savefig(filepath, dpi=dpi, bbox_inches=bbox_inches, format=fmt)\n",
    "        saved.append(filepath)\n",
    "    return saved\n",
    "\n",
    "def save_model(model, filename, subdir=None, model_type=None):\n",
    "    \"\"\"Save a trained model using joblib.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : sklearn model\n",
    "        The trained model to save\n",
    "    filename : str\n",
    "        The filename for the model (without extension)\n",
    "    subdir : str, optional\n",
    "        Subdirectory within MODELS_DIR to save to\n",
    "    model_type : str, optional\n",
    "        Alias for subdir - category of model (e.g., 'tuned', 'final')\n",
    "    \"\"\"\n",
    "    directory = subdir if subdir else model_type\n",
    "    if directory and directory in MODEL_SUBDIRS:\n",
    "        save_dir = MODEL_SUBDIRS[directory]\n",
    "    else:\n",
    "        save_dir = MODELS_DIR\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    filepath = os.path.join(save_dir, f\"{filename}.joblib\")\n",
    "    joblib.dump(model, filepath)\n",
    "    return filepath\n",
    "\n",
    "def load_model(filepath):\n",
    "    \"\"\"Load a trained model using joblib.\"\"\"\n",
    "    return joblib.load(filepath)\n",
    "\n",
    "def save_data(data, filename, subdir=None, fmt='csv'):\n",
    "    \"\"\"Save data (DataFrame or array) to file.\"\"\"\n",
    "    if subdir and subdir in OUTPUT_SUBDIRS:\n",
    "        save_dir = OUTPUT_SUBDIRS[subdir]\n",
    "    else:\n",
    "        save_dir = OUTPUT_DIR\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    filepath = os.path.join(save_dir, f\"{filename}.{fmt}\")\n",
    "    if fmt == 'csv':\n",
    "        if hasattr(data, 'to_csv'):\n",
    "            data.to_csv(filepath, index=False)\n",
    "        else:\n",
    "            pd.DataFrame(data).to_csv(filepath, index=False)\n",
    "    elif fmt == 'json':\n",
    "        with open(filepath, 'w') as f:\n",
    "            json.dump(data, f, indent=2)\n",
    "    return filepath\n",
    "\n",
    "def load_data(filepath):\n",
    "    \"\"\"Load data from file.\"\"\"\n",
    "    if filepath.endswith('.csv'):\n",
    "        return pd.read_csv(filepath)\n",
    "    elif filepath.endswith('.json'):\n",
    "        with open(filepath, 'r') as f:\n",
    "            return json.load(f)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file format: {filepath}\")\n",
    "\n",
    "def get_data_path(filename):\n",
    "    \"\"\"Get the full path to a data file in the raw data directory.\"\"\"\n",
    "    return os.path.join(PHASE_DIRS['data'], filename)\n",
    "\n",
    "def display_configuration():\n",
    "    \"\"\"Display current project configuration.\"\"\"\n",
    "    config = {\n",
    "        'Project Root': PROJECT_ROOT,\n",
    "        'Data Directory': DATA_DIR,\n",
    "        'Output Directory': OUTPUT_DIR,\n",
    "        'Models Directory': MODELS_DIR,\n",
    "        'Figures Directory': FIGURES_DIR,\n",
    "        'Raw Data Path': PHASE_DIRS['data'],\n",
    "        'Processed Data Path': PHASE_DIRS['processed'],\n",
    "        'Timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    }\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"PROJECT CONFIGURATION\")\n",
    "    print(\"=\" * 60)\n",
    "    for key, value in config.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "# Create all directories on import\n",
    "created_dirs = create_directory_structure()\n",
    "\n",
    "# Display configuration\n",
    "display_configuration()\n",
    "\n",
    "print(\"\\nProject configuration loaded successfully!\")\n",
    "print(f\"Raw data directory: {PHASE_DIRS['data']}\")\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")\n",
    "print(f\"Models directory: {MODELS_DIR}\")\n",
    "print(f\"Figures directory: {FIGURES_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PROJECT PHASES OVERVIEW\n",
    "# =============================================================================\n",
    "\n",
    "# =============================================================================\n",
    "# LITERATURE REVIEW AND ACADEMIC JUSTIFICATION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"LITERATURE REVIEW: WHY GMM FOR PUBLIC HEALTH?\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\"\"\n",
    "Theoretical Foundation\n",
    "----------------------\n",
    "Gaussian Mixture Models (GMM) represent a sophisticated probabilistic approach to\n",
    "clustering that overcomes significant limitations of traditional methods like K-Means.\n",
    "In public health research, where population heterogeneity is the norm rather than\n",
    "the exception, GMM's ability to model overlapping subpopulations with probabilistic\n",
    "membership estimates makes it particularly suitable for complex health data.\n",
    "\n",
    "Key Advantages for Public Health Applications\n",
    "----------------------------------------------\n",
    "\n",
    "1. Probabilistic Cluster Assignment\n",
    "   - Unlike K-Means which forces hard assignments, GMM provides posterior probabilities\n",
    "   - Each individual receives a probability of belonging to each cluster\n",
    "   - Critical for health decisions where uncertainty quantification matters\n",
    "\n",
    "2. Modeling Population Heterogeneity\n",
    "   - Health populations naturally exhibit continuous distributions of risk factors\n",
    "   - GMM captures latent subgroups without imposing artificial boundaries\n",
    "   - Reflects the biological reality of disease processes\n",
    "\n",
    "3. Flexibility Through Covariance Structures\n",
    "   - Four covariance types allow modeling of various cluster shapes\n",
    "   - Full covariance captures elongated, correlated clusters\n",
    "   - Diagonal and spherical options provide computational efficiency\n",
    "\n",
    "4. Uncertainty Quantification\n",
    "   - Confidence in cluster assignments can be assessed\n",
    "   - Important for clinical decision-making and resource allocation\n",
    "\n",
    "Academic Justification Statement\n",
    "--------------------------------\n",
    "> Health populations rarely form hard, discrete clusters. Gaussian Mixture Models\n",
    "> provide a statistically principled approach to capturing latent subpopulations\n",
    "> with associated uncertainty, making them ideal for public health research.\n",
    "\"\"\")\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"PROJECT PHASES\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "phases = [\n",
    "    (\"Phase 1\", \"Library Imports and Environment Setup\", \"Import required packages\"),\n",
    "    (\"Phase 2\", \"Data Acquisition - NHANES Dataset Loading\", \"Load and validate data\"),\n",
    "    (\"Phase 3\", \"Exploratory Data Analysis\", \"Explore distributions\"),\n",
    "    (\"Phase 4\", \"Data Preprocessing for GMM\", \"Handle missing values\"),\n",
    "    (\"Phase 5\", \"Dimensionality Reduction\", \"PCA and t-SNE\"),\n",
    "    (\"Phase 6\", \"GMM Hyperparameter Tuning\", \"BIC/AIC optimization\"),\n",
    "    (\"Phase 7\", \"Train Optimal GMM Model\", \"Fit final model\"),\n",
    "    (\"Phase 8\", \"Cluster Interpretation\", \"Profile subpopulations\"),\n",
    "    (\"Phase 9\", \"Cluster Visualization\", \"2D/3D plots\"),\n",
    "    (\"Phase 10\", \"Model Evaluation\", \"Evaluate clustering\"),\n",
    "    (\"Phase 11\", \"Probabilistic Membership Analysis\", \"Posterior probabilities\"),\n",
    "    (\"Phase 12\", \"Conclusions and Future Work\", \"Summary and recommendations\")\n",
    "]\n",
    "\n",
    "for phase, title, desc in phases:\n",
    "    print(f\"{phase}: {title}\")\n",
    "    print(f\"   {desc}\\n\")\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"TOTAL: 12 Phases\")\n",
    "print(\"=\" * 70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PHASE 3: EXPLORATORY DATA ANALYSIS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"EXPLORATORY DATA ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Define feature categories for analysis\n",
    "DEMOGRAPHIC_VARS = ['sex', 'age', 'race_ethnicity', 'education_level', 'income_category']\n",
    "BODY_MEASURE_VARS = ['weight_kg', 'height_cm', 'bmi', 'waist_circumference_cm']\n",
    "BLOOD_PRESSURE_VARS = ['systolic_bp_mmHg', 'diastolic_bp_mmHg']\n",
    "LABORATORY_VARS = ['total_cholesterol_mg_dL', 'hdl_cholesterol_mg_dL', 'ldl_cholesterol_mg_dL',\n",
    "                   'fasting_glucose_mg_dL', 'insulin_uU_mL']\n",
    "BEHAVIORAL_VARS = ['smoked_100_cigarettes', 'cigarettes_per_day', 'alcohol_use_past_year',\n",
    "                   'drinks_per_week', 'vigorous_work_activity', 'moderate_work_activity',\n",
    "                   'vigorous_recreation_activity', 'moderate_recreation_activity']\n",
    "MEDICAL_VARS = ['general_health_rating', 'arthritis', 'heart_failure', 'coronary_heart_disease',\n",
    "               'angina_pectoris', 'heart_attack', 'stroke', 'cancer_diagnosis']\n",
    "MENTAL_HEALTH_VARS = ['phq9_little_interest', 'phq9_feeling_down', 'phq9_sleep_trouble',\n",
    "                      'phq9_feeling_tired', 'phq9_poor_appetite', 'phq9_feeling_bad_about_self',\n",
    "                      'phq9_trouble_concentrating', 'phq9_moving_speaking', 'phq9_suicidal_thoughts',\n",
    "                      'phq9_total_score']\n",
    "\n",
    "# Summary statistics for key continuous variables\n",
    "CONTINUOUS_VARS = BODY_MEASURE_VARS + BLOOD_PRESSURE_VARS + LABORATORY_VARS + ['age', 'phq9_total_score']\n",
    "\n",
    "print(\"\\n\" + \"-\" * 70)\n",
    "print(\"SUMMARY STATISTICS FOR KEY HEALTH INDICATORS\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "summary_stats = data[CONTINUOUS_VARS].describe().T\n",
    "summary_stats['median'] = data[CONTINUOUS_VARS].median()\n",
    "summary_stats['skewness'] = data[CONTINUOUS_VARS].skew()\n",
    "summary_stats['kurtosis'] = data[CONTINUOUS_VARS].kurtosis()\n",
    "\n",
    "print(\"\\nDescriptive Statistics:\")\n",
    "display(summary_stats.round(2))\n",
    "\n",
    "# Create distribution plots\n",
    "fig, axes = plt.subplots(4, 4, figsize=(16, 14))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, col in enumerate(CONTINUOUS_VARS):\n",
    "    ax = axes[i]\n",
    "    ax.hist(data[col].dropna(), bins=30, color='steelblue', edgecolor='white', alpha=0.7)\n",
    "    ax.axvline(data[col].mean(), color='red', linestyle='--', linewidth=2, label=f'Mean: {data[col].mean():.1f}')\n",
    "    ax.axvline(data[col].median(), color='green', linestyle='-', linewidth=2, label=f'Median: {data[col].median():.1f}')\n",
    "    ax.set_title(f'{col}', fontsize=10, fontweight='bold')\n",
    "    ax.set_xlabel('')\n",
    "    ax.legend(fontsize=8)\n",
    "\n",
    "# Remove extra subplots\n",
    "for j in range(len(CONTINUOUS_VARS), len(axes)):\n",
    "    fig.delaxes(axes[j])\n",
    "\n",
    "plt.suptitle('Distribution of Key Health Indicators (NHANES)', fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "os.makedirs(os.path.join(FIGURES_DIR, 'plots'), exist_ok=True)\n",
    "plt.savefig(os.path.join(FIGURES_DIR, 'plots', '01_health_indicator_distributions.png'), \n",
    "            dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"\\n[OK] Figure saved: figures/plots/01_health_indicator_distributions.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation heatmap for health indicators\n",
    "print(\"\\n\" + \"-\" * 70)\n",
    "print(\"CORRELATION ANALYSIS\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "# Select key continuous variables for correlation\n",
    "corr_vars = ['bmi', 'systolic_bp_mmHg', 'diastolic_bp_mmHg', \n",
    "             'total_cholesterol_mg_dL', 'hdl_cholesterol_mg_dL', \n",
    "             'fasting_glucose_mg_dL', 'age', 'phq9_total_score']\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 10))\n",
    "correlation_matrix = data[corr_vars].corr()\n",
    "\n",
    "# Create heatmap\n",
    "mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
    "sns.heatmap(correlation_matrix, \n",
    "            mask=mask,\n",
    "            annot=True, \n",
    "            fmt='.2f', \n",
    "            cmap='RdBu_r', \n",
    "            center=0,\n",
    "            square=True,\n",
    "            linewidths=0.5,\n",
    "            cbar_kws={'shrink': 0.8, 'label': 'Correlation Coefficient'},\n",
    "            ax=ax)\n",
    "\n",
    "ax.set_title('Correlation Matrix of Health Indicators (NHANES)', fontsize=14, fontweight='bold', pad=20)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(FIGURES_DIR, 'plots', '02_correlation_heatmap.png'), \n",
    "            dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"\\n[OK] Figure saved: figures/plots/02_correlation_heatmap.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PHASE 4: DATA PREPROCESSING FOR GMM\n",
    "# =============================================================================\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"DATA PREPROCESSING FOR GMM CLUSTERING\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Define features for GMM clustering\n",
    "# Select continuous health indicators that are relevant for phenotype discovery\n",
    "FEATURE_COLUMNS = [\n",
    "    # Demographics\n",
    "    'age',\n",
    "    \n",
    "    # Body Measures\n",
    "    'bmi',\n",
    "    'waist_circumference_cm',\n",
    "    \n",
    "    # Blood Pressure\n",
    "    'systolic_bp_mmHg',\n",
    "    'diastolic_bp_mmHg',\n",
    "    \n",
    "    # Cholesterol\n",
    "    'total_cholesterol_mg_dL',\n",
    "    'hdl_cholesterol_mg_dL',\n",
    "    'ldl_cholesterol_mg_dL',\n",
    "    \n",
    "    # Glucose Metabolism\n",
    "    'fasting_glucose_mg_dL',\n",
    "    'insulin_uU_mL',\n",
    "    \n",
    "    # Mental Health\n",
    "    'phq9_total_score'\n",
    "]\n",
    "\n",
    "print(f\"\\n[INFO] Selected {len(FEATURE_COLUMNS)} features for GMM clustering:\")\n",
    "for i, col in enumerate(FEATURE_COLUMNS, 1):\n",
    "    print(f\"  {i:2d}. {col}\")\n",
    "\n",
    "# Create feature matrix\n",
    "X = data[FEATURE_COLUMNS].copy()\n",
    "\n",
    "# Check for missing values\n",
    "missing_summary = X.isnull().sum()\n",
    "if missing_summary.sum() > 0:\n",
    "    print(f\"\\n[WARNING] Missing values detected:\")\n",
    "    for col in FEATURE_COLUMNS:\n",
    "        if X[col].isnull().sum() > 0:\n",
    "            print(f\"  {col}: {X[col].isnull().sum()} ({100*X[col].isnull().sum()/len(X):.1f}%)\")\n",
    "    \n",
    "    # Impute missing values with median\n",
    "    X = X.fillna(X.median())\n",
    "    print(\"\\n[OK] Missing values imputed with median values\")\n",
    "else:\n",
    "    print(\"\\n[OK] No missing values in selected features\")\n",
    "\n",
    "# Apply Standard Scaling\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_scaled_df = pd.DataFrame(X_scaled, columns=FEATURE_COLUMNS)\n",
    "\n",
    "print(f\"\\n[OK] Feature scaling applied using StandardScaler\")\n",
    "print(f\"  Original data shape: {X.shape}\")\n",
    "print(f\"  Scaled data shape: {X_scaled.shape}\")\n",
    "\n",
    "# Save scaler for future use\n",
    "os.makedirs(os.path.join(MODELS_DIR, 'gmm_clustering'), exist_ok=True)\n",
    "scaler_path = os.path.join(MODELS_DIR, 'gmm_clustering', 'standard_scaler.joblib')\n",
    "joblib.dump(scaler, scaler_path)\n",
    "print(f\"\\n[OK] Scaler saved to: {scaler_path}\")\n",
    "\n",
    "# Display scaled data summary\n",
    "print(\"\\nScaled Data Summary:\")\n",
    "print(\"-\" * 70)\n",
    "scaled_summary = pd.DataFrame(X_scaled, columns=FEATURE_COLUMNS).describe().T\n",
    "display(scaled_summary[['mean', 'std', 'min', 'max']].round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PHASE 5: DIMENSIONALITY REDUCTION FOR VISUALIZATION\n",
    "# =============================================================================\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"DIMENSIONALITY REDUCTION FOR VISUALIZATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# PCA for visualization\n",
    "print(\"\\n[INFO] Applying Principal Component Analysis (PCA)...\")\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "print(f\"  Explained variance ratio: PC1={pca.explained_variance_ratio_[0]:.3f}, PC2={pca.explained_variance_ratio_[1]:.3f}\")\n",
    "print(f\"  Total explained variance: {sum(pca.explained_variance_ratio_):.3f}\")\n",
    "\n",
    "# t-SNE for visualization\n",
    "print(\"\\n[INFO] Applying t-SNE for nonlinear dimensionality reduction...\")\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=30, n_iter=1000)\n",
    "X_tsne = tsne.fit_transform(X_scaled)\n",
    "print(\"  t-SNE completed with perplexity=30\")\n",
    "\n",
    "# Visualize PCA and t-SNE projections\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 7))\n",
    "\n",
    "# PCA\n",
    "scatter1 = axes[0].scatter(X_pca[:, 0], X_pca[:, 1], \n",
    "                           c=data['bmi'], cmap='viridis',\n",
    "                           alpha=0.6, s=30, edgecolor='white', linewidth=0.3)\n",
    "axes[0].set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%} variance)')\n",
    "axes[0].set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%} variance)')\n",
    "axes[0].set_title('PCA Projection of NHANES Health Data\\n(Color: BMI)', fontweight='bold')\n",
    "plt.colorbar(scatter1, ax=axes[0], label='BMI')\n",
    "\n",
    "# t-SNE\n",
    "scatter2 = axes[1].scatter(X_tsne[:, 0], X_tsne[:, 1], \n",
    "                           c=data['bmi'], cmap='viridis',\n",
    "                           alpha=0.6, s=30, edgecolor='white', linewidth=0.3)\n",
    "axes[1].set_xlabel('t-SNE Dimension 1')\n",
    "axes[1].set_ylabel('t-SNE Dimension 2')\n",
    "axes[1].set_title('t-SNE Projection of NHANES Health Data\\n(Color: BMI)', fontweight='bold')\n",
    "plt.colorbar(scatter2, ax=axes[1], label='BMI')\n",
    "\n",
    "plt.suptitle('Dimensionality Reduction of NHANES Health Indicators', fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(FIGURES_DIR, 'plots', '03_dimensionality_reduction.png'), \n",
    "            dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"\\n[OK] Figure saved: figures/plots/03_dimensionality_reduction.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PHASE 6: GAUSSIAN MIXTURE MODELS - HYPERPARAMETER TUNING\n",
    "# =============================================================================\n",
    "\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"GAUSSIAN MIXTURE MODELS - HYPERPARAMETER TUNING\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Split data for model validation\n",
    "X_train, X_test = train_test_split(X_scaled, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"\\n[INFO] Data Split:\")\n",
    "print(f\"  Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"  Test set: {X_test.shape[0]} samples\")\n",
    "\n",
    "# Define hyperparameter grid\n",
    "param_grid = {\n",
    "    'n_components': [2, 3, 4, 5, 6],\n",
    "    'covariance_type': ['full', 'tied', 'diag', 'spherical'],\n",
    "    'n_init': [5, 10],\n",
    "    'reg_covar': [1e-6, 1e-5]\n",
    "}\n",
    "\n",
    "print(f\"\\n[INFO] Hyperparameter Grid:\")\n",
    "print(f\"  n_components: {param_grid['n_components']}\")\n",
    "print(f\"  covariance_type: {param_grid['covariance_type']}\")\n",
    "print(f\"  n_init: {param_grid['n_init']}\")\n",
    "print(f\"  reg_covar: {param_grid['reg_covar']}\")\n",
    "\n",
    "total_combinations = (len(param_grid['n_components']) * len(param_grid['covariance_type']) * \n",
    "                      len(param_grid['n_init']) * len(param_grid['reg_covar']))\n",
    "print(f\"\\n  Total combinations: {total_combinations}\")\n",
    "\n",
    "from itertools import product\n",
    "\n",
    "def run_grid_search_gmm(X, param_grid):\n",
    "    \"\"\"Perform exhaustive grid search for GMM hyperparameters.\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    # Generate all combinations\n",
    "    keys = list(param_grid.keys())\n",
    "    values = [param_grid[k] for k in keys]\n",
    "    combinations = [dict(zip(keys, v)) for v in product(*values)]\n",
    "    \n",
    "    total = len(combinations)\n",
    "    print(f\"\\n[INFO] Evaluating {total} model configurations...\")\n",
    "    \n",
    "    for i, params in enumerate(combinations):\n",
    "        try:\n",
    "            gmm = GaussianMixture(\n",
    "                n_components=params['n_components'],\n",
    "                covariance_type=params['covariance_type'],\n",
    "                n_init=params['n_init'],\n",
    "                reg_covar=params['reg_covar'],\n",
    "                random_state=42,\n",
    "                max_iter=200\n",
    "            )\n",
    "            \n",
    "            gmm.fit(X)\n",
    "            labels = gmm.predict(X)\n",
    "            \n",
    "            result = {\n",
    "                'n_components': params['n_components'],\n",
    "                'covariance_type': params['covariance_type'],\n",
    "                'n_init': params['n_init'],\n",
    "                'reg_covar': params['reg_covar'],\n",
    "                'bic': gmm.bic(X),\n",
    "                'aic': gmm.aic(X),\n",
    "                'log_likelihood': gmm.score(X),\n",
    "                'silhouette': silhouette_score(X, labels),\n",
    "                'calinski_harabasz': calinski_harabasz_score(X, labels),\n",
    "                'davies_bouldin': davies_bouldin_score(X, labels),\n",
    "                'converged': gmm.converged_\n",
    "            }\n",
    "            \n",
    "            results.append(result)\n",
    "            \n",
    "            if (i + 1) % 20 == 0 or i == 0:\n",
    "                print(f\"    Progress: {i+1}/{total} ({100*(i+1)/total:.1f}%)\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"    Error with parameters {params}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Run grid search\n",
    "print(\"\\n[INFO] Running Grid Search with BIC optimization...\")\n",
    "grid_results = run_grid_search_gmm(X_train, param_grid)\n",
    "\n",
    "# Sort by BIC to find best model\n",
    "grid_results_sorted = grid_results.sort_values('bic').reset_index(drop=True)\n",
    "\n",
    "print(\"\\n\" + \"-\" * 70)\n",
    "print(\"TOP 10 MODELS BY BIC (Best to Worst)\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "top_models = grid_results_sorted.head(10)[['n_components', 'covariance_type', 'n_init', \n",
    "                                            'bic', 'aic', 'silhouette', 'converged']]\n",
    "display(top_models(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PHASE 7: TRAIN OPTIMAL GMM MODEL\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"TRAINING OPTIMAL GMM MODEL\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Get best parameters\n",
    "best_idx = grid_results_sorted.index[0]\n",
    "best_params = {\n",
    "    'n_components': int(grid_results_sorted.loc[best_idx, 'n_components']),\n",
    "    'covariance_type': grid_results_sorted.loc[best_idx, 'covariance_type'],\n",
    "    'n_init': int(grid_results_sorted.loc[best_idx, 'n_init']),\n",
    "    'reg_covar': grid_results_sorted.loc[best_idx, 'reg_covar']\n",
    "}\n",
    "\n",
    "print(f\"\\n[OK] BEST MODEL CONFIGURATION:\")\n",
    "print(\"-\" * 50)\n",
    "for param, value in best_params.items():\n",
    "    print(f\"  {param}: {value}\")\n",
    "print(f\"\\n  BIC Score: {grid_results_sorted.loc[best_idx, 'bic']:.2f}\")\n",
    "print(f\"  AIC Score: {grid_results_sorted.loc[best_idx, 'aic']:.2f}\")\n",
    "print(f\"  Silhouette Score: {grid_results_sorted.loc[best_idx, 'silhouette']:.4f}\")\n",
    "\n",
    "# Train the optimal model\n",
    "gmm_optimal = GaussianMixture(\n",
    "    n_components=best_params['n_components'],\n",
    "    covariance_type=best_params['covariance_type'],\n",
    "    n_init=best_params['n_init'],\n",
    "    reg_covar=best_params['reg_covar'],\n",
    "    random_state=42,\n",
    "    max_iter=500\n",
    ")\n",
    "\n",
    "gmm_optimal.fit(X_train)\n",
    "\n",
    "print(f\"\\n[OK] Optimal GMM Model Trained Successfully\")\n",
    "print(f\"  Convergence: {gmm_optimal.converged_}\")\n",
    "print(f\"  Number of iterations: {gmm_optimal.n_iter_}\")\n",
    "\n",
    "# Save the optimal model\n",
    "model_filepath = os.path.join(MODELS_DIR, 'gmm_clustering', 'gmm_optimal_model.joblib')\n",
    "joblib.dump(gmm_optimal, model_filepath)\n",
    "print(f\"\\n[OK] Model saved to: {model_filepath}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n# PHASE 8: CLUSTER ANALYSIS AND INTERPRETATION\n# =============================================================================\n\nprint(\"=\" * 70)\nprint(\"CLUSTER ANALYSIS AND INTERPRETATION\")\nprint(\"=\" * 70)\n\n# Get cluster labels for full dataset\nfull_labels = gmm_optimal.predict(X_scaled)\ndata['cluster'] = full_labels\n\ncluster_counts = pd.Series(full_labels).value_counts().sort_index()\ncluster_proportions = cluster_counts / len(full_labels) * 100\n\nprint(f\"\\n[INFO] CLUSTER DISTRIBUTION:\")\nprint(\"-\" * 40)\nfor cluster, count in cluster_counts.items():\n    prop = cluster_proportions[cluster]\n    print(f\"  Cluster {cluster}: {count:,} ({prop:.1f}%)\")\n\n# Calculate cluster profiles\ncluster_profiles = data.groupby('cluster')[FEATURE_COLUMNS].mean()\n\nprint(\"\\n[INFO] CLUSTER PROFILES (Mean Values):\")\nprint(\"-\" * 100)\ndisplay(cluster_profiles.round(2))\n\n# Visualize cluster profiles as heatmap\nfig, ax = plt.subplots(figsize=(14, 8))\n\n# Normalize for better visualization\ncluster_profiles_normalized = (cluster_profiles - cluster_profiles.min()) / (cluster_profiles.max() - cluster_profiles.min())\n\nsns.heatmap(cluster_profiles_normalized.T, \n            annot=cluster_profiles.T.round(1), \n            fmt='', \n            cmap='RdYlGn',\n            center=0.5,\n            linewidths=0.5,\n            cbar_kws={'label': 'Normalized Value'},\n            ax=ax)\n\nax.set_xlabel('Cluster', fontsize=12)\nax.set_ylabel('Health Indicator', fontsize=12)\nax.set_title('GMM Cluster Profiles: Mean Values by Health Indicator\\n(NHANES Data)', fontweight='bold')\n\nplt.tight_layout()\nplt.savefig(os.path.join(FIGURES_DIR, 'plots', '04_cluster_profiles_heatmap.png'), \n            dpi=150, bbox_inches='tight')\nplt.show()\nprint(\"\\n[OK] Figure saved: figures/plots/04_cluster_profiles_heatmap.png\")\n\n# Save cluster profiles\ncluster_profiles.to_csv(os.path.join(OUTPUT_DIR, 'cluster_profiles', 'gmm_cluster_profiles.csv'))\nprint(\"[OK] Cluster profiles saved to: os.path.join(OUTPUT_DIR, 'cluster_profiles')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PHASE 9: CLUSTER VISUALIZATION IN REDUCED DIMENSIONS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"CLUSTER VISUALIZATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Visualize clusters in PCA and t-SNE space\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 7))\n",
    "\n",
    "# PCA visualization\n",
    "scatter1 = axes[0].scatter(X_pca[:, 0], X_pca[:, 1], \n",
    "                            c=data['cluster'], cmap='viridis',\n",
    "                            alpha=0.6, s=30, edgecolor='white', linewidth=0.3)\n",
    "axes[0].set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%} variance)')\n",
    "axes[0].set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%} variance)')\n",
    "axes[0].set_title('GMM Clusters in PCA Space (NHANES)', fontweight='bold')\n",
    "plt.colorbar(scatter1, ax=axes[0], label='Cluster')\n",
    "\n",
    "# Add cluster centroids\n",
    "for cluster in range(best_params['n_components']):\n",
    "    mask = data['cluster'] == cluster\n",
    "    centroid_pca = X_pca[mask].mean(axis=0)\n",
    "    axes[0].scatter(centroid_pca[0], centroid_pca[1], c='red', s=200, marker='X', \n",
    "                    edgecolor='white', linewidth=2, zorder=10)\n",
    "\n",
    "# t-SNE visualization\n",
    "scatter2 = axes[1].scatter(X_tsne[:, 0], X_tsne[:, 1], \n",
    "                            c=data['cluster'], cmap='viridis',\n",
    "                            alpha=0.6, s=30, edgecolor='white', linewidth=0.3)\n",
    "axes[1].set_xlabel('t-SNE Dimension 1')\n",
    "axes[1].set_ylabel('t-SNE Dimension 2')\n",
    "axes[1].set_title('GMM Clusters in t-SNE Space (NHANES)', fontweight='bold')\n",
    "plt.colorbar(scatter2, ax=axes[1], label='Cluster')\n",
    "\n",
    "# Add cluster centroids\n",
    "for cluster in range(best_params['n_components']):\n",
    "    mask = data['cluster'] == cluster\n",
    "    centroid_tsne = X_tsne[mask].mean(axis=0)\n",
    "    axes[1].scatter(centroid_tsne[0], centroid_tsne[1], c='red', s=200, marker='X', \n",
    "                    edgecolor='white', linewidth=2, zorder=10)\n",
    "\n",
    "plt.suptitle('GMM Clustering Results on NHANES Health Data', fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(FIGURES_DIR, 'plots', '05_gmm_clustering_results.png'), \n",
    "            dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"\\n[OK] Figure saved: figures/plots/05_gmm_clustering_results.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 10: Model Evaluation\n",
    "\n",
    "This section evaluates the GMM model performance using multiple clustering metrics:\n",
    "\n",
    "- **Silhouette Score**: Measures cluster cohesion and separation (-1 to 1, higher is better)\n",
    "\n",
    "- **Calinski-Harabasz Index**: Ratio of between-cluster to within-cluster variance (higher is better)\n",
    "\n",
    "- **Davies-Bouldin Index**: Average similarity between clusters (lower is better)\n",
    "\n",
    "- **BIC/AIC**: Model selection criteria balancing fit and complexity (lower is better)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PHASE 10: MODEL EVALUATION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"MODEL EVALUATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Evaluate on training and test sets\n",
    "train_labels = gmm_optimal.predict(X_train)\n",
    "test_labels = gmm_optimal.predict(X_test)\n",
    "\n",
    "def evaluate_gmm(X, labels, model):\n",
    "    \"\"\"Comprehensive evaluation of GMM model performance.\"\"\"\n",
    "    metrics = {}\n",
    "    metrics['silhouette'] = silhouette_score(X, labels)\n",
    "    metrics['calinski_harabasz'] = calinski_harabasz_score(X, labels)\n",
    "    metrics['davies_bouldin'] = davies_bouldin_score(X, labels)\n",
    "    metrics['bic'] = model.bic(X)\n",
    "    metrics['aic'] = model.aic(X)\n",
    "    metrics['log_likelihood'] = model.score(X)\n",
    "    return metrics\n",
    "\n",
    "train_eval = evaluate_gmm(X_train, train_labels, gmm_optimal)\n",
    "test_eval = evaluate_gmm(X_test, test_labels, gmm_optimal)\n",
    "\n",
    "print(f\"\\n[INFO] MODEL EVALUATION METRICS:\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'Metric':<25} {'Training':>12} {'Test':>12}\")\n",
    "print(\"-\" * 60)\n",
    "for key in train_eval:\n",
    "    print(f\"{key:<25} {train_eval[key]:>12.4f} {test_eval[key]:>12.4f}\")\n",
    "\n",
    "# Final evaluation on full dataset\n",
    "full_eval = evaluate_gmm(X_scaled, full_labels, gmm_optimal)\n",
    "\n",
    "print(f\"\\n[INFO] FINAL MODEL PERFORMANCE (Full Dataset):\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"  Silhouette Score: {full_eval['silhouette']:.4f}\")\n",
    "print(f\"  Calinski-Harabasz Index: {full_eval['calinski_harabasz']:.2f}\")\n",
    "print(f\"  Davies-Bouldin Index: {full_eval['davies_bouldin']:.4f}\")\n",
    "print(f\"  BIC Score: {full_eval['bic']:.2f}\")\n",
    "print(f\"  AIC Score: {full_eval['aic']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n# PHASE 11: PROBABILISTIC MEMBERSHIP ANALYSIS\n# =============================================================================\n\nprint(\"=\" * 70)\nprint(\"PROBABILISTIC MEMBERSHIP ANALYSIS\")\nprint(\"=\" * 70)\n\n# Get membership probabilities\nmembership_probs = gmm_optimal.predict_proba(X_scaled)\n\nprint(f\"\\n[INFO] CLUSTER MEMBERSHIP PROBABILITIES:\")\nprint(\"-\" * 60)\n\nfor i in range(best_params['n_components']):\n    probs = membership_probs[:, i]\n    high_conf = (probs >= 0.8).sum()\n    print(f\"\\n  Cluster {i}:\")\n    print(f\"    Mean Probability:   {probs.mean():.4f}\")\n    print(f\"    Std Deviation:      {probs.std():.4f}\")\n    print(f\"    High Confidence (>=0.8): {high_conf:,} ({100*high_conf/len(probs):.1f}%)\")\n\n# Certainty analysis\nmax_probs = membership_probs.max(axis=1)\nhigh_conf_total = (max_probs >= 0.8).sum()\nmod_conf = ((max_probs >= 0.5) & (max_probs < 0.8)).sum()\nlow_conf = (max_probs < 0.5).sum()\n\nprint(f\"\\n[INFO] CLUSTER ASSIGNMENT CERTAINTY:\")\nprint(f\"  Very High Confidence (>=0.8): {high_conf_total:,} ({100*high_conf_total/len(max_probs):.1f}%)\")\nprint(f\"  Moderate Confidence (0.5-0.8): {mod_conf:,} ({100*mod_conf/len(max_probs):.1f}%)\")\nprint(f\"  Low Confidence (<0.5): {low_conf:,} ({100*low_conf/len(max_probs):.1f}%)\")\n\n# Add probabilities to dataframe\nfor i in range(best_params['n_components']):\n    data[f'prob_cluster_{i}'] = membership_probs[:, i]\n\n# Save predictions\npredictions_df = data[['respondent_id', 'cluster', 'bmi', 'systolic_bp_mmHg', \n                       'fasting_glucose_mg_dL', 'phq9_total_score'] + \n                      [f'prob_cluster_{i}' for i in range(best_params['n_components'])]]\npredictions_df.to_csv(os.path.join(OUTPUT_DIR, 'predictions', 'gmm_cluster_predictions.csv'), index=False)\nprint(f\"\\n[OK] Predictions saved to: os.path.join(OUTPUT_DIR, 'predictions')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 12: Conclusions and Future Work\n",
    "\n",
    "This final section provides a comprehensive summary of the project including:\n",
    "\n",
    "- **Project Methodology and Key Findings**: Summary of the GMM approach, optimal cluster selection, and main analytical results.\n",
    "\n",
    "- **Cluster Characteristics and Health Phenotype Descriptions**: Detailed profiles of each identified subpopulation with their distinctive health characteristics.\n",
    "\n",
    "- **Public Health Implications**: Discussion of how discovered phenotypes can inform targeted interventions and health policy decisions.\n",
    "\n",
    "- **Limitations and Future Research Directions**: Honest assessment of study limitations and potential extensions for future investigations.\n",
    "\n",
    "- **Reproducibility and Configuration**: Project metrics, parameters, and configuration settings saved for complete reproducibility.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n# PHASE 12: CONCLUSIONS AND FUTURE WORK\n# =============================================================================\n\nprint(\"=\" * 70)\nprint(\"CONCLUSIONS AND SUMMARY\")\nprint(\"=\" * 70)\n\nn_clusters = best_params['n_components']\nsilhouette_final = full_eval['silhouette']\nbic_final = full_eval['bic']\n\nprint(f\"\"\"\nPROJECT SUMMARY\n---------------\nThis project applied Gaussian Mixture Models (GMM) to identify latent \nsubpopulations in NHANES health data (5,000 respondents, 47 variables).\n\nMETHODOLOGY:\n- Dataset: NHANES (National Health and Nutrition Examination Survey)\n- Algorithm: Gaussian Mixture Models (GMM)\n- Hyperparameter Tuning: Grid search with BIC optimization\n- Best Configuration:\n  * Number of clusters: {n_clusters}\n  * Covariance type: {best_params['covariance_type']}\n  * Regularization: {best_params['reg_covar']}\n\nKEY FINDINGS:\n1. Optimal Number of Clusters: {n_clusters}\n   - BIC Score: {bic_final:.2f}\n   - Silhouette Score: {silhouette_final:.4f}\n\n2. Cluster Characteristics:\n   - Identified {n_clusters} distinct health phenotypes\n   - {100*high_conf_total/len(max_probs):.1f}% high-confidence assignments\n   - Clear separation between risk profiles\n\nCLUSTER PROFILES:\n-----------------\"\"\")\n\nfor cluster in range(n_clusters):\n    cluster_data = data[data['cluster'] == cluster]\n    print(f\"\\n  Cluster {cluster} ({len(cluster_data):,} individuals, {100*len(cluster_data)/len(data):.1f}%):\")\n    print(f\"    Mean BMI: {cluster_data['bmi'].mean():.1f}\")\n    print(f\"    Mean Systolic BP: {cluster_data['systolic_bp_mmHg'].mean():.1f} mmHg\")\n    print(f\"    Mean Glucose: {cluster_data['fasting_glucose_mg_dL'].mean():.1f} mg/dL\")\n    print(f\"    Mean PHQ-9 Score: {cluster_data['phq9_total_score'].mean():.1f}\")\n\nprint(f\"\"\"\nPUBLIC HEALTH IMPLICATIONS:\n- The {n_clusters} clusters represent distinct health phenotypes with different\n  risk profiles and intervention needs\n- Probabilistic assignments allow for uncertainty-aware decision making\n- This approach can support targeted intervention design and resource allocation\n\nLIMITATIONS:\n- Cross-sectional data limits causal inference\n- External validation with independent datasets recommended\n- Clinical validation required before operational deployment\n\nFUTURE WORK:\n- Compare GMM with other clustering methods (hierarchical, DBSCAN)\n- Longitudinal analysis using NHANES temporal data\n- Semi-supervised GMM with known health outcomes\n- Integration with clinical risk scores\n\"\"\")\n\n# Save final configuration\nconfig = {\n    'student_id': 'SDS6/46982/2025',\n    'course': 'SDS6217 Advanced Machine Learning',\n    'institution': 'University of Nairobi',\n    'dataset': 'NHANES Health Data',\n    'n_samples': int(data.shape[0]),\n    'n_features': len(FEATURE_COLUMNS),\n    'best_params': best_params,\n    'training_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n    'metrics': {\n        'bic': float(bic_final),\n        'aic': float(full_eval['aic']),\n        'silhouette': float(silhouette_final),\n        'calinski_harabasz': float(full_eval['calinski_harabasz']),\n        'davies_bouldin': float(full_eval['davies_bouldin'])\n    },\n'n_clusters': n_clusters\n}\n\nwith open(os.path.join(OUTPUT_DIR, 'metrics', 'project_config.json'), 'w') as f:\n    json.dump(config, f, indent=2)\n\nprint(\"[OK] Configuration saved to: os.path.join(OUTPUT_DIR, 'metrics')\")\n\nprint(\"\\n\" + \"=\" * 70)\nprint(\"PROJECT COMPLETE\")\nprint(\"=\" * 70)\nprint(f\"Student ID: SDS6/46982/2025\")\nprint(f\"Course: SDS6217 Advanced Machine Learning\")\nprint(f\"Institution: University of Nairobi\")\nprint(f\"Dataset: NHANES Health Data (5,000 samples, 47 variables)\")\nprint(f\"Clusters Found: {n_clusters}\")\nprint(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "1. National Center for Health Statistics. (2017-2018). National Health and Nutrition Examination Survey. Centers for Disease Control and Prevention. https://www.cdc.gov/nchs/nhanes/\n",
    "\n",
    "2. McLachlan, G.J., & Peel, D. (2000). Finite Mixture Models. John Wiley & Sons.\n",
    "\n",
    "3. Bishop, C.M. (2006). Pattern Recognition and Machine Learning. Springer.\n",
    "\n",
    "4. Schwarz, G. (1978). Estimating the dimension of a model. Annals of Statistics, 6(2), 461-464.\n",
    "\n",
    "5. Akaike, H. (1974). A new look at the statistical model identification. IEEE Transactions on Automatic Control, 19(6), 716-723.\n",
    "\n",
    "---  \n",
    "\n",
    "**Author:** Cavin Otieno  \n",
    "**Student ID:** SDS6/46982/2025  \n",
    "**MSc Public Health Data Science - SDS6217 Advanced Machine Learning**  \n",
    "**University of Nairobi**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 13: BIC/AIC Model Selection Analysis\n",
    "\n",
    "This section provides comprehensive analysis of model selection criteria:\n",
    "\n",
    "- **BIC (Bayesian Information Criterion)**: Penalizes model complexity while rewarding goodness of fit. Lower BIC indicates a better model.\n",
    "\n",
    "- **AIC (Akaike Information Criterion)**: Based on information theory, AIC estimates the relative quality of models by balancing fit against complexity.\n",
    "\n",
    "- **Elbow Method Visualization**: Plots BIC and AIC across different numbers of components.\n",
    "\n",
    "- **Covariance Type Comparison**: Compares model performance across different covariance structures.\n",
    "\n",
    "- **Comprehensive Metrics**: Evaluates using 5 cluster quality metrics (BIC, AIC, Silhouette, Calinski-Harabasz, Davies-Bouldin).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n# PHASE 13: MODEL SELECTION - BIC/AIC CURVES\n# =============================================================================\nprint(\"=\" * 70)\nprint(\"MODEL SELECTION: COMPREHENSIVE GMM EVALUATION\")\nprint(\"=\" * 70)\n# Import additional metrics\nfrom sklearn.mixture import GaussianMixture\nfrom sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings('ignore')\n# Define cluster range to evaluate\nn_components_range = range(2, 16)\n# Storage for metrics\nbic_scores = []\naic_scores = []\nsilhouette_scores = []\ncalinski_scores = []\ndavies_bouldin_scores = []\nprint(f\"\\n[INFO] Evaluating models with k=2 to k=15 clusters...\")\nprint(\"-\" * 70)\nprint(f\"{'k':^4} | {'BIC':^12} | {'AIC':^12} | {'Silhouette':^12} | {'Calinski':^12} | {'Davies':^10}\")\nprint(\"-\" * 70)\nfor n_components in n_components_range:\n    # Create and fit GMM\n    gmm_temp = GaussianMixture(\n        n_components=n_components,\n        covariance_type='full',\n        n_init=5,\n        random_state=42,\n        max_iter=200,\n        reg_covar=1e-6\n    )\n    gmm_temp.fit(X_scaled)\n    \n    # Get predictions\n    labels_temp = gmm_temp.predict(X_scaled)\n    \n    # Calculate all metrics\n    bic = gmm_temp.bic(X_scaled)\n    aic = gmm_temp.aic(X_scaled)\n    silhouette = silhouette_score(X_scaled, labels_temp)\n    calinski = calinski_harabasz_score(X_scaled, labels_temp)\n    davies = davies_bouldin_score(X_scaled, labels_temp)\n    \n    # Store scores\n    bic_scores.append(bic)\n    aic_scores.append(aic)\n    silhouette_scores.append(silhouette)\n    calinski_scores.append(calinski)\n    davies_bouldin_scores.append(davies)\n    \n    # Print progress\n    print(f\"{n_components:^4} | {bic:^12.2f} | {aic:^12.2f} | {silhouette:^12.4f} | {calinski:^12.2f} | {davies:^10.4f}\")\nprint(\"-\" * 70)\n# Find optimal number of components for each metric\nbest_bic_n = list(n_components_range)[np.argmin(bic_scores)]\nbest_aic_n = list(n_components_range)[np.argmin(aic_scores)]\nbest_silhouette_n = list(n_components_range)[np.argmax(silhouette_scores)]\nbest_calinski_n = list(n_components_range)[np.argmax(calinski_scores)]\nbest_davies_n = list(n_components_range)[np.argmin(davies_bouldin_scores)]\nprint(f\"\\n[INFO] OPTIMAL COMPONENTS BY METRIC:\")\nprint(f\"  \u2022 BIC (lower is better):          k = {best_bic_n}\")\nprint(f\"  \u2022 AIC (lower is better):          k = {best_aic_n}\")\nprint(f\"  \u2022 Silhouette (higher is better):  k = {best_silhouette_n}\")\nprint(f\"  \u2022 Calinski-Harabasz (higher is better): k = {best_calinski_n}\")\nprint(f\"  \u2022 Davies-Bouldin (lower is better):    k = {best_davies_n}\")\n# Use best_params from previous analysis\noptimal_k = best_params['n_components']\nprint(f\"\\n[INFO] SELECTED OPTIMAL K: {optimal_k} (from best_params)\")\n# Create comprehensive visualization\nfig, axes = plt.subplots(2, 3, figsize=(18, 12))\nfig.suptitle('GMM Model Selection Analysis\\nEvaluating Clusters k=2 to k=15', \n             fontsize=16, fontweight='bold', y=1.02)\n# Plot 1: BIC and AIC curves\nax1 = axes[0, 0]\nax1.plot(list(n_components_range), bic_scores, 'b-o', linewidth=2, markersize=8, label='BIC')\nax1.plot(list(n_components_range), aic_scores, 'r-s', linewidth=2, markersize=8, label='AIC')\nax1.axvline(x=optimal_k, color='green', linestyle='--', linewidth=2, label=f'Selected k={optimal_k}')\nax1.axvline(x=best_bic_n, color='blue', linestyle=':', alpha=0.7, label=f'Best BIC={best_bic_n}')\nax1.axvline(x=best_aic_n, color='red', linestyle=':', alpha=0.7, label=f'Best AIC={best_aic_n}')\nax1.set_xlabel('Number of Clusters (k)', fontsize=12)\nax1.set_ylabel('Information Criterion Score', fontsize=12)\nax1.set_title('BIC and AIC\\n(Lower is Better)', fontsize=14, fontweight='bold')\nax1.legend(fontsize=9, loc='best')\nax1.grid(True, alpha=0.3)\nax1.set_xticks(list(n_components_range))\n# Plot 2: Silhouette Score\nax2 = axes[0, 1]\nax2.plot(list(n_components_range), silhouette_scores, 'g-^', linewidth=2, markersize=8, color='darkgreen')\nax2.axvline(x=optimal_k, color='red', linestyle='--', linewidth=2, label=f'Selected k={optimal_k}')\nax2.axvline(x=best_silhouette_n, color='green', linestyle=':', alpha=0.7, label=f'Best k={best_silhouette_n}')\nax2.fill_between(list(n_components_range), silhouette_scores, alpha=0.2, color='green')\nax2.set_xlabel('Number of Clusters (k)', fontsize=12)\nax2.set_ylabel('Silhouette Score', fontsize=12)\nax2.set_title('Silhouette Score\\n(Higher is Better)', fontsize=14, fontweight='bold')\nax2.legend(fontsize=10)\nax2.grid(True, alpha=0.3)\nax2.set_xticks(list(n_components_range))\n# Plot 3: Calinski-Harabasz Index\nax3 = axes[0, 2]\nax3.plot(list(n_components_range), calinski_scores, 'm-v', linewidth=2, markersize=8, color='purple')\nax3.axvline(x=optimal_k, color='red', linestyle='--', linewidth=2, label=f'Selected k={optimal_k}')\nax3.axvline(x=best_calinski_n, color='purple', linestyle=':', alpha=0.7, label=f'Best k={best_calinski_n}')\nax3.fill_between(list(n_components_range), calinski_scores, alpha=0.2, color='purple')\nax3.set_xlabel('Number of Clusters (k)', fontsize=12)\nax3.set_ylabel('Calinski-Harabasz Index', fontsize=12)\nax3.set_title('Calinski-Harabasz Index\\n(Higher is Better)', fontsize=14, fontweight='bold')\nax3.legend(fontsize=10)\nax3.grid(True, alpha=0.3)\nax3.set_xticks(list(n_components_range))\n# Plot 4: Davies-Bouldin Index\nax4 = axes[1, 0]\nax4.plot(list(n_components_range), davies_bouldin_scores, 'c-d', linewidth=2, markersize=8, color='teal')\nax4.axvline(x=optimal_k, color='red', linestyle='--', linewidth=2, label=f'Selected k={optimal_k}')\nax4.axvline(x=best_davies_n, color='teal', linestyle=':', alpha=0.7, label=f'Best k={best_davies_n}')\nax4.fill_between(list(n_components_range), davies_bouldin_scores, alpha=0.2, color='teal')\nax4.set_xlabel('Number of Clusters (k)', fontsize=12)\nax4.set_ylabel('Davies-Bouldin Index', fontsize=12)\nax4.set_title('Davies-Bouldin Index\\n(Lower is Better)', fontsize=14, fontweight='bold')\nax4.legend(fontsize=10)\nax4.grid(True, alpha=0.3)\nax4.set_xticks(list(n_components_range))\n# Plot 5: Combined normalized metrics\nax5 = axes[1, 1]\n# Normalize all metrics to [0, 1] for comparison\nbic_norm = (np.array(bic_scores) - min(bic_scores)) / (max(bic_scores) - min(bic_scores))\naic_norm = (np.array(aic_scores) - min(aic_scores)) / (max(aic_scores) - min(aic_scores))\nsil_norm = (np.array(silhouette_scores) - min(silhouette_scores)) / (max(silhouette_scores) - min(silhouette_scores))\ncalinski_norm = (np.array(calinski_scores) - min(calinski_scores)) / (max(calinski_scores) - min(calinski_scores))\ndavies_norm = (np.array(davies_bouldin_scores) - min(davies_bouldin_scores)) / (max(davies_bouldin_scores) - min(davies_bouldin_scores))\n# For lower-is-better metrics, invert so higher is better\nbic_norm_inv = 1 - bic_norm\naic_norm_inv = 1 - aic_norm\ndavies_norm_inv = 1 - davies_norm\n# Calculate composite score\ncomposite = (bic_norm_inv + aic_norm_inv + sil_norm + calinski_norm + davies_norm_inv) / 5\nax5.plot(list(n_components_range), bic_norm_inv, 'b--', linewidth=1.5, alpha=0.6, label='BIC (inv)')\nax5.plot(list(n_components_range), aic_norm_inv, 'r--', linewidth=1.5, alpha=0.6, label='AIC (inv)')\nax5.plot(list(n_components_range), sil_norm, 'g-', linewidth=2, label='Silhouette')\nax5.plot(list(n_components_range), calinski_norm, 'm-', linewidth=2, label='Calinski')\nax5.plot(list(n_components_range), davies_norm_inv, 'c--', linewidth=1.5, alpha=0.6, label='Davies (inv)')\nax5.plot(list(n_components_range), composite, 'k-', linewidth=3, label='Composite Score')\nax5.axvline(x=optimal_k, color='red', linestyle='--', linewidth=2, label=f'Selected k={optimal_k}')\nax5.set_xlabel('Number of Clusters (k)', fontsize=12)\nax5.set_ylabel('Normalized Score (0-1)', fontsize=12)\nax5.set_title('Normalized Metrics Comparison\\n(Higher is Better for All)', fontsize=14, fontweight='bold')\nax5.legend(fontsize=8, loc='best', ncol=2)\nax5.grid(True, alpha=0.3)\nax5.set_xticks(list(n_components_range))\n# Plot 6: Summary table\nax6 = axes[1, 2]\nax6.axis('off')\nsummary_text = f\"\"\"\nMODEL SELECTION SUMMARY\n{'='*40}\nDataset: X_scaled ({X_scaled.shape[0]:,} samples, {X_scaled.shape[1]} features)\nCluster Range Tested: k = 2 to 15\nOPTIMAL K BY METRIC:\n  \u2022 BIC:          k = {best_bic_n}\n  \u2022 AIC:          k = {best_aic_n}\n  \u2022 Silhouette:   k = {best_silhouette_n}\n  \u2022 Calinski:     k = {best_calinski_n}\n  \u2022 Davies:       k = {best_davies_n}\nSELECTED OPTIMAL K: {optimal_k}\nMETRICS AT k={optimal_k}:\n  \u2022 BIC Score:          {bic_scores[optimal_k-2]:.2f}\n  \u2022 AIC Score:          {aic_scores[optimal_k-2]:.2f}\n  \u2022 Silhouette Score:   {silhouette_scores[optimal_k-2]:.4f}\n  \u2022 Calinski-Harabasz:  {calinski_scores[optimal_k-2]:.2f}\n  \u2022 Davies-Bouldin:     {davies_bouldin_scores[optimal_k-2]:.4f}\n\"\"\"\nax6.text(0.1, 0.95, summary_text, transform=ax6.transAxes, fontsize=11,\n         verticalalignment='top', fontfamily='monospace',\n         bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\nplt.tight_layout()\n# Ensure output directory exists\nimport os\n\n# Save figure\nplt.savefig(os.path.join(FIGURES_DIR, 'plots', '06_bic_aic_analysis.png'), dpi=300, bbox_inches='tight')\nplt.show()\nprint(f\"\\n[OK] Figure saved: os.path.join(FIGURES_DIR, 'plots', '06_bic_aic_analysis.png')\")\n# Save detailed results to CSV\nmodel_selection_df = pd.DataFrame({\n    'n_components': list(n_components_range),\n    'bic_score': bic_scores,\n    'aic_score': aic_scores,\n    'silhouette_score': silhouette_scores,\n    'calinski_harabasz_index': calinski_scores,\n    'davies_bouldin_index': davies_bouldin_scores,\n    'optimal_by_bic': [n == best_bic_n for n in n_components_range],\n    'optimal_by_aic': [n == best_aic_n for n in n_components_range],\n    'optimal_by_silhouette': [n == best_silhouette_n for n in n_components_range],\n    'optimal_by_calinski': [n == best_calinski_n for n in n_components_range],\n    'optimal_by_davies': [n == best_davies_n for n in n_components_range]\n})\n\nmodel_selection_df.to_csv('os.path.join(OUTPUT_DIR, 'metrics')model_selection_results.csv', index=False)\nprint(f\"[OK] Results saved: os.path.join(OUTPUT_DIR, 'metrics')model_selection_results.csv\")\n# Print final summary\nprint(\"\\n\" + \"=\" * 70)\nprint(\"MODEL SELECTION COMPLETE\")\nprint(\"=\" * 70)\nprint(f\"\\nSelected optimal number of clusters: k = {optimal_k}\")\nprint(f\"\\nMetrics saved to: os.path.join(OUTPUT_DIR, 'metrics')model_selection_results.csv\")\nprint(f\"Visualization saved to: os.path.join(FIGURES_DIR, 'plots', '06_bic_aic_analysis.png')\")\nprint(\"=\" * 70)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 14: Radar Charts for Cluster Profiles\n",
    "\n",
    "This section creates radar charts to visualize cluster profiles across multiple health dimensions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PHASE 14: RADAR CHARTS FOR CLUSTER PROFILES\n",
    "# =============================================================================\n",
    "",
    "print(\"=\" * 70)\n",
    "print(\"RADAR CHARTS FOR CLUSTER PROFILES\")\n",
    "print(\"=\" * 70)\n",
    "",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "",
    "def create_radar_chart(ax, categories, values, title, color, alpha=0.25):\n",
    "    \"\"\"Create a radar chart for cluster profile visualization.\"\"\"\n",
    "    N = len(categories)\n",
    "    angles = [n / float(N) * 2 * np.pi for n in range(N)]\n",
    "    angles += angles[:1]  # Complete the loop\n",
    "    values = list(values)  # Convert to list if numpy array\n",
    "    values = values + [values[0]]  # Complete the loop\n",
    "    ax.plot(angles, values, linewidth=2, linestyle='solid', color=color)\n",
    "    ax.fill(angles, values, color=color, alpha=alpha)\n",
    "    ax.set_xticks(angles[:-1])\n",
    "    ax.set_xticklabels(categories, fontsize=9)\n",
    "    ax.set_title(title, fontsize=12, fontweight='bold', pad=20)\n",
    "",
    "# Define features for radar chart\n",
    "radar_features = ['age', 'bmi', 'systolic_bp_mmHg', 'fasting_glucose_mg_dL', 'phq9_total_score']\n",
    "feature_labels = ['Age', 'BMI', 'Systolic BP', 'Glucose', 'PHQ-9']\n",
    "",
    "# Normalize features\n",
    "normalized_data = data.copy()\n",
    "for col in radar_features:\n",
    "    min_val = data[col].min()\n",
    "    max_val = data[col].max()\n",
    "    if max_val > min_val:\n",
    "        normalized_data[col] = (data[col] - min_val) / (max_val - min_val)\n",
    "    else:\n",
    "        normalized_data[col] = 0.5\n",
    "",
    "# Calculate cluster profiles\n",
    "cluster_profiles = normalized_data.groupby(data['cluster'])[radar_features].mean()\n",
    "",
    "# Create radar charts\n",
    "n_clusters = len(cluster_profiles)\n",
    "colors = plt.cm.Set2(np.linspace(0, 1, n_clusters))\n",
    "",
    "fig, axes = plt.subplots(1, n_clusters, figsize=(5*n_clusters, 5), subplot_kw=dict(polar=True))\n",
    "if n_clusters == 1:\n",
    "    axes = [axes]\n",
    "",
    "for idx, (cluster_id, profile) in enumerate(cluster_profiles.iterrows()):\n",
    "    cluster_count = len(data[data['cluster'] == cluster_id])\n",
    "    create_radar_chart(\n",
    "        axes[idx],\n",
    "        feature_labels,\n",
    "        profile.values,\n",
    "        f'Cluster {cluster_id}\\n(n={cluster_count})',\n",
    "        colors[idx]\n",
    "    )\n",
    "",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(FIGURES_DIR, 'plots', '07_radar_charts.png'), dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "",
    "print(f\"\\n[OK] Figure saved: {os.path.join(FIGURES_DIR, 'plots', '07_radar_charts.png')}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combined Cluster Comparison Radar Chart\n",
    "\n",
    "This section provides a combined radar chart comparing all cluster profiles on a single visualization for direct comparison.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PHASE 14 (continued): COMBINED RADAR CHART - CLUSTER PROFILE COMPARISON\n",
    "# =============================================================================\n",
    "",
    "print(\"=\" * 70)\n",
    "print(\"RADAR CHART: CLUSTER PROFILE COMPARISON\")\n",
    "print(\"=\" * 70)\n",
    "",
    "# Select key health indicators for radar chart\n",
    "radar_features = ['bmi', 'systolic_bp_mmHg', 'fasting_glucose_mg_dL', 'phq9_total_score', 'age']\n",
    "radar_features = [f for f in radar_features if f in data.columns]\n",
    "",
    "# Normalize cluster means to 0-1 scale for comparison\n",
    "cluster_means = data.groupby('cluster')[radar_features].mean()\n",
    "feature_mins = data[radar_features].min()\n",
    "feature_maxs = data[radar_features].max()\n",
    "cluster_means_normalized = (cluster_means - feature_mins) / (feature_maxs - feature_mins)\n",
    "",
    "# Create radar chart\n",
    "fig, ax = plt.subplots(figsize=(10, 10), subplot_kw=dict(polar=True))\n",
    "",
    "# Number of features\n",
    "num_vars = len(radar_features)\n",
    "angles = np.linspace(0, 2 * np.pi, num_vars, endpoint=False).tolist()\n",
    "angles += angles[:1]  # Complete the loop\n",
    "",
    "# Colors for each cluster\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, best_params['n_components']))\n",
    "",
    "# Plot each cluster\n",
    "for i in range(best_params['n_components']):\n",
    "    values = cluster_means_normalized.loc[i].tolist()\n",
    "    values += values[:1]  # Complete the loop\n",
    "    ax.plot(angles, values, 'o-', linewidth=2, label=f'Cluster {i} (n={cluster_sizes[i]:,})', color=colors[i])\n",
    "    ax.fill(angles, values, alpha=0.15, color=colors[i])\n",
    "",
    "# Set the labels\n",
    "ax.set_xticks(angles[:-1])\n",
    "labels = ['BMI', 'Systolic BP', 'Glucose', 'PHQ-9 Score', 'Age']\n",
    "labels = labels[:len(radar_features)]\n",
    "ax.set_xticklabels(labels, fontsize=12)\n",
    "ax.set_ylim(0, 1)\n",
    "ax.set_title('Cluster Profiles: Normalized Health Indicators\\n', fontsize=14, fontweight='bold')\n",
    "ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1), fontsize=10)\n",
    "",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(FIGURES_DIR, 'plots', '08_combined_radar.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "",
    "print(f\"\\n[OK] Figure saved: {os.path.join(FIGURES_DIR, 'plots', '08_combined_radar.png')}\")\n",
    "print(\"=\" * 70)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster Size Distribution and Quality Metrics\n",
    "\n",
    "This section analyzes cluster sizes and calculates quality metrics including compactness and separation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n# PHASE 14 (continued): CLUSTER SIZE DISTRIBUTION AND QUALITY METRICS\n# =============================================================================\nprint(\"=\" * 70)\nprint(\"CLUSTER SIZE DISTRIBUTION AND QUALITY METRICS\")\nprint(\"=\" * 70)\n# Calculate cluster sizes\ncluster_sizes = data['cluster'].value_counts().sort_index()\ncluster_proportions = (cluster_sizes / len(data) * 100).round(2)\nprint(\"\\n[INFO] Cluster Size Distribution:\")\nfor cluster in range(best_params['n_components']):\n    size = cluster_sizes[cluster]\n    proportion = cluster_proportions[cluster]\n    print(f\"  Cluster {cluster}: {size:,} samples ({proportion}%)\")\n# Calculate cluster quality metrics\ncluster_metrics = {}\nfor cluster in range(best_params['n_components']):\n    mask = data['cluster'] == cluster\n    cluster_data = X_scaled[mask]\n    \n    # Calculate compactness (average distance to centroid)\n    centroid = cluster_data.mean(axis=0)\n    distances = np.sqrt(((cluster_data - centroid) ** 2).sum(axis=1))\n    compactness = distances.mean()\n    \n    # Calculate separation (distance to nearest other cluster centroid)\n    min_separation = float('inf')\n    for other_cluster in range(best_params['n_components']):\n        if other_cluster != cluster:\n            other_mask = data['cluster'] == other_cluster\n            other_centroid = X_scaled[other_mask].mean(axis=0)\n            sep = np.sqrt(((centroid - other_centroid) ** 2).sum())\n            min_separation = min(min_separation, sep)\n    \n    cluster_metrics[cluster] = {\n        'size': int(size),\n        'proportion': float(proportion),\n        'compactness': float(compactness),\n        'separation': float(min_separation),\n        'compactness_separation_ratio': float(compactness / min_separation) if min_separation > 0 else 0\n    }\n# Create visualization\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n# Cluster size bar chart\ncolors = plt.cm.viridis(np.linspace(0, 1, best_params['n_components']))\nbars = axes[0].bar(range(best_params['n_components']), cluster_sizes.values, color=colors, edgecolor='black')\naxes[0].set_xlabel('Cluster', fontsize=12)\naxes[0].set_ylabel('Number of Samples', fontsize=12)\naxes[0].set_title('Cluster Size Distribution', fontweight='bold')\naxes[0].set_xticks(range(best_params['n_components']))\n# Add count labels on bars\nfor bar, size in zip(bars, cluster_sizes.values):\n    axes[0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 50, \n                 f'{size:,}', ha='center', va='bottom', fontsize=10, fontweight='bold')\n# Compactness vs Separation\ncompactness_vals = [cluster_metrics[c]['compactness'] for c in range(best_params['n_components'])]\nseparation_vals = [cluster_metrics[c]['separation'] for c in range(best_params['n_components'])]\nx_pos = np.arange(best_params['n_components'])\nwidth = 0.35\nbars1 = axes[1].bar(x_pos - width/2, compactness_vals, width, label='Compactness (lower=better)', color='coral', edgecolor='black')\nbars2 = axes[1].bar(x_pos + width/2, separation_vals, width, label='Separation (higher=better)', color='steelblue', edgecolor='black')\naxes[1].set_xlabel('Cluster', fontsize=12)\naxes[1].set_ylabel('Score', fontsize=12)\naxes[1].set_title('Cluster Quality Metrics: Compactness vs Separation', fontweight='bold')\naxes[1].set_xticks(x_pos)\naxes[1].legend(fontsize=10)\naxes[1].grid(True, alpha=0.3, axis='y')\nplt.tight_layout()\nplt.savefig(os.path.join(FIGURES_DIR, 'plots', '09_cluster_quality_metrics.png'), dpi=150, bbox_inches='tight')\nplt.show()\n# Save cluster metrics\nmetrics_df = pd.DataFrame(cluster_metrics).T\nmetrics_df.index.name = 'cluster'\n\nmetrics_df.to_csv(os.path.join(OUTPUT_DIR, 'metrics', 'cluster_quality_metrics.csv'))\nprint(f\"\\n[OK] Figure saved: {os.path.join(FIGURES_DIR, 'plots', '09_cluster_quality_metrics.png')}\")\nprint(f\"[OK] Metrics saved: os.path.join(OUTPUT_DIR, 'metrics')cluster_quality_metrics.csv\")\nprint(\"=\" * 70)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 15: Uncertainty Analysis - Probability Distributions\n",
    "\n",
    "This section analyzes the uncertainty in GMM cluster assignments by examining probability distributions, confidence levels, and assignment entropy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PHASE 15: UNCERTAINTY ANALYSIS - PROBABILITY DISTRIBUTIONS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"UNCERTAINTY ANALYSIS: CLUSTER ASSIGNMENT PROBABILITIES\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Get membership probabilities from the optimal GMM model\n",
    "membership_probs = gmm_optimal.predict_proba(X_scaled)\n",
    "\n",
    "# Analyze certainty levels across all samples\n",
    "max_probs = membership_probs.max(axis=1)\n",
    "high_conf = (max_probs >= 0.8).sum()\n",
    "med_conf = ((max_probs >= 0.5) & (max_probs < 0.8)).sum()\n",
    "low_conf = ((max_probs >= 0.3) & (max_probs < 0.5)).sum()\n",
    "very_low_conf = (max_probs < 0.3).sum()\n",
    "\n",
    "print(\"\\n[INFO] Cluster Assignment Certainty Distribution:\")\n",
    "print(f\"  High Confidence (>=80%):     {high_conf:,} ({100*high_conf/len(max_probs):.1f}%)\")\n",
    "print(f\"  Medium Confidence (50-80%):  {med_conf:,} ({100*med_conf/len(max_probs):.1f}%)\")\n",
    "print(f\"  Low Confidence (30-50%):     {low_conf:,} ({100*low_conf/len(max_probs):.1f}%)\")\n",
    "print(f\"  Very Low Confidence (<30%):  {very_low_conf:,} ({100*very_low_conf/len(max_probs):.1f}%)\")\n",
    "\n",
    "# Calculate assignment entropy for each sample\n",
    "entropy = -np.sum(membership_probs * np.log(membership_probs + 1e-10), axis=1)\n",
    "print(f\"\\n[INFO] Assignment Entropy Statistics:\")\n",
    "print(f\"  Mean Entropy:   {entropy.mean():.4f}\")\n",
    "print(f\"  Std Entropy:    {entropy.std():.4f}\")\n",
    "print(f\"  Min Entropy:    {entropy.min():.4f}\")\n",
    "print(f\"  Max Entropy:    {entropy.max():.4f}\")\n",
    "\n",
    "# Create comprehensive uncertainty visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "fig.suptitle('GMM Cluster Assignment Uncertainty Analysis', fontsize=16, fontweight='bold', y=1.02)\n",
    "\n",
    "# Subplot 1: Distribution of maximum probabilities with threshold lines\n",
    "axes[0, 0].hist(max_probs, bins=50, color='steelblue', edgecolor='white', alpha=0.7)\n",
    "axes[0, 0].axvline(x=0.8, color='green', linestyle='--', linewidth=2, label='High confidence (0.8)')\n",
    "axes[0, 0].axvline(x=0.5, color='orange', linestyle='--', linewidth=2, label='Medium confidence (0.5)')\n",
    "axes[0, 0].axvline(x=0.3, color='red', linestyle='--', linewidth=2, label='Low confidence (0.3)')\n",
    "axes[0, 0].set_xlabel('Maximum Cluster Probability', fontsize=12)\n",
    "axes[0, 0].set_ylabel('Frequency', fontsize=12)\n",
    "axes[0, 0].set_title('Distribution of Cluster Assignment Certainty', fontweight='bold')\n",
    "axes[0, 0].legend(fontsize=10)\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Subplot 2: Certainty categories pie chart\n",
    "certainty_counts = [high_conf, med_conf, low_conf, very_low_conf]\n",
    "certainty_labels = ['High (>=80%)', 'Medium (50-80%)', 'Low (30-50%)', 'Very Low (<30%)']\n",
    "colors = ['#2ecc71', '#f1c40f', '#e67e22', '#e74c3c']\n",
    "explode = (0.05, 0, 0, 0)\n",
    "\n",
    "wedges, texts, autotexts = axes[0, 1].pie(certainty_counts, labels=certainty_labels, autopct='%1.1f%%',\n",
    "                                         colors=colors, explode=explode, shadow=True, startangle=90)\n",
    "axes[0, 1].set_title('Cluster Assignment Certainty Categories', fontweight='bold')\n",
    "\n",
    "# Subplot 3: Probability distributions by cluster (overlapping histograms)\n",
    "for i in range(best_params['n_components']):\n",
    "    axes[1, 0].hist(membership_probs[:, i], bins=30, alpha=0.5, label=f'Cluster {i}', edgecolor='white')\n",
    "axes[1, 0].set_xlabel('Membership Probability', fontsize=12)\n",
    "axes[1, 0].set_ylabel('Frequency', fontsize=12)\n",
    "axes[1, 0].set_title('Probability Distribution by Cluster', fontweight='bold')\n",
    "axes[1, 0].legend(fontsize=10)\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Subplot 4: Entropy distribution histogram\n",
    "axes[1, 1].hist(entropy, bins=50, color='purple', edgecolor='white', alpha=0.7)\n",
    "axes[1, 1].axvline(x=entropy.mean(), color='red', linestyle='--', linewidth=2, label=f'Mean ({entropy.mean():.3f})')\n",
    "axes[1, 1].set_xlabel('Assignment Entropy', fontsize=12)\n",
    "axes[1, 1].set_ylabel('Frequency', fontsize=12)\n",
    "axes[1, 1].set_title('Distribution of Assignment Entropy\\n(Lower = More Certain)', fontweight='bold')\n",
    "axes[1, 1].legend(fontsize=10)\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(FIGURES_DIR, 'plots', '08_uncertainty_analysis.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Save uncertainty metrics to CSV\n",
    "uncertainty_df = pd.DataFrame({\n",
    "    'respondent_id': range(len(data)),\n",
    "    'cluster': data['cluster'],\n",
    "    'max_probability': max_probs,\n",
    "    'entropy': entropy\n",
    "})\n",
    "\n",
    "# Add probability columns for each cluster\n",
    "for i in range(best_params['n_components']):\n",
    "    uncertainty_df[f'cluster_{i}_probability'] = membership_probs[:, i]\n",
    "\n",
    "uncertainty_df.to_csv(os.path.join(OUTPUT_DIR, 'metrics', 'uncertainty_analysis.csv'), index=False)\n",
    "\n",
    "print(f\"\\n[OK] Figure saved: {os.path.join(FIGURES_DIR, 'plots', '08_uncertainty_analysis.png')}\")\n",
    "print(f\"[OK] Analysis saved: {os.path.join(OUTPUT_DIR, 'metrics', 'uncertainty_analysis.csv')}\")\n",
    "print(\"=\" * 70)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 16: Feature Distribution by Cluster\n",
    "\n",
    "This section visualizes the distribution of key health features within each cluster using box plots and violin plots.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PHASE 16: FEATURE DISTRIBUTION BY CLUSTER\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"FEATURE DISTRIBUTION BY CLUSTER\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "# Features to plot\n",
    "plot_features = ['age', 'bmi', 'systolic_bp_mmHg', 'fasting_glucose_mg_dL', 'phq9_total_score']\n",
    "plot_titles = ['Age (years)', 'BMI (kg/m\\u00b2)', 'Systolic BP (mmHg)', 'Fasting Glucose (mg/dL)', 'PHQ-9 Score']\n",
    "\n",
    "# Create box plots\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, (feature, title) in enumerate(zip(plot_features, plot_titles)):\n",
    "    sns.boxplot(data=data, x='cluster', y=feature, ax=axes[idx], palette='Set2')\n",
    "    axes[idx].set_title(f'{title} by Cluster', fontsize=12, fontweight='bold')\n",
    "    axes[idx].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "axes[5].axis('off')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(FIGURES_DIR, 'plots', '09_feature_boxplots.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n[OK] Figure saved: {os.path.join(FIGURES_DIR, 'plots', '09_feature_boxplots.png')}\")\n",
    "\n",
    "# Create violin plots\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, (feature, title) in enumerate(zip(plot_features, plot_titles)):\n",
    "    sns.violinplot(data=data, x='cluster', y=feature, ax=axes[idx], palette='Set2', inner='box')\n",
    "    axes[idx].set_title(f'{title} Distribution by Cluster', fontsize=12, fontweight='bold')\n",
    "    axes[idx].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "axes[5].axis('off')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(FIGURES_DIR, 'plots', '10_feature_violin.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"[OK] Figure saved: {os.path.join(FIGURES_DIR, 'plots', '10_feature_violin.png')}\")\n",
    "print(\"=\" * 70)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 17: Probability Uncertainty Visualization\n",
    "\n",
    "This section provides detailed visualizations of cluster assignment confidence, including box plots by cluster and probability heatmaps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PHASE 17: PROBABILITY UNCERTAINTY VISUALIZATION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"PROBABILITY UNCERTAINTY VISUALIZATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Get maximum probability for each sample from membership probabilities\n",
    "max_probs = membership_probs.max(axis=1)\n",
    "data['max_probability'] = max_probs\n",
    "\n",
    "# Categorize confidence into three levels\n",
    "high_conf = (max_probs >= 0.8).sum()\n",
    "mod_conf = ((max_probs >= 0.5) & (max_probs < 0.8)).sum()\n",
    "low_conf = (max_probs < 0.5).sum()\n",
    "\n",
    "print(f\"\\n[INFO] Confidence Level Summary:\")\n",
    "print(f\"  High Confidence (>=0.8): {high_conf:,} ({100*high_conf/len(data):.1f}%)\")\n",
    "print(f\"  Moderate Confidence (0.5-0.8): {mod_conf:,} ({100*mod_conf/len(data):.1f}%)\")\n",
    "print(f\"  Low Confidence (<0.5): {low_conf:,} ({100*low_conf/len(data):.1f}%)\")\n",
    "\n",
    "# Create comprehensive visualizations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "fig.suptitle('GMM Probability Uncertainty Analysis', fontsize=16, fontweight='bold', y=1.02)\n",
    "\n",
    "# Subplot 1: Histogram of maximum probabilities\n",
    "axes[0, 0].hist(max_probs, bins=50, color='steelblue', edgecolor='black', alpha=0.7)\n",
    "axes[0, 0].axvline(x=0.8, color='green', linestyle='--', linewidth=2, label='High (0.8)')\n",
    "axes[0, 0].axvline(x=0.5, color='orange', linestyle='--', linewidth=2, label='Moderate (0.5)')\n",
    "axes[0, 0].set_xlabel('Maximum Assignment Probability', fontsize=12)\n",
    "axes[0, 0].set_ylabel('Frequency', fontsize=12)\n",
    "axes[0, 0].set_title('Distribution of Assignment Confidence', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].legend(fontsize=10)\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Subplot 2: Pie chart of confidence levels\n",
    "confidence_counts = [high_conf, mod_conf, low_conf]\n",
    "confidence_labels = [f'High\\n(n={high_conf:,})', f'Moderate\\n(n={mod_conf:,})', f'Low\\n(n={low_conf:,})']\n",
    "colors = ['#2ecc71', '#f39c12', '#e74c3c']\n",
    "axes[0, 1].pie(confidence_counts, labels=confidence_labels, autopct='%1.1f%%',\n",
    "               colors=colors, explode=[0.02, 0.02, 0.05], shadow=True, startangle=90)\n",
    "axes[0, 1].set_title('Confidence Level Distribution', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Subplot 3: Box plot of confidence by cluster\n",
    "cluster_prob_data = [data[data['cluster'] == c]['max_probability'].values for c in sorted(data['cluster'].unique())]\n",
    "bp = axes[1, 0].boxplot(cluster_prob_data, labels=[f'Cluster {c}' for c in sorted(data['cluster'].unique())],\n",
    "                        patch_artist=True)\n",
    "for patch, color in zip(bp['boxes'], plt.cm.Set2(np.linspace(0, 1, len(cluster_prob_data)))):\n",
    "    patch.set_facecolor(color)\n",
    "axes[1, 0].set_xlabel('Cluster', fontsize=12)\n",
    "axes[1, 0].set_ylabel('Maximum Probability', fontsize=12)\n",
    "axes[1, 0].set_title('Assignment Confidence by Cluster', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Subplot 4: Cluster probability heatmap\n",
    "prob_means = membership_probs.mean(axis=0)\n",
    "# Reshape to 2D array for imshow (1 row, n_clusters columns)\n",
    "prob_means_2d = prob_means.reshape(1, -1)\n",
    "im = axes[1, 1].imshow(prob_means_2d, aspect='auto', cmap='YlOrRd')\n",
    "axes[1, 1].set_xlabel('Sample Index (sorted)', fontsize=12)\n",
    "axes[1, 1].set_ylabel('Cluster', fontsize=12)\n",
    "axes[1, 1].set_title('Cluster Probability Heatmap', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].set_yticks(range(len(prob_means)))\n",
    "axes[1, 1].set_yticklabels([f'Cluster {i}' for i in range(len(prob_means))])\n",
    "plt.colorbar(im, ax=axes[1, 1], label='Mean Probability')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(FIGURES_DIR, 'plots', '11_probability_uncertainty.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Save detailed confidence metrics\n",
    "confidence_df = pd.DataFrame({\n",
    "    'respondent_id': range(len(data)),\n",
    "    'cluster': data['cluster'],\n",
    "    'max_probability': max_probs,\n",
    "    'confidence_category': pd.cut(max_probs, bins=[0, 0.5, 0.8, 1.0],\n",
    "                                  labels=['Low', 'Moderate', 'High'])\n",
    "})\n",
    "confidence_df.to_csv(os.path.join(OUTPUT_DIR, 'metrics', 'confidence_analysis.csv'), index=False)\n",
    "\n",
    "print(f\"\\n[OK] Figure saved: {os.path.join(FIGURES_DIR, 'plots', '11_probability_uncertainty.png')}\")\n",
    "print(f\"[OK] Analysis saved: {os.path.join(OUTPUT_DIR, 'metrics', 'confidence_analysis.csv')}\")\n",
    "print(\"=\" * 70)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 18: Cluster Size and Proportion Analysis\n",
    "\n",
    "This section analyzes the distribution of samples across clusters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PHASE 18: CLUSTER SIZE AND PROPORTION ANALYSIS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"CLUSTER SIZE AND PROPORTION ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Calculate cluster sizes and proportions\n",
    "cluster_sizes = data['cluster'].value_counts().sort_index()\n",
    "cluster_proportions = (cluster_sizes / len(data)) * 100\n",
    "\n",
    "print(\"\\n[INFO] Cluster Distribution:\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"{'Cluster':<10} {'Count':<12} {'Proportion':<15}\")\n",
    "print(\"-\" * 50)\n",
    "for cluster in cluster_sizes.index:\n",
    "    print(f\"{cluster:<10} {cluster_sizes[cluster]:<12,} {cluster_proportions[cluster]:.2f}%\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"{'Total':<10} {len(data):<12,} {100.0:.2f}%\")\n",
    "\n",
    "# Create visualizations\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "fig.suptitle('Cluster Size Distribution Analysis', fontsize=16, fontweight='bold', y=1.02)\n",
    "\n",
    "# Pie chart\n",
    "colors = plt.cm.Set2(np.linspace(0, 1, len(cluster_sizes)))\n",
    "axes[0].pie(cluster_sizes, labels=[f'Cluster {i}\\n(n={v:,})' for i, v in cluster_sizes.items()],\n",
    "            autopct='%1.1f%%', colors=colors, explode=[0.02]*len(cluster_sizes),\n",
    "            shadow=True, startangle=90)\n",
    "axes[0].set_title('Cluster Distribution (Pie Chart)', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Bar chart\n",
    "bars = axes[1].bar(cluster_sizes.index, cluster_sizes.values, color=colors, edgecolor='black', alpha=0.8)\n",
    "axes[1].set_xlabel('Cluster', fontsize=12)\n",
    "axes[1].set_ylabel('Number of Individuals', fontsize=12)\n",
    "axes[1].set_title('Cluster Size Distribution (Bar Chart)', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xticks(cluster_sizes.index)\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "for bar, count in zip(bars, cluster_sizes.values):\n",
    "    height = bar.get_height()\n",
    "    axes[1].annotate(f'{count:,}\\n({count/len(data)*100:.1f}%)',\n",
    "                    xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                    xytext=(0, 3), textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(FIGURES_DIR, 'plots', '12_cluster_distribution.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Save cluster distribution data\n",
    "distribution_df = pd.DataFrame({\n",
    "    'cluster': cluster_sizes.index,\n",
    "    'count': cluster_sizes.values,\n",
    "    'proportion': cluster_proportions.values\n",
    "})\n",
    "distribution_df.to_csv(os.path.join(OUTPUT_DIR, 'metrics', 'cluster_distribution.csv'), index=False)\n",
    "\n",
    "print(f\"\\n[OK] Figure saved: {os.path.join(FIGURES_DIR, 'plots', '12_cluster_distribution.png')}\")\n",
    "print(f\"[OK] Distribution saved: {os.path.join(OUTPUT_DIR, 'metrics', 'cluster_distribution.csv')}\")\n",
    "print(\"=\" * 70)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 19: Demographics and Cluster Association\n",
    "\n",
    "This section analyzes the relationship between demographic variables and cluster membership.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PHASE 19: DEMOGRAPHICS AND CLUSTER ASSOCIATION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"DEMOGRAPHICS AND CLUSTER ASSOCIATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "demographic_vars = ['gender', 'race/ethnicity', 'age_group']\n",
    "demographic_names = ['Gender', 'Race/Ethnicity', 'Age Group']\n",
    "\n",
    "print(\"\\n[INFO] Chi-Square Tests for Demographics:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "chi2_results = {}\n",
    "for var, name in zip(demographic_vars, demographic_names):\n",
    "    if var in data.columns:\n",
    "        contingency_table = pd.crosstab(data[var], data['cluster'])\n",
    "        chi2, p_value, dof, expected = chi2_contingency(contingency_table)\n",
    "        chi2_results[name] = {'chi2': chi2, 'p_value': p_value, 'dof': dof}\n",
    "        print(f\"\\n{name}:\")\n",
    "        print(f\"  Chi-square statistic: {chi2:.4f}\")\n",
    "        print(f\"  P-value: {p_value:.6f}\")\n",
    "        print(f\"  Significant (p<0.05): {'Yes' if p_value < 0.05 else 'No'}\")\n",
    "\n",
    "# Create visualizations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "fig.suptitle('Demographic Analysis by Cluster', fontsize=16, fontweight='bold', y=1.02)\n",
    "\n",
    "if 'gender' in data.columns:\n",
    "    gender_cluster = pd.crosstab(data['gender'], data['cluster'], normalize='index') * 100\n",
    "    gender_cluster.plot(kind='bar', ax=axes[0, 0], colormap='Set2', edgecolor='black')\n",
    "    axes[0, 0].set_title('Cluster Distribution by Gender', fontsize=14, fontweight='bold')\n",
    "    axes[0, 0].tick_params(axis='x', rotation=0)\n",
    "    axes[0, 0].set_ylabel('Percentage (%)')\n",
    "    axes[0, 0].legend(title='Cluster', fontsize=10)\n",
    "    axes[0, 0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "if 'age_group' in data.columns:\n",
    "    age_cluster = pd.crosstab(data['age_group'], data['cluster'], normalize='index') * 100\n",
    "    age_cluster.plot(kind='bar', ax=axes[0, 1], colormap='Set2', edgecolor='black')\n",
    "    axes[0, 1].set_title('Cluster Distribution by Age Group', fontsize=14, fontweight='bold')\n",
    "    axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "    axes[0, 1].set_ylabel('Percentage (%)')\n",
    "    axes[0, 1].legend(title='Cluster', fontsize=10)\n",
    "    axes[0, 1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "if 'race/ethnicity' in data.columns:\n",
    "    race_cluster = pd.crosstab(data['cluster'], data['race/ethnicity'], normalize='index') * 100\n",
    "    race_cluster.plot(kind='barh', stacked=True, ax=axes[1, 0], colormap='Set2', edgecolor='black')\n",
    "    axes[1, 0].set_title('Race/Ethnicity by Cluster', fontsize=14, fontweight='bold')\n",
    "    axes[1, 0].set_xlabel('Percentage (%)')\n",
    "    axes[1, 0].legend(title='Race/Ethnicity', fontsize=8, loc='lower right')\n",
    "    axes[1, 0].grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# Age distribution by cluster\n",
    "for cluster in sorted(data['cluster'].unique()):\n",
    "    cluster_ages = data[data['cluster'] == cluster]['age']\n",
    "    axes[1, 1].hist(cluster_ages, bins=20, alpha=0.5, label=f'Cluster {cluster}', edgecolor='black')\n",
    "axes[1, 1].set_title('Age Distribution by Cluster', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].set_xlabel('Age (years)', fontsize=12)\n",
    "axes[1, 1].set_ylabel('Frequency', fontsize=12)\n",
    "axes[1, 1].legend(fontsize=10)\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(FIGURES_DIR, 'plots', '13_demographic_analysis.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Save chi-square results\n",
    "chi2_df = pd.DataFrame(chi2_results).T\n",
    "chi2_df.to_csv(os.path.join(OUTPUT_DIR, 'metrics', 'chi_square_results.csv'))\n",
    "\n",
    "print(f\"\\n[OK] Figure saved: {os.path.join(FIGURES_DIR, 'plots', '13_demographic_analysis.png')}\")\n",
    "print(f\"[OK] Chi-square results saved: {os.path.join(OUTPUT_DIR, 'metrics', 'chi_square_results.csv')}\")\n",
    "print(\"=\" * 70)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 20: Final Summary and Export\n",
    "\n",
    "This section provides a comprehensive summary of the GMM clustering analysis and exports all results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PHASE 20: FINAL SUMMARY AND EXPORT\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"FINAL SUMMARY AND EXPORT\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Complete summary statistics\n",
    "print(\"\\n\" + \"-\" * 70)\n",
    "print(\"COMPLETE PROJECT SUMMARY\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "print(f\"\\n[DATASET CHARACTERISTICS]\")\n",
    "print(f\"  Total Samples: {len(data):,}\")\n",
    "print(f\"  Features Used: {len(FEATURE_COLUMNS)}\")\n",
    "\n",
    "print(f\"\\n[MODEL CONFIGURATION]\")\n",
    "print(f\"  Algorithm: Gaussian Mixture Models (GMM)\")\n",
    "print(f\"  Number of Components: {best_params['n_components']}\")\n",
    "print(f\"  Covariance Type: {best_params['covariance_type']}\")\n",
    "print(f\"  Number of Initializations: {best_params['n_init']}\")\n",
    "\n",
    "print(f\"\\n[MODEL PERFORMANCE]\")\n",
    "print(f\"  BIC Score: {full_eval['bic']:.2f}\")\n",
    "print(f\"  AIC Score: {full_eval['aic']:.2f}\")\n",
    "print(f\"  Silhouette Score: {full_eval['silhouette']:.4f}\")\n",
    "\n",
    "print(f\"\\n[CLUSTER SUMMARY]\")\n",
    "n_clusters = best_params['n_components']\n",
    "for c in range(n_clusters):\n",
    "    cluster_subset = data[data['cluster'] == c]\n",
    "    print(f\"  Cluster {c} ({len(cluster_subset):,} individuals, {100*len(cluster_subset)/len(data):.1f}%):\")\n",
    "    print(f\"    Mean Age: {cluster_subset['age'].mean():.1f} years\")\n",
    "    print(f\"    Mean BMI: {cluster_subset['bmi'].mean():.1f}\")\n",
    "    print(f\"    Mean Systolic BP: {cluster_subset['systolic_bp_mmHg'].mean():.1f} mmHg\")\n",
    "\n",
    "print(f\"\\n[UNCERTAINTY ANALYSIS]\")\n",
    "print(f\"  High Confidence (>=0.8): {high_conf:,} ({100*high_conf/len(data):.1f}%)\")\n",
    "print(f\"  Moderate Confidence: {mod_conf:,} ({100*mod_conf/len(data):.1f}%)\")\n",
    "print(f\"  Low Confidence (<0.5): {low_conf:,} ({100*low_conf/len(data):.1f}%)\")\n",
    "print(f\"  Mean Assignment Entropy: {entropy.mean():.4f}\")\n",
    "\n",
    "# Export all results\n",
    "print(\"\\n[INFO] Exporting results...\")\n",
    "\n",
    "# Save complete dataset with cluster assignments\n",
    "export_df = data.copy()\n",
    "export_df['max_probability'] = max_probs\n",
    "export_df['entropy'] = entropy\n",
    "export_df.to_csv(os.path.join(OUTPUT_DIR, 'predictions', 'complete_cluster_assignments.csv'), index=False)\n",
    "print(\"  [OK] Complete cluster assignments saved\")\n",
    "\n",
    "# Save cluster profiles\n",
    "cluster_profiles_export = cluster_profiles.copy()\n",
    "cluster_profiles_export['n_individuals'] = cluster_sizes.values\n",
    "cluster_profiles_export['proportion'] = cluster_proportions.values\n",
    "cluster_profiles_export.to_csv(os.path.join(OUTPUT_DIR, 'cluster_profiles', 'detailed_cluster_profiles.csv'))\n",
    "print(\"  [OK] Detailed cluster profiles saved\")\n",
    "\n",
    "# Save probability assignments\n",
    "prob_df = pd.DataFrame(membership_probs, columns=[f'Cluster_{i}_Prob' for i in range(n_clusters)])\n",
    "prob_df['Predicted_Cluster'] = data['cluster'].values\n",
    "prob_df['Max_Probability'] = max_probs\n",
    "prob_df['Entropy'] = entropy\n",
    "prob_df.to_csv(os.path.join(OUTPUT_DIR, 'predictions', 'cluster_probabilities.csv'), index=False)\n",
    "print(\"  [OK] Cluster probabilities saved\")\n",
    "\n",
    "# Save model using the save_model function\n",
    "model_filepath = save_model(gmm_optimal, 'final_gmm_model', subdir='final')\n",
    "print(f\"  [OK] Final model saved to: {model_filepath}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"ANALYSIS COMPLETE\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nAll results saved to: {OUTPUT_DIR}\")\n",
    "print(f\"Figures saved to: {os.path.join(FIGURES_DIR, 'plots')}\")\n",
    "print(f\"Models saved to: {MODELS_DIR}\")\n",
    "print(\"=\" * 70)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 21: References\n",
    "\n",
    "1. McLachlan, G. J., & Peel, D. (2000). Finite Mixture Models. John Wiley & Sons.\n",
    "\n",
    "2. Schwarz, G. (1978). Estimating the dimension of a model. The Annals of Statistics.\n",
    "\n",
    "3. Pedregosa, F., et al. (2011). Scikit-learn: Machine Learning in Python. JMLR 12, 2825-2830.\n",
    "\n",
    "4. NHANES Documentation - National Center for Health Statistics, CDC.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}