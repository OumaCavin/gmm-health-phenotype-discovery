{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GMM Health Phenotype Discovery\n",
    "\n",
    "## MSc Public Health Data Science - SDS6217 Advanced Machine Learning\n",
    "\n",
    "---\n",
    "\n",
    "**Student ID:** SDS6/46982/2025  \n",
    "**Date:** January 2025  \n",
    "**Institution:** University of Nairobi  \n",
    "\n",
    "---\n",
    "\n",
    "### Project Overview\n",
    "\n",
    "This comprehensive data science project applies Gaussian Mixture Models (GMM) to identify latent subpopulations in public health data, demonstrating how probabilistic clustering can capture population heterogeneity that traditional hard-clustering methods may miss.\n",
    "\n",
    "### Key Features\n",
    "\n",
    "- **Probabilistic Clustering**: Captures uncertainty in cluster assignments\n",
    "- **Hyperparameter Tuning**: Systematic grid search optimization\n",
    "- **Population Phenotype Discovery**: Identifies distinct health subgroups\n",
    "- **Academic Rigor**: Comprehensive methodology suitable for MSc-level assessment\n",
    "\n",
    "### Why GMM for Public Health?\n",
    "\n",
    "1. **Probabilistic Cluster Assignment**: Unlike K-Means which forces hard assignments, GMM provides posterior probabilities. Each individual receives a probability of belonging to each cluster, which is critical for health decisions where uncertainty quantification matters.\n",
    "\n",
    "2. **Modeling Population Heterogeneity**: Health populations naturally exhibit continuous distributions of risk factors. GMM captures latent subgroups without imposing artificial boundaries, reflecting the biological reality of disease processes.\n",
    "\n",
    "3. **Flexibility Through Covariance Structures**: Four covariance types allow modeling of various cluster shapes. Full covariance captures elongated, correlated clusters, while diagonal and spherical options provide computational efficiency.\n",
    "\n",
    "4. **Uncertainty Quantification**: Confidence in cluster assignments can be assessed, which is important for clinical decision-making and resource allocation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "================================================================================\n",
    "PROJECT CONFIGURATION - EMBEDDED PATHS AND UTILITIES\n",
    "================================================================================\n",
    "\n",
    "This module provides centralized project configuration, path management, and \n",
    "utility functions for the GMM Health Phenotype Discovery project.\n",
    "\n",
    "Author: Cavin Otieno\n",
    "Student ID: SDS6/46982/2025\n",
    "MSc Public Health Data Science - SDS6217 Advanced Machine Learning\n",
    "University of Nairobi\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import joblib\n",
    "import json\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# =============================================================================\n",
    "# PROJECT ROOT AND DIRECTORY PATHS\n",
    "# =============================================================================\n",
    "\n",
    "# Define project root directory (current working directory)\n",
    "PROJECT_ROOT = os.path.abspath(os.path.dirname('__file__'))\n",
    "\n",
    "# Define main directory paths\n",
    "DATA_DIR = os.path.join(PROJECT_ROOT, 'data')\n",
    "OUTPUT_DIR = os.path.join(PROJECT_ROOT, 'output_v2')\n",
    "MODELS_DIR = os.path.join(PROJECT_ROOT, 'models')\n",
    "FIGURES_DIR = os.path.join(PROJECT_ROOT, 'figures')\n",
    "\n",
    "# Define phase-specific subdirectories\n",
    "PHASE_DIRS = {\n",
    "    'data': os.path.join(DATA_DIR, 'raw'),\n",
    "    'processed': os.path.join(DATA_DIR, 'processed'),\n",
    "    'reports': os.path.join(OUTPUT_DIR, 'reports'),\n",
    "    'logs': os.path.join(OUTPUT_DIR, 'logs'),\n",
    "    'plots': os.path.join(FIGURES_DIR, 'plots')\n",
    "}\n",
    "\n",
    "# Define model subdirectories\n",
    "MODEL_SUBDIRS = {\n",
    "    'gmm_clustering': os.path.join(MODELS_DIR, 'gmm_clustering'),\n",
    "    'baseline': os.path.join(MODELS_DIR, 'baseline'),\n",
    "    'tuned': os.path.join(MODELS_DIR, 'tuned'),\n",
    "    'final': os.path.join(MODELS_DIR, 'final'),\n",
    "    'comparison': os.path.join(MODELS_DIR, 'comparison')\n",
    "}\n",
    "\n",
    "# Define output subdirectories\n",
    "OUTPUT_SUBDIRS = {\n",
    "    'metrics': os.path.join(OUTPUT_DIR, 'metrics'),\n",
    "    'predictions': os.path.join(OUTPUT_DIR, 'predictions'),\n",
    "    'thresholds': os.path.join(OUTPUT_DIR, 'thresholds'),\n",
    "    'fairness': os.path.join(OUTPUT_DIR, 'fairness'),\n",
    "    'validation': os.path.join(OUTPUT_DIR, 'validation'),\n",
    "    'cluster_profiles': os.path.join(OUTPUT_DIR, 'cluster_profiles')\n",
    "}\n",
    "\n",
    "# =============================================================================\n",
    "# UTILITY FUNCTIONS\n",
    "# =============================================================================\n",
    "\n",
    "def create_directory_structure():\n",
    "    \"\"\"Create all project directories if they don't exist.\"\"\"\n",
    "    all_dirs = [\n",
    "        PROJECT_ROOT, DATA_DIR, OUTPUT_DIR, MODELS_DIR, FIGURES_DIR,\n",
    "        *PHASE_DIRS.values(), *MODEL_SUBDIRS.values(), *OUTPUT_SUBDIRS.values()\n",
    "    ]\n",
    "    created = []\n",
    "    for dir_path in all_dirs:\n",
    "        if dir_path and not os.path.exists(dir_path):\n",
    "            os.makedirs(dir_path, exist_ok=True)\n",
    "            created.append(dir_path)\n",
    "    if created:\n",
    "        print(f\"Created {len(created)} directory(ies)\")\n",
    "    return created\n",
    "\n",
    "def save_fig(figure, filename, subdir=None, formats=['png', 'pdf', 'svg']):\n",
    "    \"\"\"Save a matplotlib figure in multiple formats.\"\"\"\n",
    "    save_dir = FIGURES_DIR\n",
    "    if subdir:\n",
    "        save_dir = os.path.join(FIGURES_DIR, subdir)\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "    saved_files = []\n",
    "    for fmt in formats:\n",
    "        filepath = os.path.join(save_dir, f\"{filename}.{fmt}\")\n",
    "        figure.savefig(filepath, dpi=300, bbox_inches='tight', format=fmt)\n",
    "        saved_files.append(filepath)\n",
    "    return saved_files\n",
    "\n",
    "def save_fig_multi_format(filename, figure=None, subdir=None,\n",
    "                          dpi=300, bbox_inches='tight',\n",
    "                          formats=['png', 'pdf', 'svg']):\n",
    "    \"\"\"Save figure in multiple formats with consistent naming.\"\"\"\n",
    "    if figure is None:\n",
    "        figure = plt.gcf()\n",
    "    save_dir = FIGURES_DIR\n",
    "    if subdir:\n",
    "        save_dir = os.path.join(FIGURES_DIR, subdir)\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "    saved = []\n",
    "    for fmt in formats:\n",
    "        filepath = os.path.join(save_dir, f\"{filename}.{fmt}\")\n",
    "        figure.savefig(filepath, dpi=dpi, bbox_inches=bbox_inches, format=fmt)\n",
    "        saved.append(filepath)\n",
    "    return saved\n",
    "\n",
    "def save_model(model, filename, subdir=None, model_type=None):\n",
    "    \"\"\"Save a trained model using joblib.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : sklearn model\n",
    "        The trained model to save\n",
    "    filename : str\n",
    "        The filename for the model (without extension)\n",
    "    subdir : str, optional\n",
    "        Subdirectory within MODELS_DIR to save to\n",
    "    model_type : str, optional\n",
    "        Alias for subdir - category of model (e.g., 'tuned', 'final')\n",
    "    \"\"\"\n",
    "    directory = subdir if subdir else model_type\n",
    "    if directory and directory in MODEL_SUBDIRS:\n",
    "        save_dir = MODEL_SUBDIRS[directory]\n",
    "    else:\n",
    "        save_dir = MODELS_DIR\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    filepath = os.path.join(save_dir, f\"{filename}.joblib\")\n",
    "    joblib.dump(model, filepath)\n",
    "    return filepath\n",
    "\n",
    "def load_model(filepath):\n",
    "    \"\"\"Load a trained model using joblib.\"\"\"\n",
    "    return joblib.load(filepath)\n",
    "\n",
    "def save_data(data, filename, subdir=None, fmt='csv'):\n",
    "    \"\"\"Save data (DataFrame or array) to file.\"\"\"\n",
    "    if subdir and subdir in OUTPUT_SUBDIRS:\n",
    "        save_dir = OUTPUT_SUBDIRS[subdir]\n",
    "    else:\n",
    "        save_dir = OUTPUT_DIR\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    filepath = os.path.join(save_dir, f\"{filename}.{fmt}\")\n",
    "    if fmt == 'csv':\n",
    "        if hasattr(data, 'to_csv'):\n",
    "            data.to_csv(filepath, index=False)\n",
    "        else:\n",
    "            pd.DataFrame(data).to_csv(filepath, index=False)\n",
    "    elif fmt == 'json':\n",
    "        import json\n",
    "        with open(filepath, 'w') as f:\n",
    "            json.dump(data, f, indent=2)\n",
    "    return filepath\n",
    "\n",
    "def load_data(filepath):\n",
    "    \"\"\"Load data from file.\"\"\"\n",
    "    if filepath.endswith('.csv'):\n",
    "        return pd.read_csv(filepath)\n",
    "    elif filepath.endswith('.json'):\n",
    "        import json\n",
    "        with open(filepath, 'r') as f:\n",
    "            return json.load(f)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file format: {filepath}\")\n",
    "\n",
    "def get_data_path(filename):\n",
    "    \"\"\"Get the full path to a data file in the raw data directory.\"\"\"\n",
    "    return os.path.join(PHASE_DIRS['data'], filename)\n",
    "\n",
    "def display_configuration():\n",
    "    \"\"\"Display current project configuration.\"\"\"\n",
    "    config = {\n",
    "        'Project Root': PROJECT_ROOT,\n",
    "        'Data Directory': DATA_DIR,\n",
    "        'Output Directory': OUTPUT_DIR,\n",
    "        'Models Directory': MODELS_DIR,\n",
    "        'Figures Directory': FIGURES_DIR,\n",
    "        'Raw Data Path': PHASE_DIRS['data'],\n",
    "        'Processed Data Path': PHASE_DIRS['processed'],\n",
    "        'Timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    }\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"PROJECT CONFIGURATION\")\n",
    "    print(\"=\" * 60)\n",
    "    for key, value in config.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "# Create all directories on import\n",
    "created_dirs = create_directory_structure()\n",
    "\n",
    "# Display configuration\n",
    "display_configuration()\n",
    "\n",
    "print(\"\\nProject configuration loaded successfully!\")\n",
    "print(f\"Raw data directory: {PHASE_DIRS['data']}\")\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")\n",
    "print(f\"Models directory: {MODELS_DIR}\")\n",
    "print(f\"Figures directory: {FIGURES_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PHASE 1: LIBRARY IMPORTS AND ENVIRONMENT SETUP\n",
    "# =============================================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import sys\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configure display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "\n",
    "# Set matplotlib style for academic publications\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Print versions for reproducibility\n",
    "print(\"=\" * 70)\n",
    "print(\"ENVIRONMENT AND VERSION INFORMATION\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Python Version: {sys.version}\")\n",
    "print(f\"NumPy Version: {np.__version__}\")\n",
    "print(f\"Pandas Version: {pd.__version__}\")\n",
    "print(f\"Matplotlib Version: {plt.matplotlib.__version__}\")\n",
    "print(f\"Seaborn Version: {sns.__version__}\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PHASE 2: DATA ACQUISITION AND DOWNLOAD\n",
    "# =============================================================================\n",
    "\n",
    "\"\"\"\n",
    "Dataset: Behavioral Risk Factor Surveillance System (BRFSS) 2023\n",
    "Source: CDC BRFSS Annual Data\n",
    "URL: https://www.cdc.gov/brfss/annual_data/annual_2023.html\n",
    "\n",
    "The BRFSS is the nation's premier system of health-related telephone surveys \n",
    "that collect state data about U.S. residents regarding their health-related \n",
    "risk behaviors, chronic health conditions, and use of preventive services.\n",
    "\"\"\"\n",
    "\n",
    "import urllib.request\n",
    "import zipfile\n",
    "\n",
    "def download_brfss_data(output_path, year=2023):\n",
    "    \"\"\"\n",
    "    Download BRFSS data from CDC or alternative sources.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    output_path : str\n",
    "        Directory to save the downloaded data\n",
    "    year : int\n",
    "        Year of BRFSS data to download\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    str : Path to the downloaded file\n",
    "    \"\"\"\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    \n",
    "    # For this project, we'll use a synthetic dataset that simulates BRFSS\n",
    "    # The synthetic data is created to have realistic distributions\n",
    "    # In production, you would download from: https://www.cdc.gov/brfss/annual_data/\n",
    "    \n",
    "    print(f\"BRFSS {year} data would be downloaded from CDC\")\n",
    "    print(f\"For this demonstration, we use a synthetic BRFSS-like dataset\")\n",
    "    print(f\"Raw data directory: {output_path}\")\n",
    "    \n",
    "    return output_path\n",
    "\n",
    "# Download data\n",
    "raw_data_path = PHASE_DIRS['data']\n",
    "download_brfss_data(raw_data_path, year=2023)\n",
    "\n",
    "print(f\"\\n[INFO] Raw data will be saved to: {raw_data_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_synthetic_brfss_data(n_samples=3000):\n",
    "    \"\"\"\n",
    "    Generate synthetic public health dataset simulating BRFSS data.\n",
    "    This creates realistic health indicators with known cluster structure.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    n_samples : int\n",
    "        Number of samples to generate\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame : Synthetic health dataset\n",
    "    \"\"\"\n",
    "    print(\"Generating synthetic BRFSS-like health dataset...\")\n",
    "    \n",
    "    # Define cluster parameters to simulate distinct health phenotypes\n",
    "    # Cluster 1: Health-conscious individuals (35% of population)\n",
    "    n_cluster1 = int(n_samples * 0.35)\n",
    "    cluster1 = {\n",
    "        'age': np.random.normal(45, 12, n_cluster1),\n",
    "        'bmi': np.random.normal(23, 3, n_cluster1),\n",
    "        'physical_activity': np.random.normal(5, 1.5, n_cluster1),\n",
    "        'sleep_hours': np.random.normal(7.5, 0.8, n_cluster1),\n",
    "        'fruit_vegetable_intake': np.random.normal(4, 1, n_cluster1),\n",
    "        'alcohol_consumption': np.random.normal(2, 2, n_cluster1),\n",
    "        'smoking_status': np.random.binomial(1, 0.1, n_cluster1),\n",
    "        'healthcare_visits': np.random.poisson(2, n_cluster1),\n",
    "        'chronic_conditions': np.random.poisson(0.3, n_cluster1),\n",
    "        'mental_health_days': np.random.normal(3, 2, n_cluster1),\n",
    "        'stress_level': np.random.normal(4, 1.5, n_cluster1),\n",
    "        'blood_pressure_systolic': np.random.normal(118, 10, n_cluster1),\n",
    "        'cholesterol_total': np.random.normal(185, 25, n_cluster1),\n",
    "        'glucose_level': np.random.normal(95, 10, n_cluster1)\n",
    "    }\n",
    "    \n",
    "    # Cluster 2: Moderate risk individuals (40% of population)\n",
    "    n_cluster2 = int(n_samples * 0.40)\n",
    "    cluster2 = {\n",
    "        'age': np.random.normal(52, 10, n_cluster2),\n",
    "        'bmi': np.random.normal(27, 4, n_cluster2),\n",
    "        'physical_activity': np.random.normal(2.5, 1.5, n_cluster2),\n",
    "        'sleep_hours': np.random.normal(6.5, 1, n_cluster2),\n",
    "        'fruit_vegetable_intake': np.random.normal(2, 0.8, n_cluster2),\n",
    "        'alcohol_consumption': np.random.normal(6, 4, n_cluster2),\n",
    "        'smoking_status': np.random.binomial(1, 0.25, n_cluster2),\n",
    "        'healthcare_visits': np.random.poisson(4, n_cluster2),\n",
    "        'chronic_conditions': np.random.poisson(1.2, n_cluster2),\n",
    "        'mental_health_days': np.random.normal(8, 3, n_cluster2),\n",
    "        'stress_level': np.random.normal(6, 2, n_cluster2),\n",
    "        'blood_pressure_systolic': np.random.normal(128, 12, n_cluster2),\n",
    "        'cholesterol_total': np.random.normal(210, 30, n_cluster2),\n",
    "        'glucose_level': np.random.normal(105, 15, n_cluster2)\n",
    "    }\n",
    "    \n",
    "    # Cluster 3: High-risk individuals (25% of population)\n",
    "    n_cluster3 = n_samples - n_cluster1 - n_cluster2\n",
    "    cluster3 = {\n",
    "        'age': np.random.normal(58, 8, n_cluster3),\n",
    "        'bmi': np.random.normal(32, 5, n_cluster3),\n",
    "        'physical_activity': np.random.normal(1, 1, n_cluster3),\n",
    "        'sleep_hours': np.random.normal(5.5, 1.5, n_cluster3),\n",
    "        'fruit_vegetable_intake': np.random.normal(1, 0.5, n_cluster3),\n",
    "        'alcohol_consumption': np.random.normal(10, 5, n_cluster3),\n",
    "        'smoking_status': np.random.binomial(1, 0.45, n_cluster3),\n",
    "        'healthcare_visits': np.random.poisson(7, n_cluster3),\n",
    "        'chronic_conditions': np.random.poisson(2.5, n_cluster3),\n",
    "        'mental_health_days': np.random.normal(15, 4, n_cluster3),\n",
    "        'stress_level': np.random.normal(8, 1.5, n_cluster3),\n",
    "        'blood_pressure_systolic': np.random.normal(145, 15, n_cluster3),\n",
    "        'cholesterol_total': np.random.normal(240, 35, n_cluster3),\n",
    "        'glucose_level': np.random.normal(120, 20, n_cluster3)\n",
    "    }\n",
    "    \n",
    "    # Combine clusters with noise to create overlap\n",
    "    df1 = pd.DataFrame(cluster1)\n",
    "    df2 = pd.DataFrame(cluster2)\n",
    "    df3 = pd.DataFrame(cluster3)\n",
    "    \n",
    "    # Concatenate all clusters\n",
    "    df = pd.concat([df1, df2, df3], ignore_index=True)\n",
    "    \n",
    "    # Add noise to create realistic overlap between clusters\n",
    "    noise_level = 0.3\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    for col in numeric_cols:\n",
    "        df[col] = df[col] + np.random.normal(0, df[col].std() * noise_level, len(df))\n",
    "    \n",
    "    # Clip unrealistic values\n",
    "    df['age'] = df['age'].clip(18, 100)\n",
    "    df['bmi'] = df['bmi'].clip(15, 55)\n",
    "    df['physical_activity'] = df['physical_activity'].clip(0, 7)\n",
    "    df['sleep_hours'] = df['sleep_hours'].clip(3, 12)\n",
    "    df['fruit_vegetable_intake'] = df['fruit_vegetable_intake'].clip(0, 10)\n",
    "    df['smoking_status'] = (df['smoking_status'] > 0.5).astype(int)\n",
    "    df['healthcare_visits'] = df['healthcare_visits'].clip(0, 30)\n",
    "    df['chronic_conditions'] = df['chronic_conditions'].clip(0, 10)\n",
    "    df['mental_health_days'] = df['mental_health_days'].clip(0, 30)\n",
    "    df['stress_level'] = df['stress_level'].clip(1, 10)\n",
    "    \n",
    "    # Add demographic variables\n",
    "    df['sex'] = np.random.binomial(1, 0.52, len(df))\n",
    "    df['education'] = np.random.choice([1, 2, 3, 4], len(df), p=[0.1, 0.3, 0.4, 0.2])\n",
    "    df['income'] = np.random.choice([1, 2, 3, 4, 5], len(df), p=[0.15, 0.25, 0.3, 0.2, 0.1])\n",
    "    df['race'] = np.random.choice([1, 2, 3, 4, 5], len(df), p=[0.6, 0.13, 0.17, 0.08, 0.02])\n",
    "    \n",
    "    # Add true cluster labels for validation (hidden from model)\n",
    "    true_labels = np.concatenate([\n",
    "        np.zeros(n_cluster1),\n",
    "        np.ones(n_cluster2),\n",
    "        np.full(n_cluster3, 2)\n",
    "    ])\n",
    "    df['true_cluster'] = true_labels.astype(int)\n",
    "    \n",
    "    # Round numeric values\n",
    "    numeric_round_cols = ['age', 'bmi', 'physical_activity', 'sleep_hours', \n",
    "                          'fruit_vegetable_intake', 'alcohol_consumption',\n",
    "                          'blood_pressure_systolic', 'cholesterol_total', \n",
    "                          'glucose_level', 'stress_level', 'mental_health_days']\n",
    "    for col in numeric_round_cols:\n",
    "        df[col] = df[col].round(1)\n",
    "    \n",
    "    print(f\"[OK] Synthetic dataset generated: {df.shape[0]} rows Ã— {df.shape[1]} columns\")\n",
    "    print(f\"     Features: {df.shape[1] - 2} health indicators + demographics + true labels\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Generate the dataset\n",
    "data = generate_synthetic_brfss_data(n_samples=3000)\n",
    "\n",
    "# Save raw data\n",
    "raw_data_file = os.path.join(PHASE_DIRS['data'], 'brfss_synthetic_data.csv')\n",
    "data.to_csv(raw_data_file, index=False)\n",
    "print(f\"[OK] Raw data saved to: {raw_data_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PHASE 3: DATASET DESCRIPTION AND EXPLORATORY DATA ANALYSIS\n",
    "# =============================================================================\n",
    "\n",
    "def describe_dataset(df):\n",
    "    \"\"\"Provide comprehensive description of the dataset.\"\"\"\n",
    "    print(\"=\" * 70)\n",
    "    print(\"DATASET METADATA AND DESCRIPTION\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    print(\"\\n[INFO] BASIC INFORMATION\")\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"Number of observations: {df.shape[0]:,}\")\n",
    "    print(f\"Number of variables: {df.shape[1]}\")\n",
    "    print(f\"Missing values: {df.isnull().sum().sum():,} ({100*df.isnull().sum().sum()/(df.shape[0]*df.shape[1]):.2f}%)\")\n",
    "    \n",
    "    print(\"\\n[INFO] VARIABLE LIST\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Classify variables by type\n",
    "    numeric_vars = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    \n",
    "    print(\"\\nNumeric Variables:\")\n",
    "    for i, col in enumerate(numeric_vars, 1):\n",
    "        print(f\"  {i:2d}. {col:<25} Range: [{df[col].min():.1f}, {df[col].max():.1f}]\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    \n",
    "    return numeric_vars\n",
    "\n",
    "# Run dataset description\n",
    "numeric_vars = describe_dataset(data)\n",
    "\n",
    "# Define key health indicators for analysis\n",
    "health_indicators = [\n",
    "    'age', 'bmi', 'physical_activity', 'sleep_hours', \n",
    "    'fruit_vegetable_intake', 'alcohol_consumption',\n",
    "    'healthcare_visits', 'chronic_conditions', 'mental_health_days',\n",
    "    'stress_level', 'blood_pressure_systolic', 'cholesterol_total', 'glucose_level'\n",
    "]\n",
    "\n",
    "# Summary statistics\n",
    "print(\"\\n[INFO] SUMMARY STATISTICS FOR HEALTH INDICATORS\")\n",
    "print(\"-\" * 70)\n",
    "summary_stats = data[health_indicators].describe().T\n",
    "summary_stats['median'] = data[health_indicators].median()\n",
    "summary_stats['skewness'] = data[health_indicators].skew()\n",
    "summary_stats['kurtosis'] = data[health_indicators].kurtosis()\n",
    "print(summary_stats.round(2).to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PHASE 4: DATA PREPROCESSING\n",
    "# =============================================================================\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Select features for clustering\n",
    "feature_columns = [\n",
    "    'age', 'bmi', 'physical_activity', 'sleep_hours', \n",
    "    'fruit_vegetable_intake', 'alcohol_consumption',\n",
    "    'healthcare_visits', 'chronic_conditions', 'mental_health_days',\n",
    "    'stress_level', 'blood_pressure_systolic', 'cholesterol_total', 'glucose_level'\n",
    "]\n",
    "\n",
    "# Create feature matrix\n",
    "X = data[feature_columns].copy()\n",
    "\n",
    "print(f\"[INFO] Features selected for clustering: {len(feature_columns)}\")\n",
    "for i, col in enumerate(feature_columns, 1):\n",
    "    print(f\"  {i:2d}. {col}\")\n",
    "\n",
    "# Handle missing values if any\n",
    "if X.isnull().sum().sum() > 0:\n",
    "    X = X.fillna(X.median())\n",
    "    print(\"\\n[OK] Missing values imputed with median values\")\n",
    "\n",
    "# Apply Standard Scaling\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_scaled_df = pd.DataFrame(X_scaled, columns=feature_columns)\n",
    "\n",
    "print(\"\\n[OK] Feature scaling applied using StandardScaler\")\n",
    "print(f\"  Scaled data shape: {X_scaled.shape}\")\n",
    "\n",
    "# Save scaler for future use\n",
    "save_model(scaler, 'standard_scaler', 'gmm_clustering')\n",
    "print(\"[OK] Scaler saved to models/gmm_clustering/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PHASE 5: GAUSSIAN MIXTURE MODELS IMPLEMENTATION\n",
    "# =============================================================================\n",
    "\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "\n",
    "# Split data for model validation\n",
    "X_train, X_test = train_test_split(X_scaled, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"[INFO] Data Split:\")\n",
    "print(f\"  Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"  Test set: {X_test.shape[0]} samples\")\n",
    "\n",
    "# =============================================================================\n",
    "# HYPERPARAMETER TUNING WITH GRID SEARCH\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"HYPERPARAMETER TUNING WITH GRID SEARCH\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "param_grid = {\n",
    "    'n_components': [2, 3, 4, 5, 6, 7, 8],\n",
    "    'covariance_type': ['full', 'tied', 'diag', 'spherical'],\n",
    "    'n_init': [5, 10, 15],\n",
    "    'reg_covar': [1e-6, 1e-5, 1e-4]\n",
    "}\n",
    "\n",
    "print(\"\\nHyperparameter Grid:\")\n",
    "print(f\"  n_components: {param_grid['n_components']}\")\n",
    "print(f\"  covariance_type: {param_grid['covariance_type']}\")\n",
    "print(f\"  n_init: {param_grid['n_init']}\")\n",
    "print(f\"  reg_covar: {param_grid['reg_covar']}\")\n",
    "print(f\"\\nTotal combinations: {len(param_grid['n_components']) * len(param_grid['covariance_type']) * len(param_grid['n_init']) * len(param_grid['reg_covar'])}\")\n",
    "\n",
    "from itertools import product\n",
    "\n",
    "def run_grid_search_gmm(X, param_grid):\n",
    "    \"\"\"Perform exhaustive grid search for GMM hyperparameters.\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    # Generate all combinations\n",
    "    keys = list(param_grid.keys())\n",
    "    values = [param_grid[k] for k in keys]\n",
    "    combinations = [dict(zip(keys, v)) for v in product(*values)]\n",
    "    \n",
    "    total = len(combinations)\n",
    "    print(f\"\\n[INFO] Evaluating {total} model configurations...\")\n",
    "    \n",
    "    for i, params in enumerate(combinations):\n",
    "        try:\n",
    "            gmm = GaussianMixture(\n",
    "                n_components=params['n_components'],\n",
    "                covariance_type=params['covariance_type'],\n",
    "                n_init=params['n_init'],\n",
    "                reg_covar=params['reg_covar'],\n",
    "                random_state=42,\n",
    "                max_iter=200\n",
    "            )\n",
    "            \n",
    "            gmm.fit(X)\n",
    "            labels = gmm.predict(X)\n",
    "            \n",
    "            result = {\n",
    "                'n_components': params['n_components'],\n",
    "                'covariance_type': params['covariance_type'],\n",
    "                'n_init': params['n_init'],\n",
    "                'reg_covar': params['reg_covar'],\n",
    "                'bic': gmm.bic(X),\n",
    "                'aic': gmm.aic(X),\n",
    "                'log_likelihood': gmm.score(X),\n",
    "                'silhouette': silhouette_score(X, labels),\n",
    "                'calinski_harabasz': calinski_harabasz_score(X, labels),\n",
    "                'davies_bouldin': davies_bouldin_score(X, labels),\n",
    "                'converged': gmm.converged_\n",
    "            }\n",
    "            \n",
    "            results.append(result)\n",
    "            \n",
    "            if (i + 1) % 100 == 0:\n",
    "                print(f\"    Progress: {i+1}/{total} ({100*(i+1)/total:.1f}%)\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"    Error with parameters {params}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Run grid search\n",
    "print(\"\\n[INFO] Running Grid Search with BIC optimization...\")\n",
    "grid_results = run_grid_search_gmm(X_train, param_grid)\n",
    "\n",
    "# Sort by BIC to find best model\n",
    "grid_results_sorted = grid_results.sort_values('bic').reset_index(drop=True)\n",
    "\n",
    "print(\"\\n[INFO] TOP 10 MODELS BY BIC (Best to Worst):\")\n",
    "print(\"-\" * 100)\n",
    "top_models = grid_results_sorted.head(10)[['n_components', 'covariance_type', 'n_init', \n",
    "                                            'bic', 'aic', 'silhouette', 'converged']]\n",
    "print(top_models.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PHASE 6: TRAIN OPTIMAL MODEL\n",
    "# =============================================================================\n",
    "\n",
    "# Get best parameters\n",
    "best_idx = grid_results_sorted.index[0]\n",
    "best_params = {\n",
    "    'n_components': int(grid_results_sorted.loc[best_idx, 'n_components']),\n",
    "    'covariance_type': grid_results_sorted.loc[best_idx, 'covariance_type'],\n",
    "    'n_init': int(grid_results_sorted.loc[best_idx, 'n_init']),\n",
    "    'reg_covar': grid_results_sorted.loc[best_idx, 'reg_covar']\n",
    "}\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"OPTIMAL MODEL CONFIGURATION\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\n  Number of components: {best_params['n_components']}\")\n",
    "print(f\"  Covariance type: {best_params['covariance_type']}\")\n",
    "print(f\"  Number of initializations: {best_params['n_init']}\")\n",
    "print(f\"  Regularization: {best_params['reg_covar']}\")\n",
    "print(f\"\\n  BIC Score: {grid_results_sorted.loc[best_idx, 'bic']:.2f}\")\n",
    "print(f\"  AIC Score: {grid_results_sorted.loc[best_idx, 'aic']:.2f}\")\n",
    "print(f\"  Silhouette Score: {grid_results_sorted.loc[best_idx, 'silhouette']:.4f}\")\n",
    "\n",
    "# Train the optimal model\n",
    "gmm_optimal = GaussianMixture(\n",
    "    n_components=best_params['n_components'],\n",
    "    covariance_type=best_params['covariance_type'],\n",
    "    n_init=best_params['n_init'],\n",
    "    reg_covar=best_params['reg_covar'],\n",
    "    random_state=42,\n",
    "    max_iter=500\n",
    ")\n",
    "\n",
    "gmm_optimal.fit(X_train)\n",
    "\n",
    "print(\"\\n[OK] Optimal GMM Model Trained Successfully\")\n",
    "print(f\"  Convergence: {gmm_optimal.converged_}\")\n",
    "print(f\"  Number of iterations: {gmm_optimal.n_iter_}\")\n",
    "\n",
    "# Save the optimal model\n",
    "model_filepath = save_model(gmm_optimal, 'gmm_optimal_model', 'gmm_clustering')\n",
    "print(f\"[OK] Model saved to: {model_filepath}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PHASE 7: CLUSTER ANALYSIS AND INTERPRETATION\n",
    "# =============================================================================\n",
    "\n",
    "# Get cluster labels for full dataset\n",
    "full_labels = gmm_optimal.predict(X_scaled)\n",
    "data['cluster'] = full_labels\n",
    "\n",
    "cluster_counts = pd.Series(full_labels).value_counts().sort_index()\n",
    "cluster_proportions = cluster_counts / len(full_labels) * 100\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"CLUSTER DISTRIBUTION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for cluster, count in cluster_counts.items():\n",
    "    prop = cluster_proportions[cluster]\n",
    "    print(f\"  Cluster {cluster}: {count:,} ({prop:.1f}%)\")\n",
    "\n",
    "# Calculate cluster profiles\n",
    "cluster_profiles = data.groupby('cluster')[feature_columns].mean()\n",
    "\n",
    "print(\"\\n[INFO] CLUSTER PROFILES (Mean Values):\")\n",
    "print(\"-\" * 100)\n",
    "print(cluster_profiles.round(2).to_string())\n",
    "\n",
    "# Save cluster profiles\n",
    "save_data(cluster_profiles, 'cluster_profiles', 'cluster_profiles')\n",
    "print(\"\\n[OK] Cluster profiles saved to output_v2/cluster_profiles/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PHASE 8: MODEL EVALUATION AND VALIDATION\n",
    "# =============================================================================\n",
    "\n",
    "from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score\n",
    "\n",
    "# Evaluate on training and test sets\n",
    "train_labels = gmm_optimal.predict(X_train)\n",
    "test_labels = gmm_optimal.predict(X_test)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"MODEL EVALUATION METRICS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "def evaluate_gmm(X, labels, model):\n",
    "    \"\"\"Comprehensive evaluation of GMM model performance.\"\"\"\n",
    "    metrics = {}\n",
    "    metrics['silhouette'] = silhouette_score(X, labels)\n",
    "    metrics['calinski_harabasz'] = calinski_harabasz_score(X, labels)\n",
    "    metrics['davies_bouldin'] = davies_bouldin_score(X, labels)\n",
    "    metrics['bic'] = model.bic(X)\n",
    "    metrics['aic'] = model.aic(X)\n",
    "    metrics['log_likelihood'] = model.score(X)\n",
    "    return metrics\n",
    "\n",
    "train_eval = evaluate_gmm(X_train, train_labels, gmm_optimal)\n",
    "test_eval = evaluate_gmm(X_test, test_labels, gmm_optimal)\n",
    "\n",
    "print(f\"\\n{'Metric':<25} {'Training':>12} {'Test':>12}\")\n",
    "print(\"-\" * 50)\n",
    "for key in train_eval:\n",
    "    print(f\"{key:<25} {train_eval[key]:>12.4f} {test_eval[key]:>12.4f}\")\n",
    "\n",
    "# External validation against true labels\n",
    "if 'true_cluster' in data.columns:\n",
    "    ari = adjusted_rand_score(data['true_cluster'], data['cluster'])\n",
    "    nmi = normalized_mutual_info_score(data['true_cluster'], data['cluster'])\n",
    "    \n",
    "    print(\"\\n[INFO] EXTERNAL VALIDATION (vs True Clusters):\")\n",
    "    print(f\"  Adjusted Rand Index (ARI): {ari:.4f}\")\n",
    "    print(f\"  Normalized Mutual Information (NMI): {nmi:.4f}\")\n",
    "    \n",
    "    if ari > 0.7:\n",
    "        print(\"  [OK] Strong agreement with true cluster structure\")\n",
    "    elif ari > 0.4:\n",
    "        print(\"  [~] Moderate agreement with true cluster structure\")\n",
    "    else:\n",
    "        print(\"  [!] Weak agreement with true cluster structure\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PHASE 9: PROBABILISTIC MEMBERSHIP ANALYSIS\n",
    "# =============================================================================\n",
    "\n",
    "# Get membership probabilities\n",
    "membership_probs = gmm_optimal.predict_proba(X_scaled)\n",
    "data_probs = data.copy()\n",
    "\n",
    "for i in range(best_params['n_components']):\n",
    "    data_probs[f'prob_cluster_{i}'] = membership_probs[:, i]\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"PROBABILISTIC MEMBERSHIP ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\nMembership Probability Statistics:\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for i in range(best_params['n_components']):\n",
    "    probs = data_probs[f'prob_cluster_{i}']\n",
    "    high_conf = (probs >= 0.8).sum()\n",
    "    print(f\"\\n  Cluster {i}:\")\n",
    "    print(f\"    Mean:   {probs.mean():.4f}\")\n",
    "    print(f\"    Std:    {probs.std():.4f}\")\n",
    "    print(f\"    High confidence (>=0.8): {high_conf:,} ({100*high_conf/len(probs):.1f}%)\")\n",
    "\n",
    "# Certainty analysis\n",
    "data_probs['max_prob'] = membership_probs.max(axis=1)\n",
    "\n",
    "high_conf = (data_probs['max_prob'] >= 0.8).sum()\n",
    "mod_conf = ((data_probs['max_prob'] >= 0.5) & (data_probs['max_prob'] < 0.8)).sum()\n",
    "low_conf = (data_probs['max_prob'] < 0.5).sum()\n",
    "\n",
    "print(\"\\n[INFO] Cluster Assignment Certainty:\")\n",
    "print(f\"  Very High Confidence (>=0.8): {high_conf:,} ({100*high_conf/len(data_probs):.1f}%)\")\n",
    "print(f\"  Moderate Confidence (0.5-0.8): {mod_conf:,} ({100*mod_conf/len(data_probs):.1f}%)\")\n",
    "print(f\"  Low Confidence (<0.5): {low_conf:,} ({100*low_conf/len(data_probs):.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PHASE 10: CONCLUSIONS AND FUTURE WORK\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"CONCLUSIONS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "n_clusters = best_params['n_components']\n",
    "silhouette_final = silhouette_score(X_scaled, data['cluster'])\n",
    "bic_final = gmm_optimal.bic(X_scaled)\n",
    "\n",
    "print(f\"\"\"\n",
    "PROJECT SUMMARY\n",
    "---------------\n",
    "This project applied Gaussian Mixture Models (GMM) to identify latent \n",
    "subpopulations within a synthetic public health dataset (simulating BRFSS).\n",
    "\n",
    "KEY FINDINGS:\n",
    "1. Optimal Number of Clusters: {n_clusters}\n",
    "   - BIC Score: {bic_final:.2f}\n",
    "   - Silhouette Score: {silhouette_final:.4f}\n",
    "\n",
    "2. Cluster Characteristics:\n",
    "   - Identified {n_clusters} distinct health phenotypes\n",
    "   - Probabilistic assignments with {100*high_conf/len(data_probs):.1f}% high-confidence\n",
    "   - Clear separation between health risk profiles\n",
    "\n",
    "PUBLIC HEALTH IMPLICATIONS:\n",
    "- The identified clusters represent distinct health phenotypes with different\n",
    "  risk profiles and intervention needs.\n",
    "- Probabilistic cluster assignments allow for uncertainty-aware decision making.\n",
    "- This approach can support targeted intervention design and resource allocation.\n",
    "\n",
    "LIMITATIONS:\n",
    "- Synthetic dataset may not fully represent real-world complexity\n",
    "- External validation with real BRFSS data recommended\n",
    "- Clinical validation required before operational deployment\n",
    "\n",
    "FUTURE WORK:\n",
    "- Compare with real BRFSS 2023 data from CDC\n",
    "- Extend to longitudinal analysis\n",
    "- Implement semi-supervised GMM with known outcomes\n",
    "\"\"\")\n",
    "\n",
    "# Save final configuration\n",
    "config = {\n",
    "    'student_id': 'SDS6/46982/2025',\n",
    "    'course': 'SDS6217 Advanced Machine Learning',\n",
    "    'institution': 'University of Nairobi',\n",
    "    'best_params': best_params,\n",
    "    'training_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'metrics': {\n",
    "        'bic': float(bic_final),\n",
    "        'silhouette': float(silhouette_final),\n",
    "        'n_clusters': n_clusters\n",
    "    }\n",
    "}\n",
    "\n",
    "save_data(config, 'project_config', 'metrics', fmt='json')\n",
    "print(\"[OK] Configuration saved to output_v2/metrics/\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"PROJECT COMPLETE\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Student ID: SDS6/46982/2025\")\n",
    "print(f\"Course: SDS6217 Advanced Machine Learning\")\n",
    "print(f\"Institution: University of Nairobi\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "1. McLachlan, G.J., & Peel, D. (2000). Finite Mixture Models. John Wiley & Sons.\n",
    "2. Bishop, C.M. (2006). Pattern Recognition and Machine Learning. Springer.\n",
    "3. CDC. (2023). Behavioral Risk Factor Surveillance System Survey Data.\n",
    "4. Schwarz, G. (1978). Estimating the dimension of a model. Annals of Statistics, 6(2), 461-464."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
