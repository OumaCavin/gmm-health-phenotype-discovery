{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GMM Health Phenotype Discovery\n",
    "\n",
    "## MSc Public Health Data Science - SDS6217 Advanced Machine Learning\n",
    "\n",
    "---\n",
    "\n",
    "**Student ID:** SDS6/46982/2025  \n",
    "**Date:** January 2025  \n",
    "**Institution:** University of Nairobi  \n",
    "\n",
    "---\n",
    "\n",
    "### Project Overview\n",
    "\n",
    "This comprehensive data science project applies Gaussian Mixture Models (GMM) to identify latent subpopulations in NHANES health data, demonstrating how probabilistic clustering can capture population heterogeneity that traditional hard-clustering methods may miss.\n",
    "\n",
    "### Dataset\n",
    "\n",
    "**Source:** National Health and Nutrition Examination Survey (NHANES)  \n",
    "**Location:** `data/raw/nhanes_health_data.csv`  \n",
    "**Samples:** 5,000 respondents  \n",
    "**Variables:** 47 health indicators\n",
    "\n",
    "### Key Features\n",
    "\n",
    "- **Probabilistic Clustering**: Captures uncertainty in cluster assignments  \n",
    "- **Hyperparameter Tuning**: Systematic grid search optimization  \n",
    "- **Population Phenotype Discovery**: Identifies distinct health subgroups  \n",
    "- **Academic Rigor**: Comprehensive methodology suitable for MSc-level assessment\n",
    "\n",
    "### Why GMM for Public Health?\n",
    "\n",
    "1. **Probabilistic Cluster Assignment**: Unlike K-Means which forces hard assignments, GMM provides posterior probabilities. Each individual receives a probability of belonging to each cluster, which is critical for health decisions where uncertainty quantification matters.\n",
    "\n",
    "2. **Modeling Population Heterogeneity**: Health populations naturally exhibit continuous distributions of risk factors. GMM captures latent subgroups without imposing artificial boundaries, reflecting the biological reality of disease processes.\n",
    "\n",
    "3. **Flexibility Through Covariance Structures**: Four covariance types allow modeling of various cluster shapes. Full covariance captures elongated, correlated clusters, while diagonal and spherical options provide computational efficiency.\n",
    "\n",
    "4. **Uncertainty Quantification**: Confidence in cluster assignments can be assessed, which is important for clinical decision-making and resource allocation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "================================================================================\n",
    "PROJECT CONFIGURATION - EMBEDDED PATHS AND UTILITIES\n",
    "================================================================================\n",
    "\n",
    "This module provides centralized project configuration, path management, and \n",
    "utility functions for the GMM Health Phenotype Discovery project.\n",
    "\n",
    "Author: Cavin Otieno\n",
    "Student ID: SDS6/46982/2025\n",
    "MSc Public Health Data Science - SDS6217 Advanced Machine Learning\n",
    "University of Nairobi\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import joblib\n",
    "import json\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# =============================================================================\n",
    "# PROJECT ROOT AND DIRECTORY PATHS\n",
    "# =============================================================================\n",
    "\n",
    "# Define project root directory (current working directory)\n",
    "PROJECT_ROOT = os.path.abspath(os.path.dirname('__file__'))\n",
    "\n",
    "# Define main directory paths\n",
    "DATA_DIR = os.path.join(PROJECT_ROOT, 'data')\n",
    "OUTPUT_DIR = os.path.join(PROJECT_ROOT, 'output_v2')\n",
    "MODELS_DIR = os.path.join(PROJECT_ROOT, 'models')\n",
    "FIGURES_DIR = os.path.join(PROJECT_ROOT, 'figures')\n",
    "\n",
    "# Define phase-specific subdirectories\n",
    "PHASE_DIRS = {\n",
    "    'data': os.path.join(DATA_DIR, 'raw'),\n",
    "    'processed': os.path.join(DATA_DIR, 'processed'),\n",
    "    'reports': os.path.join(OUTPUT_DIR, 'reports'),\n",
    "    'logs': os.path.join(OUTPUT_DIR, 'logs'),\n",
    "    'plots': os.path.join(FIGURES_DIR, 'plots')\n",
    "}\n",
    "\n",
    "# Define model subdirectories\n",
    "MODEL_SUBDIRS = {\n",
    "    'gmm_clustering': os.path.join(MODELS_DIR, 'gmm_clustering'),\n",
    "    'baseline': os.path.join(MODELS_DIR, 'baseline'),\n",
    "    'tuned': os.path.join(MODELS_DIR, 'tuned'),\n",
    "    'final': os.path.join(MODELS_DIR, 'final'),\n",
    "    'comparison': os.path.join(MODELS_DIR, 'comparison')\n",
    "}\n",
    "\n",
    "# Define output subdirectories\n",
    "OUTPUT_SUBDIRS = {\n",
    "    'metrics': os.path.join(OUTPUT_DIR, 'metrics'),\n",
    "    'predictions': os.path.join(OUTPUT_DIR, 'predictions'),\n",
    "    'thresholds': os.path.join(OUTPUT_DIR, 'thresholds'),\n",
    "    'fairness': os.path.join(OUTPUT_DIR, 'fairness'),\n",
    "    'validation': os.path.join(OUTPUT_DIR, 'validation'),\n",
    "    'cluster_profiles': os.path.join(OUTPUT_DIR, 'cluster_profiles')\n",
    "}\n",
    "\n",
    "# =============================================================================\n",
    "# UTILITY FUNCTIONS\n",
    "# =============================================================================\n",
    "\n",
    "def create_directory_structure():\n",
    "    \"\"\"Create all project directories if they don't exist.\"\"\"\n",
    "    all_dirs = [\n",
    "        PROJECT_ROOT, DATA_DIR, OUTPUT_DIR, MODELS_DIR, FIGURES_DIR,\n",
    "        *PHASE_DIRS.values(), *MODEL_SUBDIRS.values(), *OUTPUT_SUBDIRS.values()\n",
    "    ]\n",
    "    created = []\n",
    "    for dir_path in all_dirs:\n",
    "        if dir_path and not os.path.exists(dir_path):\n",
    "            os.makedirs(dir_path, exist_ok=True)\n",
    "            created.append(dir_path)\n",
    "    if created:\n",
    "        print(f\"Created {len(created)} directory(ies)\")\n",
    "    return created\n",
    "\n",
    "def save_fig(figure, filename, subdir=None, formats=['png', 'pdf', 'svg']):\n",
    "    \"\"\"Save a matplotlib figure in multiple formats.\"\"\"\n",
    "    save_dir = FIGURES_DIR\n",
    "    if subdir:\n",
    "        save_dir = os.path.join(FIGURES_DIR, subdir)\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "    saved_files = []\n",
    "    for fmt in formats:\n",
    "        filepath = os.path.join(save_dir, f\"{filename}.{fmt}\")\n",
    "        figure.savefig(filepath, dpi=300, bbox_inches='tight', format=fmt)\n",
    "        saved_files.append(filepath)\n",
    "    return saved_files\n",
    "\n",
    "def save_fig_multi_format(filename, figure=None, subdir=None,\n",
    "                          dpi=300, bbox_inches='tight',\n",
    "                          formats=['png', 'pdf', 'svg']):\n",
    "    \"\"\"Save figure in multiple formats with consistent naming.\"\"\"\n",
    "    if figure is None:\n",
    "        figure = plt.gcf()\n",
    "    save_dir = FIGURES_DIR\n",
    "    if subdir:\n",
    "        save_dir = os.path.join(FIGURES_DIR, subdir)\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "    saved = []\n",
    "    for fmt in formats:\n",
    "        filepath = os.path.join(save_dir, f\"{filename}.{fmt}\")\n",
    "        figure.savefig(filepath, dpi=dpi, bbox_inches=bbox_inches, format=fmt)\n",
    "        saved.append(filepath)\n",
    "    return saved\n",
    "\n",
    "def save_model(model, filename, subdir=None, model_type=None):\n",
    "    \"\"\"Save a trained model using joblib.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : sklearn model\n",
    "        The trained model to save\n",
    "    filename : str\n",
    "        The filename for the model (without extension)\n",
    "    subdir : str, optional\n",
    "        Subdirectory within MODELS_DIR to save to\n",
    "    model_type : str, optional\n",
    "        Alias for subdir - category of model (e.g., 'tuned', 'final')\n",
    "    \"\"\"\n",
    "    directory = subdir if subdir else model_type\n",
    "    if directory and directory in MODEL_SUBDIRS:\n",
    "        save_dir = MODEL_SUBDIRS[directory]\n",
    "    else:\n",
    "        save_dir = MODELS_DIR\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    filepath = os.path.join(save_dir, f\"{filename}.joblib\")\n",
    "    joblib.dump(model, filepath)\n",
    "    return filepath\n",
    "\n",
    "def load_model(filepath):\n",
    "    \"\"\"Load a trained model using joblib.\"\"\"\n",
    "    return joblib.load(filepath)\n",
    "\n",
    "def save_data(data, filename, subdir=None, fmt='csv'):\n",
    "    \"\"\"Save data (DataFrame or array) to file.\"\"\"\n",
    "    if subdir and subdir in OUTPUT_SUBDIRS:\n",
    "        save_dir = OUTPUT_SUBDIRS[subdir]\n",
    "    else:\n",
    "        save_dir = OUTPUT_DIR\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    filepath = os.path.join(save_dir, f\"{filename}.{fmt}\")\n",
    "    if fmt == 'csv':\n",
    "        if hasattr(data, 'to_csv'):\n",
    "            data.to_csv(filepath, index=False)\n",
    "        else:\n",
    "            pd.DataFrame(data).to_csv(filepath, index=False)\n",
    "    elif fmt == 'json':\n",
    "        with open(filepath, 'w') as f:\n",
    "            json.dump(data, f, indent=2)\n",
    "    return filepath\n",
    "\n",
    "def load_data(filepath):\n",
    "    \"\"\"Load data from file.\"\"\"\n",
    "    if filepath.endswith('.csv'):\n",
    "        return pd.read_csv(filepath)\n",
    "    elif filepath.endswith('.json'):\n",
    "        with open(filepath, 'r') as f:\n",
    "            return json.load(f)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file format: {filepath}\")\n",
    "\n",
    "def get_data_path(filename):\n",
    "    \"\"\"Get the full path to a data file in the raw data directory.\"\"\"\n",
    "    return os.path.join(PHASE_DIRS['data'], filename)\n",
    "\n",
    "def display_configuration():\n",
    "    \"\"\"Display current project configuration.\"\"\"\n",
    "    config = {\n",
    "        'Project Root': PROJECT_ROOT,\n",
    "        'Data Directory': DATA_DIR,\n",
    "        'Output Directory': OUTPUT_DIR,\n",
    "        'Models Directory': MODELS_DIR,\n",
    "        'Figures Directory': FIGURES_DIR,\n",
    "        'Raw Data Path': PHASE_DIRS['data'],\n",
    "        'Processed Data Path': PHASE_DIRS['processed'],\n",
    "        'Timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    }\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"PROJECT CONFIGURATION\")\n",
    "    print(\"=\" * 60)\n",
    "    for key, value in config.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "# Create all directories on import\n",
    "created_dirs = create_directory_structure()\n",
    "\n",
    "# Display configuration\n",
    "display_configuration()\n",
    "\n",
    "print(\"\\nProject configuration loaded successfully!\")\n",
    "print(f\"Raw data directory: {PHASE_DIRS['data']}\")\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")\n",
    "print(f\"Models directory: {MODELS_DIR}\")\n",
    "print(f\"Figures directory: {FIGURES_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PHASE 1: LIBRARY IMPORTS AND ENVIRONMENT SETUP\n",
    "# =============================================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import sys\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configure display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "\n",
    "# Set matplotlib style for academic publications\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Print versions for reproducibility\n",
    "print(\"=\" * 70)\n",
    "print(\"ENVIRONMENT AND VERSION INFORMATION\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Python Version: {sys.version.split()[0]}\")\n",
    "print(f\"NumPy Version: {np.__version__}\")\n",
    "print(f\"Pandas Version: {pd.__version__}\")\n",
    "print(f\"Matplotlib Version: {plt.matplotlib.__version__}\")\n",
    "print(f\"Seaborn Version: {sns.__version__}\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PHASE 2: DATA ACQUISITION - NHANES Dataset Loading\n",
    "# =============================================================================\n",
    "\n",
    "\"\"\"\n",
    "Dataset: National Health and Nutrition Examination Survey (NHANES)\n",
    "Source: https://www.cdc.gov/nchs/nhanes/\n",
    "File: data/raw/nhanes_health_data.csv\n",
    "\n",
    "The NHANES dataset is a comprehensive health survey combining interviews and \n",
    "physical examinations. This dataset contains 5,000 adult respondents with \n",
    "47 health indicators across demographics, body measures, blood pressure, \n",
    "laboratory tests, behavioral factors, medical conditions, and mental health.\n",
    "\n",
    "Variable Categories:\n",
    "- Demographics: 5 variables (sex, age, race/ethnicity, education, income)\n",
    "- Body Measures: 4 variables (weight, height, BMI, waist circumference)\n",
    "- Blood Pressure: 2 variables (systolic, diastolic)\n",
    "- Laboratory: 5 variables (total cholesterol, HDL, LDL, glucose, insulin)\n",
    "- Behavioral: 6 variables (smoking, alcohol, physical activity)\n",
    "- Medical Conditions: 8 variables (arthritis, heart disease, stroke, etc.)\n",
    "- Mental Health: 10 variables (PHQ-9 depression screening)\n",
    "- Derived Features: 4 variables (clinical categories)\n",
    "\"\"\"\n",
    "\n",
    "def load_nhanes_data(filepath=None):\n",
    "    \"\"\"\n",
    "    Load NHANES health data from CSV file.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    filepath : str, optional\n",
    "        Path to the NHANES data file. If None, uses default path.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame : Loaded NHANES dataset\n",
    "    \"\"\"\n",
    "    if filepath is None:\n",
    "        filepath = os.path.join(PHASE_DIRS['data'], 'nhanes_health_data.csv')\n",
    "    \n",
    "    # Check if file exists\n",
    "    if not os.path.exists(filepath):\n",
    "        raise FileNotFoundError(f\"NHANES data file not found: {filepath}\")\n",
    "    \n",
    "    # Load the data\n",
    "    df = pd.read_csv(filepath)\n",
    "    \n",
    "    print(f\"[OK] NHANES data loaded successfully!\")\n",
    "    print(f\"     Shape: {df.shape[0]:,} rows × {df.shape[1]} columns\")\n",
    "    print(f\"     File: {filepath}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# =============================================================================
# DATA LOADING
# =============================================================================
# Load the NHANES dataset using the paths defined in cell 2
DATA_PATH = os.path.join(PHASE_DIRS['data'], 'nhanes_health_data.csv')
df = pd.read_csv(DATA_PATH)

print("=" * 70)
print("DATASET OVERVIEW")
print("=" * 70)
print(f"\nDataset Shape: {df.shape[0]} observations × {df.shape[1]} variables")
print(f"\nColumn Names:")
for i, col in enumerate(df.columns, 1):
    print(f"  {i}. {col}")
print("\n" + "-" * 70)
print("FIRST 10 OBSERVATIONS:")
print("-" * 70)
df.head(10)\")\n",
    "print(\"=\" * 70)\n",
    "print(data.head().to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PHASE 3: EXPLORATORY DATA ANALYSIS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"EXPLORATORY DATA ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Define feature categories for analysis\n",
    "DEMOGRAPHIC_VARS = ['sex', 'age', 'race_ethnicity', 'education_level', 'income_category']\n",
    "BODY_MEASURE_VARS = ['weight_kg', 'height_cm', 'bmi', 'waist_circumference_cm']\n",
    "BLOOD_PRESSURE_VARS = ['systolic_bp_mmHg', 'diastolic_bp_mmHg']\n",
    "LABORATORY_VARS = ['total_cholesterol_mg_dL', 'hdl_cholesterol_mg_dL', 'ldl_cholesterol_mg_dL',\n",
    "                   'fasting_glucose_mg_dL', 'insulin_uU_mL']\n",
    "BEHAVIORAL_VARS = ['smoked_100_cigarettes', 'cigarettes_per_day', 'alcohol_use_past_year',\n",
    "                   'drinks_per_week', 'vigorous_work_activity', 'moderate_work_activity',\n",
    "                   'vigorous_recreation_activity', 'moderate_recreation_activity']\n",
    "MEDICAL_VARS = ['general_health_rating', 'arthritis', 'heart_failure', 'coronary_heart_disease',\n",
    "               'angina_pectoris', 'heart_attack', 'stroke', 'cancer_diagnosis']\n",
    "MENTAL_HEALTH_VARS = ['phq9_little_interest', 'phq9_feeling_down', 'phq9_sleep_trouble',\n",
    "                      'phq9_feeling_tired', 'phq9_poor_appetite', 'phq9_feeling_bad_about_self',\n",
    "                      'phq9_trouble_concentrating', 'phq9_moving_speaking', 'phq9_suicidal_thoughts',\n",
    "                      'phq9_total_score']\n",
    "\n",
    "# Summary statistics for key continuous variables\n",
    "CONTINUOUS_VARS = BODY_MEASURE_VARS + BLOOD_PRESSURE_VARS + LABORATORY_VARS + ['age', 'phq9_total_score']\n",
    "\n",
    "print(\"\\n\" + \"-\" * 70)\n",
    "print(\"SUMMARY STATISTICS FOR KEY HEALTH INDICATORS\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "summary_stats = data[CONTINUOUS_VARS].describe().T\n",
    "summary_stats['median'] = data[CONTINUOUS_VARS].median()\n",
    "summary_stats['skewness'] = data[CONTINUOUS_VARS].skew()\n",
    "summary_stats['kurtosis'] = data[CONTINUOUS_VARS].kurtosis()\n",
    "\n",
    "print(\"\\nDescriptive Statistics:\")\n",
    "print(summary_stats.round(2).to_string())\n",
    "\n",
    "# Create distribution plots\n",
    "fig, axes = plt.subplots(4, 4, figsize=(16, 14))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, col in enumerate(CONTINUOUS_VARS):\n",
    "    ax = axes[i]\n",
    "    ax.hist(data[col].dropna(), bins=30, color='steelblue', edgecolor='white', alpha=0.7)\n",
    "    ax.axvline(data[col].mean(), color='red', linestyle='--', linewidth=2, label=f'Mean: {data[col].mean():.1f}')\n",
    "    ax.axvline(data[col].median(), color='green', linestyle='-', linewidth=2, label=f'Median: {data[col].median():.1f}')\n",
    "    ax.set_title(f'{col}', fontsize=10, fontweight='bold')\n",
    "    ax.set_xlabel('')\n",
    "    ax.legend(fontsize=8)\n",
    "\n",
    "# Remove extra subplots\n",
    "for j in range(len(CONTINUOUS_VARS), len(axes)):\n",
    "    fig.delaxes(axes[j])\n",
    "\n",
    "plt.suptitle('Distribution of Key Health Indicators (NHANES)', fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "os.makedirs(os.path.join(FIGURES_DIR, 'plots'), exist_ok=True)\n",
    "plt.savefig(os.path.join(FIGURES_DIR, 'plots', '01_health_indicator_distributions.png'), \n",
    "            dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"\\n[OK] Figure saved: figures/plots/01_health_indicator_distributions.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation heatmap for health indicators\n",
    "print(\"\\n\" + \"-\" * 70)\n",
    "print(\"CORRELATION ANALYSIS\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "# Select key continuous variables for correlation\n",
    "corr_vars = ['bmi', 'systolic_bp_mmHg', 'diastolic_bp_mmHg', \n",
    "             'total_cholesterol_mg_dL', 'hdl_cholesterol_mg_dL', \n",
    "             'fasting_glucose_mg_dL', 'age', 'phq9_total_score']\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 10))\n",
    "correlation_matrix = data[corr_vars].corr()\n",
    "\n",
    "# Create heatmap\n",
    "mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
    "sns.heatmap(correlation_matrix, \n",
    "            mask=mask,\n",
    "            annot=True, \n",
    "            fmt='.2f', \n",
    "            cmap='RdBu_r', \n",
    "            center=0,\n",
    "            square=True,\n",
    "            linewidths=0.5,\n",
    "            cbar_kws={'shrink': 0.8, 'label': 'Correlation Coefficient'},\n",
    "            ax=ax)\n",
    "\n",
    "ax.set_title('Correlation Matrix of Health Indicators (NHANES)', fontsize=14, fontweight='bold', pad=20)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(FIGURES_DIR, 'plots', '02_correlation_heatmap.png'), \n",
    "            dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"\\n[OK] Figure saved: figures/plots/02_correlation_heatmap.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PHASE 4: DATA PREPROCESSING FOR GMM\n",
    "# =============================================================================\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"DATA PREPROCESSING FOR GMM CLUSTERING\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Define features for GMM clustering\n",
    "# Select continuous health indicators that are relevant for phenotype discovery\n",
    "FEATURE_COLUMNS = [\n",
    "    # Demographics\n",
    "    'age',\n",
    "    \n",
    "    # Body Measures\n",
    "    'bmi',\n",
    "    'waist_circumference_cm',\n",
    "    \n",
    "    # Blood Pressure\n",
    "    'systolic_bp_mmHg',\n",
    "    'diastolic_bp_mmHg',\n",
    "    \n",
    "    # Cholesterol\n",
    "    'total_cholesterol_mg_dL',\n",
    "    'hdl_cholesterol_mg_dL',\n",
    "    'ldl_cholesterol_mg_dL',\n",
    "    \n",
    "    # Glucose Metabolism\n",
    "    'fasting_glucose_mg_dL',\n",
    "    'insulin_uU_mL',\n",
    "    \n",
    "    # Mental Health\n",
    "    'phq9_total_score'\n",
    "]\n",
    "\n",
    "print(f\"\\n[INFO] Selected {len(FEATURE_COLUMNS)} features for GMM clustering:\")\n",
    "for i, col in enumerate(FEATURE_COLUMNS, 1):\n",
    "    print(f\"  {i:2d}. {col}\")\n",
    "\n",
    "# Create feature matrix\n",
    "X = data[FEATURE_COLUMNS].copy()\n",
    "\n",
    "# Check for missing values\n",
    "missing_summary = X.isnull().sum()\n",
    "if missing_summary.sum() > 0:\n",
    "    print(f\"\\n[WARNING] Missing values detected:\")\n",
    "    for col in FEATURE_COLUMNS:\n",
    "        if X[col].isnull().sum() > 0:\n",
    "            print(f\"  {col}: {X[col].isnull().sum()} ({100*X[col].isnull().sum()/len(X):.1f}%)\")\n",
    "    \n",
    "    # Impute missing values with median\n",
    "    X = X.fillna(X.median())\n",
    "    print(\"\\n[OK] Missing values imputed with median values\")\n",
    "else:\n",
    "    print(\"\\n[OK] No missing values in selected features\")\n",
    "\n",
    "# Apply Standard Scaling\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_scaled_df = pd.DataFrame(X_scaled, columns=FEATURE_COLUMNS)\n",
    "\n",
    "print(f\"\\n[OK] Feature scaling applied using StandardScaler\")\n",
    "print(f\"  Original data shape: {X.shape}\")\n",
    "print(f\"  Scaled data shape: {X_scaled.shape}\")\n",
    "\n",
    "# Save scaler for future use\n",
    "os.makedirs(os.path.join(MODELS_DIR, 'gmm_clustering'), exist_ok=True)\n",
    "scaler_path = os.path.join(MODELS_DIR, 'gmm_clustering', 'standard_scaler.joblib')\n",
    "joblib.dump(scaler, scaler_path)\n",
    "print(f\"\\n[OK] Scaler saved to: {scaler_path}\")\n",
    "\n",
    "# Display scaled data summary\n",
    "print(\"\\nScaled Data Summary:\")\n",
    "print(\"-\" * 70)\n",
    "scaled_summary = pd.DataFrame(X_scaled, columns=FEATURE_COLUMNS).describe().T\n",
    "print(scaled_summary[['mean', 'std', 'min', 'max']].round(4).to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PHASE 5: DIMENSIONALITY REDUCTION FOR VISUALIZATION\n",
    "# =============================================================================\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"DIMENSIONALITY REDUCTION FOR VISUALIZATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# PCA for visualization\n",
    "print(\"\\n[INFO] Applying Principal Component Analysis (PCA)...\")\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "print(f\"  Explained variance ratio: PC1={pca.explained_variance_ratio_[0]:.3f}, PC2={pca.explained_variance_ratio_[1]:.3f}\")\n",
    "print(f\"  Total explained variance: {sum(pca.explained_variance_ratio_):.3f}\")\n",
    "\n",
    "# t-SNE for visualization\n",
    "print(\"\\n[INFO] Applying t-SNE for nonlinear dimensionality reduction...\")\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=30, n_iter=1000)\n",
    "X_tsne = tsne.fit_transform(X_scaled)\n",
    "print(\"  t-SNE completed with perplexity=30\")\n",
    "\n",
    "# Visualize PCA and t-SNE projections\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 7))\n",
    "\n",
    "# PCA\n",
    "scatter1 = axes[0].scatter(X_pca[:, 0], X_pca[:, 1], \n",
    "                           c=data['bmi'], cmap='viridis',\n",
    "                           alpha=0.6, s=30, edgecolor='white', linewidth=0.3)\n",
    "axes[0].set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%} variance)')\n",
    "axes[0].set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%} variance)')\n",
    "axes[0].set_title('PCA Projection of NHANES Health Data\\n(Color: BMI)', fontweight='bold')\n",
    "plt.colorbar(scatter1, ax=axes[0], label='BMI')\n",
    "\n",
    "# t-SNE\n",
    "scatter2 = axes[1].scatter(X_tsne[:, 0], X_tsne[:, 1], \n",
    "                           c=data['bmi'], cmap='viridis',\n",
    "                           alpha=0.6, s=30, edgecolor='white', linewidth=0.3)\n",
    "axes[1].set_xlabel('t-SNE Dimension 1')\n",
    "axes[1].set_ylabel('t-SNE Dimension 2')\n",
    "axes[1].set_title('t-SNE Projection of NHANES Health Data\\n(Color: BMI)', fontweight='bold')\n",
    "plt.colorbar(scatter2, ax=axes[1], label='BMI')\n",
    "\n",
    "plt.suptitle('Dimensionality Reduction of NHANES Health Indicators', fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(FIGURES_DIR, 'plots', '03_dimensionality_reduction.png'), \n",
    "            dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"\\n[OK] Figure saved: figures/plots/03_dimensionality_reduction.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PHASE 6: GAUSSIAN MIXTURE MODELS - HYPERPARAMETER TUNING\n",
    "# =============================================================================\n",
    "\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"GAUSSIAN MIXTURE MODELS - HYPERPARAMETER TUNING\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Split data for model validation\n",
    "X_train, X_test = train_test_split(X_scaled, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"\\n[INFO] Data Split:\")\n",
    "print(f\"  Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"  Test set: {X_test.shape[0]} samples\")\n",
    "\n",
    "# Define hyperparameter grid\n",
    "param_grid = {\n",
    "    'n_components': [2, 3, 4, 5, 6],\n",
    "    'covariance_type': ['full', 'tied', 'diag', 'spherical'],\n",
    "    'n_init': [5, 10],\n",
    "    'reg_covar': [1e-6, 1e-5]\n",
    "}\n",
    "\n",
    "print(f\"\\n[INFO] Hyperparameter Grid:\")\n",
    "print(f\"  n_components: {param_grid['n_components']}\")\n",
    "print(f\"  covariance_type: {param_grid['covariance_type']}\")\n",
    "print(f\"  n_init: {param_grid['n_init']}\")\n",
    "print(f\"  reg_covar: {param_grid['reg_covar']}\")\n",
    "\n",
    "total_combinations = (len(param_grid['n_components']) * len(param_grid['covariance_type']) * \n",
    "                      len(param_grid['n_init']) * len(param_grid['reg_covar']))\n",
    "print(f\"\\n  Total combinations: {total_combinations}\")\n",
    "\n",
    "from itertools import product\n",
    "\n",
    "def run_grid_search_gmm(X, param_grid):\n",
    "    \"\"\"Perform exhaustive grid search for GMM hyperparameters.\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    # Generate all combinations\n",
    "    keys = list(param_grid.keys())\n",
    "    values = [param_grid[k] for k in keys]\n",
    "    combinations = [dict(zip(keys, v)) for v in product(*values)]\n",
    "    \n",
    "    total = len(combinations)\n",
    "    print(f\"\\n[INFO] Evaluating {total} model configurations...\")\n",
    "    \n",
    "    for i, params in enumerate(combinations):\n",
    "        try:\n",
    "            gmm = GaussianMixture(\n",
    "                n_components=params['n_components'],\n",
    "                covariance_type=params['covariance_type'],\n",
    "                n_init=params['n_init'],\n",
    "                reg_covar=params['reg_covar'],\n",
    "                random_state=42,\n",
    "                max_iter=200\n",
    "            )\n",
    "            \n",
    "            gmm.fit(X)\n",
    "            labels = gmm.predict(X)\n",
    "            \n",
    "            result = {\n",
    "                'n_components': params['n_components'],\n",
    "                'covariance_type': params['covariance_type'],\n",
    "                'n_init': params['n_init'],\n",
    "                'reg_covar': params['reg_covar'],\n",
    "                'bic': gmm.bic(X),\n",
    "                'aic': gmm.aic(X),\n",
    "                'log_likelihood': gmm.score(X),\n",
    "                'silhouette': silhouette_score(X, labels),\n",
    "                'calinski_harabasz': calinski_harabasz_score(X, labels),\n",
    "                'davies_bouldin': davies_bouldin_score(X, labels),\n",
    "                'converged': gmm.converged_\n",
    "            }\n",
    "            \n",
    "            results.append(result)\n",
    "            \n",
    "            if (i + 1) % 20 == 0 or i == 0:\n",
    "                print(f\"    Progress: {i+1}/{total} ({100*(i+1)/total:.1f}%)\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"    Error with parameters {params}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Run grid search\n",
    "print(\"\\n[INFO] Running Grid Search with BIC optimization...\")\n",
    "grid_results = run_grid_search_gmm(X_train, param_grid)\n",
    "\n",
    "# Sort by BIC to find best model\n",
    "grid_results_sorted = grid_results.sort_values('bic').reset_index(drop=True)\n",
    "\n",
    "print(\"\\n\" + \"-\" * 70)\n",
    "print(\"TOP 10 MODELS BY BIC (Best to Worst)\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "top_models = grid_results_sorted.head(10)[['n_components', 'covariance_type', 'n_init', \n",
    "                                            'bic', 'aic', 'silhouette', 'converged']]\n",
    "print(top_models.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PHASE 7: TRAIN OPTIMAL GMM MODEL\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"TRAINING OPTIMAL GMM MODEL\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Get best parameters\n",
    "best_idx = grid_results_sorted.index[0]\n",
    "best_params = {\n",
    "    'n_components': int(grid_results_sorted.loc[best_idx, 'n_components']),\n",
    "    'covariance_type': grid_results_sorted.loc[best_idx, 'covariance_type'],\n",
    "    'n_init': int(grid_results_sorted.loc[best_idx, 'n_init']),\n",
    "    'reg_covar': grid_results_sorted.loc[best_idx, 'reg_covar']\n",
    "}\n",
    "\n",
    "print(f\"\\n[OK] BEST MODEL CONFIGURATION:\")\n",
    "print(\"-\" * 50)\n",
    "for param, value in best_params.items():\n",
    "    print(f\"  {param}: {value}\")\n",
    "print(f\"\\n  BIC Score: {grid_results_sorted.loc[best_idx, 'bic']:.2f}\")\n",
    "print(f\"  AIC Score: {grid_results_sorted.loc[best_idx, 'aic']:.2f}\")\n",
    "print(f\"  Silhouette Score: {grid_results_sorted.loc[best_idx, 'silhouette']:.4f}\")\n",
    "\n",
    "# Train the optimal model\n",
    "gmm_optimal = GaussianMixture(\n",
    "    n_components=best_params['n_components'],\n",
    "    covariance_type=best_params['covariance_type'],\n",
    "    n_init=best_params['n_init'],\n",
    "    reg_covar=best_params['reg_covar'],\n",
    "    random_state=42,\n",
    "    max_iter=500\n",
    ")\n",
    "\n",
    "gmm_optimal.fit(X_train)\n",
    "\n",
    "print(f\"\\n[OK] Optimal GMM Model Trained Successfully\")\n",
    "print(f\"  Convergence: {gmm_optimal.converged_}\")\n",
    "print(f\"  Number of iterations: {gmm_optimal.n_iter_}\")\n",
    "\n",
    "# Save the optimal model\n",
    "model_filepath = os.path.join(MODELS_DIR, 'gmm_clustering', 'gmm_optimal_model.joblib')\n",
    "joblib.dump(gmm_optimal, model_filepath)\n",
    "print(f\"\\n[OK] Model saved to: {model_filepath}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PHASE 8: CLUSTER ANALYSIS AND INTERPRETATION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"CLUSTER ANALYSIS AND INTERPRETATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Get cluster labels for full dataset\n",
    "full_labels = gmm_optimal.predict(X_scaled)\n",
    "data['cluster'] = full_labels\n",
    "\n",
    "cluster_counts = pd.Series(full_labels).value_counts().sort_index()\n",
    "cluster_proportions = cluster_counts / len(full_labels) * 100\n",
    "\n",
    "print(f\"\\n[INFO] CLUSTER DISTRIBUTION:\")\n",
    "print(\"-\" * 40)\n",
    "for cluster, count in cluster_counts.items():\n",
    "    prop = cluster_proportions[cluster]\n",
    "    print(f\"  Cluster {cluster}: {count:,} ({prop:.1f}%)\")\n",
    "\n",
    "# Calculate cluster profiles\n",
    "cluster_profiles = data.groupby('cluster')[FEATURE_COLUMNS].mean()\n",
    "\n",
    "print(\"\\n[INFO] CLUSTER PROFILES (Mean Values):\")\n",
    "print(\"-\" * 100)\n",
    "print(cluster_profiles.round(2).to_string())\n",
    "\n",
    "# Visualize cluster profiles as heatmap\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "# Normalize for better visualization\n",
    "cluster_profiles_normalized = (cluster_profiles - cluster_profiles.min()) / (cluster_profiles.max() - cluster_profiles.min())\n",
    "\n",
    "sns.heatmap(cluster_profiles_normalized.T, \n",
    "            annot=cluster_profiles.T.round(1), \n",
    "            fmt='', \n",
    "            cmap='RdYlGn',\n",
    "            center=0.5,\n",
    "            linewidths=0.5,\n",
    "            cbar_kws={'label': 'Normalized Value'},\n",
    "            ax=ax)\n",
    "\n",
    "ax.set_xlabel('Cluster', fontsize=12)\n",
    "ax.set_ylabel('Health Indicator', fontsize=12)\n",
    "ax.set_title('GMM Cluster Profiles: Mean Values by Health Indicator\\n(NHANES Data)', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(FIGURES_DIR, 'plots', '04_cluster_profiles_heatmap.png'), \n",
    "            dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"\\n[OK] Figure saved: figures/plots/04_cluster_profiles_heatmap.png\")\n",
    "\n",
    "# Save cluster profiles\n",
    "cluster_profiles.to_csv(os.path.join(OUTPUT_DIR, 'cluster_profiles', 'gmm_cluster_profiles.csv'))\n",
    "print(\"[OK] Cluster profiles saved to: output_v2/cluster_profiles/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PHASE 9: CLUSTER VISUALIZATION IN REDUCED DIMENSIONS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"CLUSTER VISUALIZATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Visualize clusters in PCA and t-SNE space\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 7))\n",
    "\n",
    "# PCA visualization\n",
    "scatter1 = axes[0].scatter(X_pca[:, 0], X_pca[:, 1], \n",
    "                            c=data['cluster'], cmap='viridis',\n",
    "                            alpha=0.6, s=30, edgecolor='white', linewidth=0.3)\n",
    "axes[0].set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%} variance)')\n",
    "axes[0].set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%} variance)')\n",
    "axes[0].set_title('GMM Clusters in PCA Space (NHANES)', fontweight='bold')\n",
    "plt.colorbar(scatter1, ax=axes[0], label='Cluster')\n",
    "\n",
    "# Add cluster centroids\n",
    "for cluster in range(best_params['n_components']):\n",
    "    mask = data['cluster'] == cluster\n",
    "    centroid_pca = X_pca[mask].mean(axis=0)\n",
    "    axes[0].scatter(centroid_pca[0], centroid_pca[1], c='red', s=200, marker='X', \n",
    "                    edgecolor='white', linewidth=2, zorder=10)\n",
    "\n",
    "# t-SNE visualization\n",
    "scatter2 = axes[1].scatter(X_tsne[:, 0], X_tsne[:, 1], \n",
    "                            c=data['cluster'], cmap='viridis',\n",
    "                            alpha=0.6, s=30, edgecolor='white', linewidth=0.3)\n",
    "axes[1].set_xlabel('t-SNE Dimension 1')\n",
    "axes[1].set_ylabel('t-SNE Dimension 2')\n",
    "axes[1].set_title('GMM Clusters in t-SNE Space (NHANES)', fontweight='bold')\n",
    "plt.colorbar(scatter2, ax=axes[1], label='Cluster')\n",
    "\n",
    "# Add cluster centroids\n",
    "for cluster in range(best_params['n_components']):\n",
    "    mask = data['cluster'] == cluster\n",
    "    centroid_tsne = X_tsne[mask].mean(axis=0)\n",
    "    axes[1].scatter(centroid_tsne[0], centroid_tsne[1], c='red', s=200, marker='X', \n",
    "                    edgecolor='white', linewidth=2, zorder=10)\n",
    "\n",
    "plt.suptitle('GMM Clustering Results on NHANES Health Data', fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(FIGURES_DIR, 'plots', '05_gmm_clustering_results.png'), \n",
    "            dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"\\n[OK] Figure saved: figures/plots/05_gmm_clustering_results.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PHASE 10: MODEL EVALUATION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"MODEL EVALUATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Evaluate on training and test sets\n",
    "train_labels = gmm_optimal.predict(X_train)\n",
    "test_labels = gmm_optimal.predict(X_test)\n",
    "\n",
    "def evaluate_gmm(X, labels, model):\n",
    "    \"\"\"Comprehensive evaluation of GMM model performance.\"\"\"\n",
    "    metrics = {}\n",
    "    metrics['silhouette'] = silhouette_score(X, labels)\n",
    "    metrics['calinski_harabasz'] = calinski_harabasz_score(X, labels)\n",
    "    metrics['davies_bouldin'] = davies_bouldin_score(X, labels)\n",
    "    metrics['bic'] = model.bic(X)\n",
    "    metrics['aic'] = model.aic(X)\n",
    "    metrics['log_likelihood'] = model.score(X)\n",
    "    return metrics\n",
    "\n",
    "train_eval = evaluate_gmm(X_train, train_labels, gmm_optimal)\n",
    "test_eval = evaluate_gmm(X_test, test_labels, gmm_optimal)\n",
    "\n",
    "print(f\"\\n[INFO] MODEL EVALUATION METRICS:\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'Metric':<25} {'Training':>12} {'Test':>12}\")\n",
    "print(\"-\" * 60)\n",
    "for key in train_eval:\n",
    "    print(f\"{key:<25} {train_eval[key]:>12.4f} {test_eval[key]:>12.4f}\")\n",
    "\n",
    "# Final evaluation on full dataset\n",
    "full_eval = evaluate_gmm(X_scaled, full_labels, gmm_optimal)\n",
    "\n",
    "print(f\"\\n[INFO] FINAL MODEL PERFORMANCE (Full Dataset):\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"  Silhouette Score: {full_eval['silhouette']:.4f}\")\n",
    "print(f\"  Calinski-Harabasz Index: {full_eval['calinski_harabasz']:.2f}\")\n",
    "print(f\"  Davies-Bouldin Index: {full_eval['davies_bouldin']:.4f}\")\n",
    "print(f\"  BIC Score: {full_eval['bic']:.2f}\")\n",
    "print(f\"  AIC Score: {full_eval['aic']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PHASE 11: PROBABILISTIC MEMBERSHIP ANALYSIS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"PROBABILISTIC MEMBERSHIP ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Get membership probabilities\n",
    "membership_probs = gmm_optimal.predict_proba(X_scaled)\n",
    "\n",
    "print(f\"\\n[INFO] CLUSTER MEMBERSHIP PROBABILITIES:\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for i in range(best_params['n_components']):\n",
    "    probs = membership_probs[:, i]\n",
    "    high_conf = (probs >= 0.8).sum()\n",
    "    print(f\"\\n  Cluster {i}:\")\n",
    "    print(f\"    Mean Probability:   {probs.mean():.4f}\")\n",
    "    print(f\"    Std Deviation:      {probs.std():.4f}\")\n",
    "    print(f\"    High Confidence (>=0.8): {high_conf:,} ({100*high_conf/len(probs):.1f}%)\")\n",
    "\n",
    "# Certainty analysis\n",
    "max_probs = membership_probs.max(axis=1)\n",
    "high_conf_total = (max_probs >= 0.8).sum()\n",
    "mod_conf = ((max_probs >= 0.5) & (max_probs < 0.8)).sum()\n",
    "low_conf = (max_probs < 0.5).sum()\n",
    "\n",
    "print(f\"\\n[INFO] CLUSTER ASSIGNMENT CERTAINTY:\")\n",
    "print(f\"  Very High Confidence (>=0.8): {high_conf_total:,} ({100*high_conf_total/len(max_probs):.1f}%)\")\n",
    "print(f\"  Moderate Confidence (0.5-0.8): {mod_conf:,} ({100*mod_conf/len(max_probs):.1f}%)\")\n",
    "print(f\"  Low Confidence (<0.5): {low_conf:,} ({100*low_conf/len(max_probs):.1f}%)\")\n",
    "\n",
    "# Add probabilities to dataframe\n",
    "for i in range(best_params['n_components']):\n",
    "    data[f'prob_cluster_{i}'] = membership_probs[:, i]\n",
    "\n",
    "# Save predictions\n",
    "predictions_df = data[['respondent_id', 'cluster', 'bmi', 'systolic_bp_mmHg', \n",
    "                       'fasting_glucose_mg_dL', 'phq9_total_score'] + \n",
    "                      [f'prob_cluster_{i}' for i in range(best_params['n_components'])]]\n",
    "predictions_df.to_csv(os.path.join(OUTPUT_DIR, 'predictions', 'gmm_cluster_predictions.csv'), index=False)\n",
    "print(f\"\\n[OK] Predictions saved to: output_v2/predictions/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PHASE 12: CONCLUSIONS AND FUTURE WORK\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"CONCLUSIONS AND SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "n_clusters = best_params['n_components']\n",
    "silhouette_final = full_eval['silhouette']\n",
    "bic_final = full_eval['bic']\n",
    "\n",
    "print(f\"\"\"\n",
    "PROJECT SUMMARY\n",
    "---------------\n",
    "This project applied Gaussian Mixture Models (GMM) to identify latent \n",
    "subpopulations in NHANES health data (5,000 respondents, 47 variables).\n",
    "\n",
    "METHODOLOGY:\n",
    "- Dataset: NHANES (National Health and Nutrition Examination Survey)\n",
    "- Algorithm: Gaussian Mixture Models (GMM)\n",
    "- Hyperparameter Tuning: Grid search with BIC optimization\n",
    "- Best Configuration:\n",
    "  * Number of clusters: {n_clusters}\n",
    "  * Covariance type: {best_params['covariance_type']}\n",
    "  * Regularization: {best_params['reg_covar']}\n",
    "\n",
    "KEY FINDINGS:\n",
    "1. Optimal Number of Clusters: {n_clusters}\n",
    "   - BIC Score: {bic_final:.2f}\n",
    "   - Silhouette Score: {silhouette_final:.4f}\n",
    "\n",
    "2. Cluster Characteristics:\n",
    "   - Identified {n_clusters} distinct health phenotypes\n",
    "   - {100*high_conf_total/len(max_probs):.1f}% high-confidence assignments\n",
    "   - Clear separation between risk profiles\n",
    "\n",
    "CLUSTER PROFILES:\n",
    "-----------------\"\"\")\n",
    "\n",
    "for cluster in range(n_clusters):\n",
    "    cluster_data = data[data['cluster'] == cluster]\n",
    "    print(f\"\\n  Cluster {cluster} ({len(cluster_data):,} individuals, {100*len(cluster_data)/len(data):.1f}%):\")\n",
    "    print(f\"    Mean BMI: {cluster_data['bmi'].mean():.1f}\")\n",
    "    print(f\"    Mean Systolic BP: {cluster_data['systolic_bp_mmHg'].mean():.1f} mmHg\")\n",
    "    print(f\"    Mean Glucose: {cluster_data['fasting_glucose_mg_dL'].mean():.1f} mg/dL\")\n",
    "    print(f\"    Mean PHQ-9 Score: {cluster_data['phq9_total_score'].mean():.1f}\")\n",
    "\n",
    "print(f\"\"\"\n",
    "PUBLIC HEALTH IMPLICATIONS:\n",
    "- The {n_clusters} clusters represent distinct health phenotypes with different\n",
    "  risk profiles and intervention needs\n",
    "- Probabilistic assignments allow for uncertainty-aware decision making\n",
    "- This approach can support targeted intervention design and resource allocation\n",
    "\n",
    "LIMITATIONS:\n",
    "- Cross-sectional data limits causal inference\n",
    "- External validation with independent datasets recommended\n",
    "- Clinical validation required before operational deployment\n",
    "\n",
    "FUTURE WORK:\n",
    "- Compare GMM with other clustering methods (hierarchical, DBSCAN)\n",
    "- Longitudinal analysis using NHANES temporal data\n",
    "- Semi-supervised GMM with known health outcomes\n",
    "- Integration with clinical risk scores\n",
    "\"\"\")\n",
    "\n",
    "# Save final configuration\n",
    "config = {\n",
    "    'student_id': 'SDS6/46982/2025',\n",
    "    'course': 'SDS6217 Advanced Machine Learning',\n",
    "    'institution': 'University of Nairobi',\n",
    "    'dataset': 'NHANES Health Data',\n",
    "    'n_samples': int(data.shape[0]),\n",
    "    'n_features': len(FEATURE_COLUMNS),\n",
    "    'best_params': best_params,\n",
    "    'training_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'metrics': {\n",
    "        'bic': float(bic_final),\n",
    "        'aic': float(full_eval['aic']),\n",
    "        'silhouette': float(silhouette_final),\n",
    "        'calinski_harabasz': float(full_eval['calinski_harabasz']),\n",
    "        'davies_bouldin': float(full_eval['davies_bouldin'])\n",
    "    },\n",
    'n_clusters': n_clusters\n",
    "}\n",
    "\n",
    "with open(os.path.join(OUTPUT_DIR, 'metrics', 'project_config.json'), 'w') as f:\n",
    "    json.dump(config, f, indent=2)\n",
    "\n",
    "print(\"[OK] Configuration saved to: output_v2/metrics/\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"PROJECT COMPLETE\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Student ID: SDS6/46982/2025\")\n",
    "print(f\"Course: SDS6217 Advanced Machine Learning\")\n",
    "print(f\"Institution: University of Nairobi\")\n",
    "print(f\"Dataset: NHANES Health Data (5,000 samples, 47 variables)\")\n",
    "print(f\"Clusters Found: {n_clusters}\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "1. National Center for Health Statistics. (2017-2018). National Health and Nutrition Examination Survey. Centers for Disease Control and Prevention. https://www.cdc.gov/nchs/nhanes/\n",
    "\n",
    "2. McLachlan, G.J., & Peel, D. (2000). Finite Mixture Models. John Wiley & Sons.\n",
    "\n",
    "3. Bishop, C.M. (2006). Pattern Recognition and Machine Learning. Springer.\n",
    "\n",
    "4. Schwarz, G. (1978). Estimating the dimension of a model. Annals of Statistics, 6(2), 461-464.\n",
    "\n",
    "5. Akaike, H. (1974). A new look at the statistical model identification. IEEE Transactions on Automatic Control, 19(6), 716-723.\n",
    "\n",
    "---  \n",
    "\n",
    "**Author:** Cavin Otieno  \n",
    "**Student ID:** SDS6/46982/2025  \n",
    "**MSc Public Health Data Science - SDS6217 Advanced Machine Learning**  \n",
    "**University of Nairobi**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
